{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2dd3c2c",
   "metadata": {},
   "source": [
    "# Part 2: Time Series Forecast with ARIMA (2 days) 📈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3770fe",
   "metadata": {},
   "source": [
    "Welcome to the second part of your journey towards mastering the art of forecasting with ARIMA models. We will demystify ARIMA models, beginning with its foundational principles - autoregression, differencing, moving average. Using the same dataset as in Part 1, you will fit different models and assess their quality in order to compare them. You will also learn the different way of performing train/test splits and cross-validation in the context of time series!\n",
    "\n",
    "In this notebook, we will cover the following concepts:\n",
    "- Train / Test splitting for time series\n",
    "- Naive forecasting models\n",
    "- ARIMA and SARIMA models\n",
    "- Cross-Validation for time series"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "399da4c9",
   "metadata": {},
   "source": [
    "**Objectives**\n",
    "- Get to use few prediction models\n",
    "- Understand Cross-Validation in the context of Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6744bffa",
   "metadata": {},
   "source": [
    "**Ressources**\n",
    "\n",
    "AR, MA, ARIMA :\n",
    "\n",
    "- https://www.youtube.com/watch?v=Mc6sBAUdDP4&list=PLjwX9KFWtvNnOc4HtsvaDf1XYG3O5bv5s&index=6\n",
    "- https://www.youtube.com/watch?v=zNLG8tsA_Go&list=PLjwX9KFWtvNnOc4HtsvaDf1XYG3O5bv5s&index=6\n",
    "- https://www.youtube.com/watch?v=dXND1OEBABI&list=PLjwX9KFWtvNnOc4HtsvaDf1XYG3O5bv5s&index=7\n",
    "\n",
    "ACF & PACF :\n",
    "\n",
    "- https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/\n",
    "\n",
    "Métriques :\n",
    "\n",
    "- https://towardsdatascience.com/time-series-forecast-error-metrics-you-should-know-cc88b8c67f27\n",
    "\n",
    "Cross-validation :\n",
    "\n",
    "- https://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model-selection\n",
    "- https://robjhyndman.com/hyndsight/tscv/\n",
    "\n",
    "Forecasting: Principles and Practice livre (Rob Hyndman) :\n",
    "\n",
    "- https://otexts.com/fpp2/\n",
    "- https://otexts.com/fpp2/non-seasonal-arima.html\n",
    "- https://otexts.com/fpp2/transformations.html\n",
    "- https://otexts.com/fpp2/seasonal-arima.html\n",
    "\n",
    "Documentation de Pmdarima:\n",
    "\n",
    "- http://alkaline-ml.com/pmdarima/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a189b1a",
   "metadata": {},
   "source": [
    "**Required librairies**\n",
    "- [ ] numpy\n",
    "- [ ] pandas\n",
    "- [ ] matplotlib\n",
    "- [ ] seaborn\n",
    "- [ ] pmdarima\n",
    "- [ ] sktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "839f4565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pmdarima as pm\n",
    "import sktime as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccf1ef0",
   "metadata": {},
   "source": [
    "# A bit of theory, again 😁"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b68a73e",
   "metadata": {},
   "source": [
    "The ARMA (AutoRegressive Moving Average) model is suitable for predicting any time series whose value $Y$ at time $t$ can be explained by previous values and previous perturbations (noise).\n",
    "\n",
    "Before specifying what a dataset is according to the ARIMA model, let's specify a dataset according to the conditions :  \n",
    "\n",
    "- **Autoregressive**: the model predicts the variable $Y$ at time $t$, denoted $Y_t$, from previous values of $Y$ according to a linear regression model, i.e. $Y_{t-1}, Y_{t-2}, ..., Y_0$. Let $p$ denote the number of components used to predict $Y_t$, with\n",
    "\n",
    "> $Y_t = \\sum_{i=1}^p \\varphi_i\\,Y_{t-i} + \\varepsilon_t + c$, with $\\varepsilon_t$ a perturbation, and c a constant.\n",
    "\n",
    "- **Moving average**: a model where the data are the result of the moving average of order $q$. The value $Y_t$ can be expressed using the perturbation at time $t$ and its previous values:\n",
    "\n",
    "> $Y_t = \\mu + \\varepsilon_t + \\sum_{1}^{q}\\theta_q\\varepsilon_{t-q}$, with $\\mu$ a constant.\n",
    "\n",
    "Any ARMA model is the combination of the *AR* and *MA* conditions, i.e. it is autoregressive and its noise follows a moving average. Let :\n",
    "\n",
    "> $Y_t = c + \\sum_{i=1}^p \\varphi_i\\,Y_{t-i} + \\varepsilon_t + \\sum_{1}^{q}\\theta_q\\varepsilon_{t_q}$\n",
    "\n",
    "or :\n",
    "\n",
    "> $Y_t - \\sum_{i=1}^p \\varphi_i Y_{t-i} = \\varepsilon_t + \\sum_{i=1}^q \\theta_i \\varepsilon_{t-i}$ with $\\varepsilon_t$ the error of the autoregressive model at time $t$.\n",
    "\n",
    "The model is optimized using the least-squares method.\n",
    "\n",
    "An ARMA model is suitable for stationary data sets, i.e. with a fixed mean and standard deviation over time. Such data are rarely observed. Many time series datasets show trends in the mean or seasonal patterns. To do this, the problem must first be stationarized. As we saw before, this can be done by differencing the data, apply a model, and then performing the reverse operation: **integration**. An **ARIMA** model is said to be :\n",
    "\n",
    "- **integrated**: when the data set can be corrected by running the model on $Y'_t = Y_t - Y_{t-1}$ for a correction of order 1 (d=1), or $Y''_t = Y_t - Y_{t-1} - Y_{t-2}$ for a correction of order 2 (d=2), etc...  We note $d$ the order of *correction by integration* to suppress polynomial tendencies of order $d$, with:\n",
    "    \n",
    "> $Ycorr_t = Y_t - \\sum_{1}^{d} Y_{t-d}$\n",
    "\n",
    "\n",
    "ARIMA models are often denoted: ARIMA(p, d, q), or **ARIMA(2, 1, 1)** means :\n",
    "- an autoregressive model of order 2,\n",
    "- on data integrated 1 times,\n",
    "- with a moving average of order 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f2bd5",
   "metadata": {},
   "source": [
    "# 1. Train / Test Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c0d6cc",
   "metadata": {},
   "source": [
    "Because the temporal order is important in time series, we cannot perform a random train/test split. Rather, we can use the most recent data (e.g. the last year) as the test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd9c3fe",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "- Explain why train/test splitting can be a challenge for time series\n",
    "- Write a function to split the dataset into a train and test set\n",
    "- Use the function to split the airline passengers dataset, keeping the last twelve months as a test set\n",
    "- Plot the train and test set in different colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "518601f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_airline\n",
    "\n",
    "# Load dataset\n",
    "airline = load_airline()\n",
    "\n",
    "# Clean & Adjust the data\n",
    "airline = airline.dropna()\n",
    "airline_adj = airline / airline.index.days_in_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "447f208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_split(data, split_date):\n",
    "    '''\n",
    "    Split time series into training and test data\n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    data - pd.DataFrame - time series data.  Index expected as datatimeindex\n",
    "    split_date - the date on which to split the time series\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple (len=2) \n",
    "    0. pandas.DataFrame - training dataset\n",
    "    1. pandas.DataFrame - test dataset\n",
    "    '''\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592583b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "# Hold back the last three years as a TEST set\n",
    "\n",
    "\n",
    "# Check train and test sizes are what we expect\n",
    "\n",
    "\n",
    "# Plot the series\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb3851",
   "metadata": {},
   "source": [
    "# 2. Naive Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2722ffb5",
   "metadata": {},
   "source": [
    "We need a baseline (as always!) to know if our more complex models are any use! Given the strong seasonal component that was confirmed by the seasonal decomposition we made earlier, it may be a good idea to use a **seasonal naive** forecasting method. This model takes the value from the same period in the previous season (in this case the same month last year). This is part of the Carry-Forward-Previous-Values' family of **naive** forecasting methods. In general, if we have data with period $k$ are at time $t$ and we are predicting time $Y_{t+1}$ then we simply carry forward the value from $Y_{t+1-k}$. In other words, we have yearly data so we just take the value from the same month last year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d7aa0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "HORIZON = 12  # We plan to make forecasts for the following 12 months\n",
    "PERIOD = 12  # Expected period of seasonality\n",
    "\n",
    "# Define the length of time into the future for which forecasts are to be prepared\n",
    "fh = ForecastingHorizon(test.index, is_relative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86e896",
   "metadata": {},
   "source": [
    "### 2.1 Fit a Naive Forecaster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8fb9af",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "- Fit a Naive and Seasonal Naive forecaster to the train data, and show the predictions along with the test data.\n",
    "- Plot the residuals using Seaborn's `displot()`, `regplot()` and `residplot()` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3317a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the Naive Forecaster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae3d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the Seasonal Naive Forecaster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d799486c",
   "metadata": {},
   "source": [
    "Plotting the **residuals** can give us information about how the model is performing and the errors it is making.  Sometimes there are reffered to as **in sample** diagnostics. This just means we are looking at diagnostics of data which has been used to fit the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea281d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc48f8e7",
   "metadata": {},
   "source": [
    "### 2.2 Error Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe927b74",
   "metadata": {},
   "source": [
    "You've probably already worked with the Mean Squared Error (MSE). Let's have a look at different metrics.\n",
    "\n",
    "RMSE and MAE are called 'scale dependent' measures as the units and magnitude are specific to the problem and context.  An alternative approach is to use a scale invariant measure such as the **mean absolute percentage error (MAPE)**\n",
    "\n",
    "The percentage error is given by $p_t = 100e_t/y_t$ where $e_t$ is the error in predicting $y_t$.  Therefore, MAPE = $mean(|p_t|)$. A limitation of MAPE is that it is inflated when the denominator is small relative to the absolute forecast error (such in the case of outliers or extreme unexpected events). It also penalises negative errors more than positive errors.  A consequence of this property is that MAPE can lead to selecting a model that tends to under forecast.  The two following examples illustrate the issue. $$APE_{1} = \\left| \\frac{y_t - \\hat{y_t}}{y_t} \\right|= \\left| \\frac{150 - 100}{150} \\right| = \\frac{50}{150} = 33.33\\%$$  \n",
    "\n",
    "$$APE_{2} = \\left| \\frac{100 - 150}{100} \\right| = \\frac{50}{100} = 50\\%$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b11b423",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "- Write a function to compute the MAPE\n",
    "- Calculate the in-sample RMSE and MAPE for the last twelve months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d90d5939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    '''\n",
    "    MAPE\n",
    "\n",
    "    Parameters:\n",
    "    --------\n",
    "    y_true -- np.array actual observations from time series\n",
    "    y_pred -- the predictions to evaluate\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float, scalar value representing the MAPE (0-100)\n",
    "    '''\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf103dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code calculating the RMSE and MAPE for the Naive and Seasonal Naive models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd062799",
   "metadata": {},
   "source": [
    "# 3. ARIMA Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96179daf",
   "metadata": {},
   "source": [
    "Let's try an ARIMA model at last. Using a classical statsitical approach you would select the order of model which produces a model with acceptable residual plots (remember the first part of linear regression module?). Selecting the best model can also be done automatically using packages which do it for you! Wohoo! 🎉\n",
    "\n",
    "The `pmdarima` package is an excellent forecasting library for building ARIMA models.  I recommend it over the options available in the core `statsmodels` package.  It is easier to use and offers an `auto_arima()` function that iteratively searches for a model that minimises the **Akaike Information Criterion (AIC)**\n",
    "\n",
    "* ${\\displaystyle \\mathrm {AIC} \\,=\\,2k-2\\ln({\\hat {L}})}$\n",
    "\n",
    "where $k$ = number of parameters in the model and $\\hat{L}$ is the maximum value of the likelihood function for the model.  A likelihood function measures the 'goodness' of fit of a model to data given a set of parameters.  \n",
    "\n",
    "This looks very complicated at first, but all we need to remember is that the models we are working with are very flexible. This means that we can easily create complex models that overfit. Recall that overfitting is when a model will predict the training data exceptionally well, but will perform poorly on out of sample data.  The form of AIC means that it rewards models that fit the training data well, but also penalises models with high $k$ (complicated models with lots of parameters).  That means that AIC will prefer simpler models - in turn reducing overfitting.  That's a great formula for automatically selecting a good ARIMA forecasting model.\n",
    "\n",
    "Even though modern applications tend to opt for the automatic approach for selecting the ARIMA parameters, we will have a go a trying and comparing how we perform with manual selection. As we discussed above, to train an ARIMA($p$,$d$,$q$) model we need to specify three parameters: $p$ (*AR*), $d$ (*I*) and $q$ (*MA*):\n",
    "- To determine $p$, we look at the *direct* correlations, i.e. the *PACF*, and search for the lag $i$ such that $PACF_i \\approx 0$\n",
    "- To determine $q$, we look at the *full* correlations, i.e. the *ACF*, and search for the lag $i$ such that $ACF_i \\approx 0$\n",
    "\n",
    "Sometimes, we cannot apply directly these guidelines, and we must rely on a more detailed analysis or a grid search to determine the best $p,q$ for the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7101e4b",
   "metadata": {},
   "source": [
    "### Load the dataset & transform it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07578885",
   "metadata": {},
   "source": [
    "ARIMA models can take into account an increasing mean over time, but not the variance. Thus, a log transformation is enough for our purposes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82c5ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload dataset from pmdarima\n",
    "from pmdarima.datasets import load_airpassengers\n",
    "from pmdarima.utils import acf, pacf\n",
    "from pmdarima import ARIMA\n",
    "\n",
    "# Load dataset\n",
    "START_DATE = '1949-01-01'\n",
    "airline = load_airpassengers(as_series=True)\n",
    "\n",
    "# There's no DateTimeIndex from the bundled dataset. So let's add one.\n",
    "airline.index = pd.date_range(START_DATE, periods=len(airline), freq='MS')\n",
    "\n",
    "# Clean & Adjust data\n",
    "airline_adj = airline / airline.index.days_in_month\n",
    "\n",
    "# Train / Test split\n",
    "train, test = ts_train_test_split(airline_adj, '1960-01-01')\n",
    "\n",
    "# Apply log transformation\n",
    "train_log = np.log(train).dropna()\n",
    "test_log = np.log(test).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6a028a",
   "metadata": {},
   "source": [
    "### 3.1 Estimate $p$, $d$ and $q$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1339e2db",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "- Based on the analysis done on the previous notebook, what would be a plausible value for $d$ ?\n",
    "- Plot the ACF and PACF and determine some plausible values for $p$ and $q$.\n",
    "- Fit the ARIMA model and show the diagnostics using the `plot_diagnostics()` of pmdarima. Describe the different plots. How do you interpret these results ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec6749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the ACF & PACF plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1288f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the ARIMA model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90881a41",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "- Make a figure showing the train set, the test set, the predictions from ARIMA and the confidence intervals\n",
    "- Make a second figure showing the predictions for the next 100 periods\n",
    "- Calculate the RMSE and the MAPE on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7064cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code showing train, test, predictions and confidence interval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f92fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for showing the predictions for 100 periods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eb8a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Your code calculating the RMSE and MAPE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48152ef6",
   "metadata": {},
   "source": [
    "**Conclusions**\n",
    "- What are your conclusions ?\n",
    "- What do you think about the error?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df043d6d",
   "metadata": {},
   "source": [
    "### 3.2 Seasonal ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c03dbf",
   "metadata": {},
   "source": [
    "\n",
    "As we have seen previously, ARMA models are well suited for stationary data. They may even works for data presenting seasonality, however seasonality is not clearly expressed. If data shows seasonality less simple as for the airport's data, calculation complexity and time can explode.\n",
    "\n",
    "**Example** : Data compiling sun activity with a seasonality of 12 +/-1 year would require a minimum of $p=12*12$ to comprehend seasonality of the problem!\n",
    "\n",
    "<span style=\"color: blue\">Here comes SARIMA: Seasonal-ARIMA!</span>\n",
    "\n",
    "The Seasonal ARIMA model, also known as SARIMA, is an extension of the basic ARIMA model that takes into account seasonality in the data. In a SARIMA model, we introduce additional seasonal components to the ARIMA model. The seasonal components mirror the ARIMA components but are applied to the seasonal patterns in the data. So, the SARIMA model has six additional parameters:\n",
    "\n",
    "- Seasonal Autoregressive (SAR) component: This is similar to the AR component but for the seasonal patterns. It represents the relationship between the current value of the time series and its past values at the seasonal frequency.\n",
    "\n",
    "- Seasonal Integrated (SI) component: This is similar to the I component but for the seasonal patterns. It involves differencing the time series at the seasonal frequency to achieve seasonality stationarity.\n",
    "\n",
    "- Seasonal Moving Average (SMA) component: This is similar to the MA component but for the seasonal patterns. It models the relationship between the current value of the time series and past forecast errors at the seasonal frequency.\n",
    "\n",
    "The seasonal frequency $s$ refers to the number of time periods that make up one seasonal cycle, and must also be provided to a SARIMA model. For example, if the time series exhibits a yearly seasonality, the seasonal frequency is 12 (assuming monthly data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd8171c",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "- Using a seasonal decomposition of the dataset, separate it into trend, seasonality and residual parts.\n",
    "- Plot the moving mean and std of the trend and seasonal parts of the data.\n",
    "- What can you conclude about stationarity of seasonal data? Can you then define $D$ and the seasonal frequency $s$ ?\n",
    "- Does intra-seasonal data show specific seasonality ? What is $d$ (0, 1, or 2) ?\n",
    "- Use the same procedure applied previously to a build a SARIMA model and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad8e886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_mean(x, n):\n",
    "    return x.rolling(n).mean()\n",
    "\n",
    "def moving_std(x, n):\n",
    "    return x.rolling(n).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f0c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose the data into a trend and seasonal part\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dd9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF & PACF plots\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e434dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the SARIMA model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1639ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show train, test, predictions and confidence interval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b8a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the RMSE and the MAPE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8def3",
   "metadata": {},
   "source": [
    "**Conclusions**\n",
    "- What are your conclusions with respect to the previous ARIMA model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48044e0",
   "metadata": {},
   "source": [
    "### 3.3 Auto-Arima  🎉"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dd66c0",
   "metadata": {},
   "source": [
    "Auto ARIMA is an automated time series forecasting method that automatically selects the best combination of ARIMA model parameters ($p$, $d$, $q$) based on the data's characteristics. It uses algorithms to iteratively search and evaluate different models to find the most suitable one, saving users from manually tuning the parameters. Auto ARIMA is a powerful tool for efficiently forecasting time series data, especially when dealing with large datasets or when users have limited knowledge of time series modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1b2f4e",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "- Fit an `auto_arima` model and plot the diagnostics.\n",
    "- How does it compare to the best model you found previously ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886cf961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.arima import auto_arima\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70396da1",
   "metadata": {},
   "source": [
    "**Conclusions**\n",
    "- How does `auto_arima` perform with respect to your previous models ? Can you explain why ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f3a031",
   "metadata": {},
   "source": [
    "# 4. Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75c99fc",
   "metadata": {},
   "source": [
    "As you know, cross-validation is a crucial technique in the context of machine learning, designed to assess the predictive performance of these models and mitigate potential pitfalls associated with using limited data. Unlike traditional cross-validation used in other machine learning applications, time series cross-validation takes into account the temporal order of the data, ensuring that does not have any information from the future that it would not otherwise have at the time of making a forecast. \n",
    "By dividing the time series data into multiple subsets and iteratively validating the model's performance against unseen data, cross-validation provides valuable insights into how well the ARIMA model generalizes to different time periods. This rigorous evaluation will allow you to fine-tune model hyperparameters (such as ARIMA's parameters), assess forecast accuracy, and build more robust and reliable time series forecasting models.\n",
    "\n",
    "In the classicial time series literature, time series cross validation is called a **Rolling Forecasting Horizon**. This is explained in details [here](https://robjhyndman.com/hyndsight/tscv/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba27fb7",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "- Split the train data into a train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602ebc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've written a function to help with the cv of the baseline model\n",
    "# THIS FUNCTION IS LOCATED IN THE ./forecast DIRECTORY, ALONG WITH THE NOTEBOOK\n",
    "from forecast.model_selection import time_series_cv\n",
    "\n",
    "# It requires the data to be split into train and validation\n",
    "\n",
    "\n",
    "# Print the sizes to see if it's all good\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948da18a",
   "metadata": {},
   "source": [
    "### 4.1 Cross-Validation with Naive model\n",
    "\n",
    "**TODO**\n",
    "- Perform a cross-validation with the Seasonal Naive model you built previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2456a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "HORIZON = 12  # We plan to make forecasts for the following 12 months\n",
    "PERIOD = 12  # Expected period of seasonality\n",
    "\n",
    "# Define the length of time into the future for which forecasts are to be prepared\n",
    "fh = ForecastingHorizon(test.index, is_relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe42b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "\n",
    "# Call time_series_cv\n",
    "\n",
    "\n",
    "# Show results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8cdb30",
   "metadata": {},
   "source": [
    "### 4.2 Cross-Validation with ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbca360",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "- Perform a cross-validation with your best ARIMA model. \n",
    "- Try different step values in the RollingForecastCV. What's the difference ?\n",
    "- In each case, what's the size of the the data the model is trained on ?\n",
    "- How many splits did we get ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad053f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.arima import ARIMA\n",
    "from pmdarima.model_selection import RollingForecastCV, cross_val_score\n",
    "\n",
    "# Manually create the arima model\n",
    "\n",
    "\n",
    "# Create a RollingForecastCV instance\n",
    "\n",
    "\n",
    "# Call cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c6158a",
   "metadata": {},
   "source": [
    "# 5. The Final Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76139220",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "- Following the same methodology as above, produce your final forecast with your best model!\n",
    "- Plot the predictions of the **Naive**, **Seasonal Naive**, and your best **SARIMA** model along with the test data, and compare they performances!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c80c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sktime.utils.plotting import plot_series\n",
    "from sktime.datasets import load_airline\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "\n",
    "from pmdarima.utils import acf, pacf\n",
    "from pmdarima import ARIMA\n",
    "\n",
    "airline = load_airline().dropna()\n",
    "airline = airline / airline.index.days_in_month\n",
    "train, test = ts_train_test_split(airline, '1960-01-01')\n",
    "train_log = np.log(train).dropna()\n",
    "\n",
    "HORIZON = 12  # We plan to make forecasts for the following 12 months\n",
    "PERIOD = 12  # Expected period of seasonality\n",
    "\n",
    "\n",
    "# Define models\n",
    "\n",
    "\n",
    "# Fit models\n",
    "\n",
    "\n",
    "# Create predictions\n",
    "\n",
    "\n",
    "# Plot predictions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6255549d",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5330bae9",
   "metadata": {},
   "source": [
    "### B1. Write a class with the following methods:\n",
    "* The necessary arguments to split the dataset, fit the auto_arima model etc.\n",
    "* A `fit` method which fits the auto_arima naive using a train dataset\n",
    "* A `get_metrics` method which computes the MAPE for each models using a test dataset\n",
    "* A `predict` method which takes as input a prediction horizon and returns predicted values.\n",
    "* A `plot` method which plots the data, and if available the predictions \n",
    "\n",
    "**Have some time left ?** \n",
    "* Add a method `get_cv` Which performs cross validation\n",
    "* To further automatize the process, add an automatic detection of the period.\n",
    "* Add the possibility to apply a preprocessing of the data before the fit such as a log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db317926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your class here\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91c6ed1a",
   "metadata": {},
   "source": [
    "### B2. Test this method on all the other [11 datasets] of pmdarima\n",
    "\n",
    "https://alkaline-ml.com/pmdarima/modules/classes.html#module-pmdarima.datasets\n",
    "To help you, we provide the list of dataset names and a function to load any of these datasets.\n",
    "\n",
    "On some of them the method does not work well, why? What could be done to improve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c706e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['airpassengers', 'ausbeer', 'austres', 'gasoline', 'heartrate', 'lynx', 'msft', 'sunspots', 'taylor', 'wineind', 'woolyrnq']\n",
    "\n",
    "def get_ds(ds_name, red_factor=None):\n",
    "    '''\n",
    "    Loads a dataset of pmdarima from its ds_name.\n",
    "    '''\n",
    "    ds = getattr(pm.datasets, 'load_'+ds_name)(as_series=True).dropna()\n",
    "    \n",
    "    if red_factor is not None:\n",
    "        assert isinstance(red_factor, int)\n",
    "        ds= ds.groupby(np.arange(ds.index.shape[0])//red_factor).mean()\n",
    "    \n",
    "    return ds.dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c92419d",
   "metadata": {},
   "source": [
    "### B3. Want some more ?\n",
    "\n",
    "Choose your poison: https://data.world/datasets/time-series"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
