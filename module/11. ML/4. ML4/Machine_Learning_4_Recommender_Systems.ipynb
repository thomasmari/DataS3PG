{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 09:32:48.087254: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources** :\n",
    "\n",
    "*Matrix Factorization techniques for Recommender Systems*, Koren (2009)    \n",
    "https://courses.ischool.berkeley.edu/i290-dm/s11/SECURE/Koren_Matrix_Factorization.pdf\n",
    "\n",
    "Hands on Machine Learning with scikit-learn and tensorflow:             \n",
    "https://drive.google.com/file/d/1t0rc3x5YQBgLXVLET6BzR4jn5vzMI_m0/view?usp=sharing\n",
    "\n",
    "The movieLens dataset:                                                \n",
    "https://grouplens.org/datasets/movielens/ \n",
    "\n",
    "Keras Functional API doc :                                            \n",
    "https://keras.io/guides/functional_api/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender systems : collaborative filtering via matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you wonder how Netflix is able to recommend you movies despite it doesn't know anything about you but the ratings you gave to the movies you watched ? This is what we are going to explore during this 3 days machine learning module.\n",
    "\n",
    "First off, let's learn about what are recommender system, collaborative filtering and matrix factorization techniques, which are all very well introduced in Koren's 2009 famous article : *Matrix Factorization techniques for Recommender Systems* : https://courses.ischool.berkeley.edu/i290-dm/s11/SECURE/Koren_Matrix_Factorization.pdf . Read the 4 first pages (up to section *adding biases* included). \n",
    "\n",
    "Through this notebook we are going to re-implement the model described in the pages you read, and apply it to a classic movie ratings dataset coming from the website *movieLens*. To do so, we will use a powerful deep learning python library called *Keras*, that makes it easy to train complex models based on linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this module, we are going to use the movieLens dataset, that contains data from the movie recommending website movielens. The data is a subset of ratings from 0 to 5 given by some users of the website to a subset of movies. You can read more about it here (we are using the latest small dataset) : https://grouplens.org/datasets/movielens/ , and in the *README* file that is in the *data/ml-latest-small/* folder.\n",
    "\n",
    "Load the ratings data from the `ratings.csv` file into a dataframe. The userId and movieId provided in the file don't start from 0, and are not contiguous (i.e. there are missing indexes).\n",
    "\n",
    "Re-index the user and movie ids to indexes going from 0 to `nb_users` and 0 to `nb_movies` respectively, by building two dictionnaries `user_ids_map` and `movie_ids_map` that maps the file ids to your new ids. \n",
    "And finally, split the rows of this dataframe in a random 90%/10% train/test sets.\n",
    "\n",
    "To do so, fill the `get_train_test_sets` function below, and respect the returned objects structures that are described in the docstring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def get_train_test_sets(data_path, train_prop = 0.9):\n",
    "    \"\"\"\n",
    "    Build train and test sets and reindex userIds and MovieIds from 0 with contiguous indexes.\n",
    "    \n",
    "    Input: \n",
    "        data_path : string : the path to the ratings file\n",
    "        train_prop : float : The proportion of the training set \n",
    "    \n",
    "    Output:\n",
    "        train : pandas.DataFrame : A dataframe with columns [userId, movieId, rating, timestamp], where\n",
    "            the userId and movieId value have been replaced with new ids starting at 0. \n",
    "            Contains `train_prop` random entries from the input file.\n",
    "        test : pandas.DataFrame : Same as `train`, contains the 1 - `train_prop` remaining entries.\n",
    "        nb_users : int : Number of unique user ids\n",
    "        nb_movies : int : Number of unique movie ids\n",
    "        user_ids_map : dict : A mapping of original file userId to a new index starting at 0.\n",
    "            Keys are int from the original userId column, values are int of the new indexation.\n",
    "        movie_ids_map : dict : Same as `user_ids_map` for the movieIds.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    nb_users, nb_movies = df['userId'].unique().size,df['movieId'].unique().size\n",
    "    #translation dicts\n",
    "    user_ids_map, movie_ids_map = {},{}  \n",
    "    id_cont=0\n",
    "    ds = df['userId'].unique()\n",
    "    ds.sort()\n",
    "    for id in ds:\n",
    "        user_ids_map[int(id)] = id_cont\n",
    "        id_cont += 1\n",
    "    print(user_ids_map)\n",
    "    id_cont=0\n",
    "    ds = df['movieId'].unique()\n",
    "    ds.sort()\n",
    "    for id in ds:\n",
    "        movie_ids_map[int(id)] = id_cont\n",
    "        id_cont += 1\n",
    "    print(movie_ids_map)\n",
    "    #updating new ids \n",
    "    df['userId'] = df['userId'].map(lambda id : user_ids_map[id])\n",
    "    df['movieId'] = df['movieId'].map(lambda id : movie_ids_map[id])\n",
    "    #train test split\n",
    "    X_train,X_test = train_test_split(df,train_size=train_prop)\n",
    "    train,test = pd.DataFrame(X_train),pd.DataFrame(X_test)\n",
    "\n",
    "    return(train, test, nb_users, nb_movies, user_ids_map, movie_ids_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 26: 25, 27: 26, 28: 27, 29: 28, 30: 29, 31: 30, 32: 31, 33: 32, 34: 33, 35: 34, 36: 35, 37: 36, 38: 37, 39: 38, 40: 39, 41: 40, 42: 41, 43: 42, 44: 43, 45: 44, 46: 45, 47: 46, 48: 47, 49: 48, 50: 49, 51: 50, 52: 51, 53: 52, 54: 53, 55: 54, 56: 55, 57: 56, 58: 57, 59: 58, 60: 59, 61: 60, 62: 61, 63: 62, 64: 63, 65: 64, 66: 65, 67: 66, 68: 67, 69: 68, 70: 69, 71: 70, 72: 71, 73: 72, 74: 73, 75: 74, 76: 75, 77: 76, 78: 77, 79: 78, 80: 79, 81: 80, 82: 81, 83: 82, 84: 83, 85: 84, 86: 85, 87: 86, 88: 87, 89: 88, 90: 89, 91: 90, 92: 91, 93: 92, 94: 93, 95: 94, 96: 95, 97: 96, 98: 97, 99: 98, 100: 99, 101: 100, 102: 101, 103: 102, 104: 103, 105: 104, 106: 105, 107: 106, 108: 107, 109: 108, 110: 109, 111: 110, 112: 111, 113: 112, 114: 113, 115: 114, 116: 115, 117: 116, 118: 117, 119: 118, 120: 119, 121: 120, 122: 121, 123: 122, 124: 123, 125: 124, 126: 125, 127: 126, 128: 127, 129: 128, 130: 129, 131: 130, 132: 131, 133: 132, 134: 133, 135: 134, 136: 135, 137: 136, 138: 137, 139: 138, 140: 139, 141: 140, 142: 141, 143: 142, 144: 143, 145: 144, 146: 145, 147: 146, 148: 147, 149: 148, 150: 149, 151: 150, 152: 151, 153: 152, 154: 153, 155: 154, 156: 155, 157: 156, 158: 157, 159: 158, 160: 159, 161: 160, 162: 161, 163: 162, 164: 163, 165: 164, 166: 165, 167: 166, 168: 167, 169: 168, 170: 169, 171: 170, 172: 171, 173: 172, 174: 173, 175: 174, 176: 175, 177: 176, 178: 177, 179: 178, 180: 179, 181: 180, 182: 181, 183: 182, 184: 183, 185: 184, 186: 185, 187: 186, 188: 187, 189: 188, 190: 189, 191: 190, 192: 191, 193: 192, 194: 193, 195: 194, 196: 195, 197: 196, 198: 197, 199: 198, 200: 199, 201: 200, 202: 201, 203: 202, 204: 203, 205: 204, 206: 205, 207: 206, 208: 207, 209: 208, 210: 209, 211: 210, 212: 211, 213: 212, 214: 213, 215: 214, 216: 215, 217: 216, 218: 217, 219: 218, 220: 219, 221: 220, 222: 221, 223: 222, 224: 223, 225: 224, 226: 225, 227: 226, 228: 227, 229: 228, 230: 229, 231: 230, 232: 231, 233: 232, 234: 233, 235: 234, 236: 235, 237: 236, 238: 237, 239: 238, 240: 239, 241: 240, 242: 241, 243: 242, 244: 243, 245: 244, 246: 245, 247: 246, 248: 247, 249: 248, 250: 249, 251: 250, 252: 251, 253: 252, 254: 253, 255: 254, 256: 255, 257: 256, 258: 257, 259: 258, 260: 259, 261: 260, 262: 261, 263: 262, 264: 263, 265: 264, 266: 265, 267: 266, 268: 267, 269: 268, 270: 269, 271: 270, 272: 271, 273: 272, 274: 273, 275: 274, 276: 275, 277: 276, 278: 277, 279: 278, 280: 279, 281: 280, 282: 281, 283: 282, 284: 283, 285: 284, 286: 285, 287: 286, 288: 287, 289: 288, 290: 289, 291: 290, 292: 291, 293: 292, 294: 293, 295: 294, 296: 295, 297: 296, 298: 297, 299: 298, 300: 299, 301: 300, 302: 301, 303: 302, 304: 303, 305: 304, 306: 305, 307: 306, 308: 307, 309: 308, 310: 309, 311: 310, 312: 311, 313: 312, 314: 313, 315: 314, 316: 315, 317: 316, 318: 317, 319: 318, 320: 319, 321: 320, 322: 321, 323: 322, 324: 323, 325: 324, 326: 325, 327: 326, 328: 327, 329: 328, 330: 329, 331: 330, 332: 331, 333: 332, 334: 333, 335: 334, 336: 335, 337: 336, 338: 337, 339: 338, 340: 339, 341: 340, 342: 341, 343: 342, 344: 343, 345: 344, 346: 345, 347: 346, 348: 347, 349: 348, 350: 349, 351: 350, 352: 351, 353: 352, 354: 353, 355: 354, 356: 355, 357: 356, 358: 357, 359: 358, 360: 359, 361: 360, 362: 361, 363: 362, 364: 363, 365: 364, 366: 365, 367: 366, 368: 367, 369: 368, 370: 369, 371: 370, 372: 371, 373: 372, 374: 373, 375: 374, 376: 375, 377: 376, 378: 377, 379: 378, 380: 379, 381: 380, 382: 381, 383: 382, 384: 383, 385: 384, 386: 385, 387: 386, 388: 387, 389: 388, 390: 389, 391: 390, 392: 391, 393: 392, 394: 393, 395: 394, 396: 395, 397: 396, 398: 397, 399: 398, 400: 399, 401: 400, 402: 401, 403: 402, 404: 403, 405: 404, 406: 405, 407: 406, 408: 407, 409: 408, 410: 409, 411: 410, 412: 411, 413: 412, 414: 413, 415: 414, 416: 415, 417: 416, 418: 417, 419: 418, 420: 419, 421: 420, 422: 421, 423: 422, 424: 423, 425: 424, 426: 425, 427: 426, 428: 427, 429: 428, 430: 429, 431: 430, 432: 431, 433: 432, 434: 433, 435: 434, 436: 435, 437: 436, 438: 437, 439: 438, 440: 439, 441: 440, 442: 441, 443: 442, 444: 443, 445: 444, 446: 445, 447: 446, 448: 447, 449: 448, 450: 449, 451: 450, 452: 451, 453: 452, 454: 453, 455: 454, 456: 455, 457: 456, 458: 457, 459: 458, 460: 459, 461: 460, 462: 461, 463: 462, 464: 463, 465: 464, 466: 465, 467: 466, 468: 467, 469: 468, 470: 469, 471: 470, 472: 471, 473: 472, 474: 473, 475: 474, 476: 475, 477: 476, 478: 477, 479: 478, 480: 479, 481: 480, 482: 481, 483: 482, 484: 483, 485: 484, 486: 485, 487: 486, 488: 487, 489: 488, 490: 489, 491: 490, 492: 491, 493: 492, 494: 493, 495: 494, 496: 495, 497: 496, 498: 497, 499: 498, 500: 499, 501: 500, 502: 501, 503: 502, 504: 503, 505: 504, 506: 505, 507: 506, 508: 507, 509: 508, 510: 509, 511: 510, 512: 511, 513: 512, 514: 513, 515: 514, 516: 515, 517: 516, 518: 517, 519: 518, 520: 519, 521: 520, 522: 521, 523: 522, 524: 523, 525: 524, 526: 525, 527: 526, 528: 527, 529: 528, 530: 529, 531: 530, 532: 531, 533: 532, 534: 533, 535: 534, 536: 535, 537: 536, 538: 537, 539: 538, 540: 539, 541: 540, 542: 541, 543: 542, 544: 543, 545: 544, 546: 545, 547: 546, 548: 547, 549: 548, 550: 549, 551: 550, 552: 551, 553: 552, 554: 553, 555: 554, 556: 555, 557: 556, 558: 557, 559: 558, 560: 559, 561: 560, 562: 561, 563: 562, 564: 563, 565: 564, 566: 565, 567: 566, 568: 567, 569: 568, 570: 569, 571: 570, 572: 571, 573: 572, 574: 573, 575: 574, 576: 575, 577: 576, 578: 577, 579: 578, 580: 579, 581: 580, 582: 581, 583: 582, 584: 583, 585: 584, 586: 585, 587: 586, 588: 587, 589: 588, 590: 589, 591: 590, 592: 591, 593: 592, 594: 593, 595: 594, 596: 595, 597: 596, 598: 597, 599: 598, 600: 599, 601: 600, 602: 601, 603: 602, 604: 603, 605: 604, 606: 605, 607: 606, 608: 607, 609: 608, 610: 609}\n",
      "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 26: 25, 27: 26, 28: 27, 29: 28, 30: 29, 31: 30, 32: 31, 34: 32, 36: 33, 38: 34, 39: 35, 40: 36, 41: 37, 42: 38, 43: 39, 44: 40, 45: 41, 46: 42, 47: 43, 48: 44, 49: 45, 50: 46, 52: 47, 53: 48, 54: 49, 55: 50, 57: 51, 58: 52, 60: 53, 61: 54, 62: 55, 63: 56, 64: 57, 65: 58, 66: 59, 68: 60, 69: 61, 70: 62, 71: 63, 72: 64, 73: 65, 74: 66, 75: 67, 76: 68, 77: 69, 78: 70, 79: 71, 80: 72, 81: 73, 82: 74, 83: 75, 85: 76, 86: 77, 87: 78, 88: 79, 89: 80, 92: 81, 93: 82, 94: 83, 95: 84, 96: 85, 97: 86, 99: 87, 100: 88, 101: 89, 102: 90, 103: 91, 104: 92, 105: 93, 106: 94, 107: 95, 108: 96, 110: 97, 111: 98, 112: 99, 113: 100, 116: 101, 117: 102, 118: 103, 119: 104, 121: 105, 122: 106, 123: 107, 125: 108, 126: 109, 128: 110, 129: 111, 132: 112, 135: 113, 137: 114, 140: 115, 141: 116, 144: 117, 145: 118, 146: 119, 147: 120, 148: 121, 149: 122, 150: 123, 151: 124, 152: 125, 153: 126, 154: 127, 155: 128, 156: 129, 157: 130, 158: 131, 159: 132, 160: 133, 161: 134, 162: 135, 163: 136, 164: 137, 165: 138, 166: 139, 168: 140, 169: 141, 170: 142, 171: 143, 172: 144, 173: 145, 174: 146, 175: 147, 176: 148, 177: 149, 178: 150, 179: 151, 180: 152, 181: 153, 183: 154, 184: 155, 185: 156, 186: 157, 187: 158, 188: 159, 189: 160, 190: 161, 191: 162, 193: 163, 194: 164, 195: 165, 196: 166, 198: 167, 199: 168, 201: 169, 202: 170, 203: 171, 204: 172, 205: 173, 206: 174, 207: 175, 208: 176, 209: 177, 210: 178, 211: 179, 212: 180, 213: 181, 214: 182, 215: 183, 216: 184, 217: 185, 218: 186, 219: 187, 220: 188, 222: 189, 223: 190, 224: 191, 225: 192, 227: 193, 228: 194, 229: 195, 230: 196, 231: 197, 232: 198, 233: 199, 234: 200, 235: 201, 236: 202, 237: 203, 238: 204, 239: 205, 240: 206, 241: 207, 242: 208, 243: 209, 246: 210, 247: 211, 248: 212, 249: 213, 250: 214, 251: 215, 252: 216, 253: 217, 254: 218, 255: 219, 256: 220, 257: 221, 258: 222, 259: 223, 260: 224, 261: 225, 262: 226, 263: 227, 265: 228, 266: 229, 267: 230, 269: 231, 270: 232, 271: 233, 272: 234, 273: 235, 274: 236, 275: 237, 276: 238, 277: 239, 278: 240, 279: 241, 280: 242, 281: 243, 282: 244, 283: 245, 284: 246, 285: 247, 287: 248, 288: 249, 289: 250, 290: 251, 291: 252, 292: 253, 293: 254, 294: 255, 295: 256, 296: 257, 298: 258, 299: 259, 300: 260, 301: 261, 302: 262, 303: 263, 304: 264, 305: 265, 306: 266, 307: 267, 308: 268, 310: 269, 311: 270, 312: 271, 313: 272, 314: 273, 315: 274, 316: 275, 317: 276, 318: 277, 319: 278, 320: 279, 321: 280, 322: 281, 324: 282, 325: 283, 326: 284, 327: 285, 328: 286, 329: 287, 330: 288, 331: 289, 332: 290, 333: 291, 334: 292, 335: 293, 336: 294, 337: 295, 338: 296, 339: 297, 340: 298, 341: 299, 342: 300, 343: 301, 344: 302, 345: 303, 346: 304, 347: 305, 348: 306, 349: 307, 350: 308, 351: 309, 352: 310, 353: 311, 354: 312, 355: 313, 356: 314, 357: 315, 358: 316, 359: 317, 360: 318, 361: 319, 362: 320, 363: 321, 364: 322, 365: 323, 366: 324, 367: 325, 368: 326, 369: 327, 370: 328, 371: 329, 372: 330, 373: 331, 374: 332, 376: 333, 377: 334, 378: 335, 379: 336, 380: 337, 381: 338, 382: 339, 383: 340, 384: 341, 385: 342, 386: 343, 387: 344, 388: 345, 389: 346, 390: 347, 391: 348, 393: 349, 405: 350, 406: 351, 407: 352, 408: 353, 409: 354, 410: 355, 412: 356, 413: 357, 414: 358, 415: 359, 416: 360, 417: 361, 418: 362, 419: 363, 420: 364, 421: 365, 422: 366, 423: 367, 424: 368, 425: 369, 426: 370, 427: 371, 428: 372, 429: 373, 430: 374, 431: 375, 432: 376, 433: 377, 434: 378, 435: 379, 436: 380, 437: 381, 438: 382, 440: 383, 441: 384, 442: 385, 444: 386, 445: 387, 446: 388, 448: 389, 449: 390, 450: 391, 451: 392, 452: 393, 453: 394, 454: 395, 455: 396, 456: 397, 457: 398, 458: 399, 459: 400, 460: 401, 461: 402, 464: 403, 466: 404, 467: 405, 468: 406, 469: 407, 470: 408, 471: 409, 472: 410, 473: 411, 474: 412, 475: 413, 476: 414, 477: 415, 478: 416, 479: 417, 480: 418, 481: 419, 482: 420, 484: 421, 485: 422, 486: 423, 487: 424, 488: 425, 489: 426, 490: 427, 491: 428, 492: 429, 493: 430, 494: 431, 495: 432, 496: 433, 497: 434, 499: 435, 500: 436, 501: 437, 502: 438, 504: 439, 505: 440, 506: 441, 507: 442, 508: 443, 509: 444, 510: 445, 511: 446, 512: 447, 513: 448, 514: 449, 515: 450, 516: 451, 517: 452, 518: 453, 519: 454, 520: 455, 521: 456, 522: 457, 523: 458, 524: 459, 526: 460, 527: 461, 528: 462, 529: 463, 531: 464, 532: 465, 533: 466, 534: 467, 535: 468, 536: 469, 537: 470, 538: 471, 539: 472, 540: 473, 541: 474, 542: 475, 543: 476, 544: 477, 546: 478, 547: 479, 548: 480, 549: 481, 550: 482, 551: 483, 552: 484, 553: 485, 555: 486, 556: 487, 558: 488, 562: 489, 563: 490, 564: 491, 567: 492, 568: 493, 569: 494, 573: 495, 574: 496, 575: 497, 577: 498, 579: 499, 580: 500, 581: 501, 583: 502, 585: 503, 586: 504, 587: 505, 588: 506, 589: 507, 590: 508, 592: 509, 593: 510, 594: 511, 595: 512, 596: 513, 597: 514, 599: 515, 600: 516, 602: 517, 605: 518, 606: 519, 608: 520, 609: 521, 610: 522, 611: 523, 612: 524, 613: 525, 615: 526, 616: 527, 617: 528, 618: 529, 619: 530, 626: 531, 627: 532, 628: 533, 631: 534, 632: 535, 633: 536, 634: 537, 635: 538, 636: 539, 637: 540, 638: 541, 639: 542, 640: 543, 645: 544, 647: 545, 648: 546, 649: 547, 650: 548, 653: 549, 656: 550, 661: 551, 662: 552, 663: 553, 665: 554, 667: 555, 668: 556, 670: 557, 671: 558, 673: 559, 674: 560, 678: 561, 679: 562, 680: 563, 685: 564, 688: 565, 691: 566, 692: 567, 694: 568, 695: 569, 697: 570, 698: 571, 700: 572, 703: 573, 704: 574, 706: 575, 707: 576, 708: 577, 709: 578, 710: 579, 711: 580, 714: 581, 715: 582, 718: 583, 719: 584, 720: 585, 722: 586, 724: 587, 725: 588, 726: 589, 728: 590, 731: 591, 733: 592, 735: 593, 736: 594, 737: 595, 741: 596, 742: 597, 743: 598, 745: 599, 747: 600, 748: 601, 750: 602, 757: 603, 759: 604, 760: 605, 761: 606, 762: 607, 764: 608, 765: 609, 766: 610, 773: 611, 775: 612, 778: 613, 779: 614, 780: 615, 781: 616, 782: 617, 783: 618, 784: 619, 785: 620, 786: 621, 788: 622, 790: 623, 791: 624, 795: 625, 798: 626, 799: 627, 800: 628, 801: 629, 802: 630, 803: 631, 804: 632, 805: 633, 806: 634, 808: 635, 809: 636, 810: 637, 813: 638, 818: 639, 823: 640, 824: 641, 828: 642, 829: 643, 830: 644, 832: 645, 833: 646, 835: 647, 836: 648, 837: 649, 838: 650, 839: 651, 840: 652, 841: 653, 842: 654, 848: 655, 849: 656, 851: 657, 852: 658, 858: 659, 861: 660, 866: 661, 867: 662, 869: 663, 870: 664, 875: 665, 876: 666, 879: 667, 880: 668, 881: 669, 882: 670, 885: 671, 886: 672, 888: 673, 889: 674, 891: 675, 892: 676, 893: 677, 896: 678, 897: 679, 898: 680, 899: 681, 900: 682, 901: 683, 902: 684, 903: 685, 904: 686, 905: 687, 906: 688, 907: 689, 908: 690, 909: 691, 910: 692, 911: 693, 912: 694, 913: 695, 914: 696, 915: 697, 916: 698, 917: 699, 918: 700, 919: 701, 920: 702, 921: 703, 922: 704, 923: 705, 924: 706, 926: 707, 927: 708, 928: 709, 929: 710, 930: 711, 931: 712, 932: 713, 933: 714, 934: 715, 935: 716, 936: 717, 937: 718, 938: 719, 940: 720, 941: 721, 942: 722, 943: 723, 944: 724, 945: 725, 946: 726, 947: 727, 948: 728, 949: 729, 950: 730, 951: 731, 952: 732, 953: 733, 954: 734, 955: 735, 956: 736, 959: 737, 961: 738, 963: 739, 965: 740, 968: 741, 969: 742, 970: 743, 971: 744, 973: 745, 976: 746, 979: 747, 981: 748, 982: 749, 984: 750, 986: 751, 987: 752, 988: 753, 990: 754, 991: 755, 993: 756, 994: 757, 996: 758, 998: 759, 999: 760, 1003: 761, 1004: 762, 1005: 763, 1006: 764, 1007: 765, 1008: 766, 1009: 767, 1010: 768, 1011: 769, 1012: 770, 1013: 771, 1014: 772, 1015: 773, 1016: 774, 1017: 775, 1018: 776, 1019: 777, 1020: 778, 1021: 779, 1022: 780, 1023: 781, 1024: 782, 1025: 783, 1027: 784, 1028: 785, 1029: 786, 1030: 787, 1031: 788, 1032: 789, 1033: 790, 1034: 791, 1035: 792, 1036: 793, 1037: 794, 1040: 795, 1041: 796, 1042: 797, 1043: 798, 1046: 799, 1047: 800, 1049: 801, 1050: 802, 1051: 803, 1053: 804, 1054: 805, 1055: 806, 1056: 807, 1057: 808, 1059: 809, 1060: 810, 1061: 811, 1064: 812, 1066: 813, 1068: 814, 1073: 815, 1077: 816, 1078: 817, 1079: 818, 1080: 819, 1081: 820, 1082: 821, 1083: 822, 1084: 823, 1085: 824, 1086: 825, 1088: 826, 1089: 827, 1090: 828, 1091: 829, 1092: 830, 1093: 831, 1094: 832, 1095: 833, 1096: 834, 1097: 835, 1099: 836, 1100: 837, 1101: 838, 1103: 839, 1104: 840, 1105: 841, 1107: 842, 1111: 843, 1112: 844, 1114: 845, 1116: 846, 1117: 847, 1119: 848, 1120: 849, 1121: 850, 1123: 851, 1124: 852, 1125: 853, 1126: 854, 1127: 855, 1128: 856, 1129: 857, 1130: 858, 1131: 859, 1132: 860, 1135: 861, 1136: 862, 1137: 863, 1140: 864, 1144: 865, 1147: 866, 1148: 867, 1150: 868, 1151: 869, 1156: 870, 1161: 871, 1162: 872, 1163: 873, 1167: 874, 1170: 875, 1171: 876, 1172: 877, 1173: 878, 1175: 879, 1176: 880, 1177: 881, 1178: 882, 1179: 883, 1180: 884, 1183: 885, 1184: 886, 1185: 887, 1186: 888, 1187: 889, 1188: 890, 1189: 891, 1190: 892, 1191: 893, 1192: 894, 1193: 895, 1194: 896, 1196: 897, 1197: 898, 1198: 899, 1199: 900, 1200: 901, 1201: 902, 1202: 903, 1203: 904, 1204: 905, 1206: 906, 1207: 907, 1208: 908, 1209: 909, 1210: 910, 1211: 911, 1212: 912, 1213: 913, 1214: 914, 1215: 915, 1216: 916, 1217: 917, 1218: 918, 1219: 919, 1220: 920, 1221: 921, 1222: 922, 1223: 923, 1224: 924, 1225: 925, 1226: 926, 1227: 927, 1228: 928, 1230: 929, 1231: 930, 1232: 931, 1233: 932, 1234: 933, 1235: 934, 1236: 935, 1237: 936, 1238: 937, 1240: 938, 1241: 939, 1242: 940, 1243: 941, 1244: 942, 1245: 943, 1246: 944, 1247: 945, 1248: 946, 1249: 947, 1250: 948, 1251: 949, 1252: 950, 1253: 951, 1254: 952, 1255: 953, 1256: 954, 1257: 955, 1258: 956, 1259: 957, 1260: 958, 1261: 959, 1262: 960, 1263: 961, 1264: 962, 1265: 963, 1266: 964, 1267: 965, 1268: 966, 1269: 967, 1270: 968, 1271: 969, 1272: 970, 1273: 971, 1274: 972, 1275: 973, 1276: 974, 1277: 975, 1278: 976, 1279: 977, 1280: 978, 1281: 979, 1282: 980, 1283: 981, 1284: 982, 1285: 983, 1286: 984, 1287: 985, 1288: 986, 1289: 987, 1290: 988, 1291: 989, 1292: 990, 1293: 991, 1295: 992, 1296: 993, 1297: 994, 1298: 995, 1299: 996, 1300: 997, 1301: 998, 1302: 999, 1303: 1000, 1304: 1001, 1305: 1002, 1306: 1003, 1307: 1004, 1310: 1005, 1312: 1006, 1318: 1007, 1320: 1008, 1321: 1009, 1322: 1010, 1323: 1011, 1324: 1012, 1325: 1013, 1326: 1014, 1327: 1015, 1328: 1016, 1329: 1017, 1330: 1018, 1331: 1019, 1332: 1020, 1333: 1021, 1334: 1022, 1335: 1023, 1336: 1024, 1337: 1025, 1339: 1026, 1340: 1027, 1341: 1028, 1342: 1029, 1343: 1030, 1344: 1031, 1345: 1032, 1346: 1033, 1347: 1034, 1348: 1035, 1349: 1036, 1350: 1037, 1351: 1038, 1352: 1039, 1353: 1040, 1354: 1041, 1355: 1042, 1356: 1043, 1357: 1044, 1358: 1045, 1359: 1046, 1361: 1047, 1363: 1048, 1365: 1049, 1366: 1050, 1367: 1051, 1370: 1052, 1371: 1053, 1372: 1054, 1373: 1055, 1374: 1056, 1375: 1057, 1376: 1058, 1377: 1059, 1378: 1060, 1379: 1061, 1380: 1062, 1381: 1063, 1382: 1064, 1385: 1065, 1387: 1066, 1388: 1067, 1389: 1068, 1390: 1069, 1391: 1070, 1392: 1071, 1393: 1072, 1394: 1073, 1395: 1074, 1396: 1075, 1397: 1076, 1398: 1077, 1399: 1078, 1401: 1079, 1405: 1080, 1406: 1081, 1407: 1082, 1408: 1083, 1409: 1084, 1411: 1085, 1412: 1086, 1413: 1087, 1414: 1088, 1415: 1089, 1416: 1090, 1417: 1091, 1419: 1092, 1422: 1093, 1423: 1094, 1425: 1095, 1426: 1096, 1427: 1097, 1428: 1098, 1429: 1099, 1430: 1100, 1431: 1101, 1432: 1102, 1437: 1103, 1438: 1104, 1439: 1105, 1440: 1106, 1441: 1107, 1442: 1108, 1445: 1109, 1446: 1110, 1447: 1111, 1449: 1112, 1453: 1113, 1454: 1114, 1456: 1115, 1457: 1116, 1458: 1117, 1459: 1118, 1460: 1119, 1461: 1120, 1464: 1121, 1465: 1122, 1466: 1123, 1468: 1124, 1473: 1125, 1474: 1126, 1475: 1127, 1476: 1128, 1477: 1129, 1479: 1130, 1480: 1131, 1483: 1132, 1484: 1133, 1485: 1134, 1487: 1135, 1488: 1136, 1489: 1137, 1490: 1138, 1493: 1139, 1495: 1140, 1496: 1141, 1497: 1142, 1498: 1143, 1499: 1144, 1500: 1145, 1502: 1146, 1503: 1147, 1507: 1148, 1513: 1149, 1514: 1150, 1515: 1151, 1516: 1152, 1517: 1153, 1518: 1154, 1519: 1155, 1526: 1156, 1527: 1157, 1529: 1158, 1533: 1159, 1537: 1160, 1541: 1161, 1542: 1162, 1544: 1163, 1545: 1164, 1546: 1165, 1547: 1166, 1549: 1167, 1550: 1168, 1551: 1169, 1552: 1170, 1554: 1171, 1556: 1172, 1562: 1173, 1564: 1174, 1565: 1175, 1566: 1176, 1569: 1177, 1571: 1178, 1572: 1179, 1573: 1180, 1574: 1181, 1580: 1182, 1581: 1183, 1582: 1184, 1583: 1185, 1584: 1186, 1585: 1187, 1586: 1188, 1587: 1189, 1588: 1190, 1589: 1191, 1590: 1192, 1591: 1193, 1592: 1194, 1593: 1195, 1594: 1196, 1596: 1197, 1597: 1198, 1598: 1199, 1599: 1200, 1600: 1201, 1601: 1202, 1602: 1203, 1603: 1204, 1604: 1205, 1605: 1206, 1606: 1207, 1608: 1208, 1609: 1209, 1610: 1210, 1611: 1211, 1612: 1212, 1613: 1213, 1614: 1214, 1615: 1215, 1616: 1216, 1617: 1217, 1619: 1218, 1620: 1219, 1621: 1220, 1623: 1221, 1624: 1222, 1625: 1223, 1626: 1224, 1627: 1225, 1629: 1226, 1631: 1227, 1633: 1228, 1635: 1229, 1639: 1230, 1641: 1231, 1642: 1232, 1643: 1233, 1644: 1234, 1645: 1235, 1646: 1236, 1647: 1237, 1648: 1238, 1649: 1239, 1650: 1240, 1652: 1241, 1653: 1242, 1654: 1243, 1655: 1244, 1656: 1245, 1658: 1246, 1659: 1247, 1660: 1248, 1661: 1249, 1662: 1250, 1663: 1251, 1665: 1252, 1667: 1253, 1670: 1254, 1671: 1255, 1672: 1256, 1673: 1257, 1674: 1258, 1675: 1259, 1676: 1260, 1677: 1261, 1678: 1262, 1679: 1263, 1680: 1264, 1681: 1265, 1682: 1266, 1683: 1267, 1684: 1268, 1685: 1269, 1686: 1270, 1687: 1271, 1688: 1272, 1689: 1273, 1690: 1274, 1693: 1275, 1694: 1276, 1695: 1277, 1696: 1278, 1699: 1279, 1701: 1280, 1702: 1281, 1703: 1282, 1704: 1283, 1707: 1284, 1711: 1285, 1713: 1286, 1717: 1287, 1718: 1288, 1719: 1289, 1721: 1290, 1722: 1291, 1726: 1292, 1727: 1293, 1729: 1294, 1730: 1295, 1731: 1296, 1732: 1297, 1733: 1298, 1734: 1299, 1735: 1300, 1739: 1301, 1746: 1302, 1747: 1303, 1748: 1304, 1752: 1305, 1753: 1306, 1754: 1307, 1755: 1308, 1757: 1309, 1759: 1310, 1760: 1311, 1762: 1312, 1767: 1313, 1769: 1314, 1770: 1315, 1771: 1316, 1772: 1317, 1777: 1318, 1779: 1319, 1783: 1320, 1784: 1321, 1785: 1322, 1791: 1323, 1792: 1324, 1793: 1325, 1794: 1326, 1797: 1327, 1798: 1328, 1799: 1329, 1801: 1330, 1804: 1331, 1805: 1332, 1806: 1333, 1807: 1334, 1809: 1335, 1810: 1336, 1812: 1337, 1816: 1338, 1821: 1339, 1824: 1340, 1825: 1341, 1826: 1342, 1827: 1343, 1829: 1344, 1831: 1345, 1833: 1346, 1834: 1347, 1835: 1348, 1836: 1349, 1837: 1350, 1839: 1351, 1840: 1352, 1841: 1353, 1844: 1354, 1845: 1355, 1848: 1356, 1855: 1357, 1856: 1358, 1857: 1359, 1858: 1360, 1859: 1361, 1860: 1362, 1862: 1363, 1863: 1364, 1866: 1365, 1867: 1366, 1870: 1367, 1873: 1368, 1875: 1369, 1876: 1370, 1881: 1371, 1882: 1372, 1883: 1373, 1884: 1374, 1885: 1375, 1887: 1376, 1888: 1377, 1889: 1378, 1891: 1379, 1892: 1380, 1893: 1381, 1894: 1382, 1895: 1383, 1897: 1384, 1900: 1385, 1902: 1386, 1904: 1387, 1906: 1388, 1907: 1389, 1909: 1390, 1910: 1391, 1911: 1392, 1912: 1393, 1913: 1394, 1914: 1395, 1916: 1396, 1917: 1397, 1918: 1398, 1919: 1399, 1920: 1400, 1921: 1401, 1922: 1402, 1923: 1403, 1924: 1404, 1926: 1405, 1927: 1406, 1928: 1407, 1929: 1408, 1931: 1409, 1932: 1410, 1933: 1411, 1934: 1412, 1936: 1413, 1937: 1414, 1938: 1415, 1939: 1416, 1940: 1417, 1941: 1418, 1942: 1419, 1944: 1420, 1945: 1421, 1946: 1422, 1947: 1423, 1948: 1424, 1949: 1425, 1950: 1426, 1951: 1427, 1952: 1428, 1953: 1429, 1954: 1430, 1955: 1431, 1956: 1432, 1957: 1433, 1958: 1434, 1959: 1435, 1960: 1436, 1961: 1437, 1962: 1438, 1963: 1439, 1964: 1440, 1965: 1441, 1966: 1442, 1967: 1443, 1968: 1444, 1969: 1445, 1970: 1446, 1971: 1447, 1972: 1448, 1973: 1449, 1974: 1450, 1975: 1451, 1976: 1452, 1977: 1453, 1978: 1454, 1979: 1455, 1980: 1456, 1981: 1457, 1982: 1458, 1983: 1459, 1984: 1460, 1985: 1461, 1986: 1462, 1987: 1463, 1990: 1464, 1991: 1465, 1992: 1466, 1993: 1467, 1994: 1468, 1995: 1469, 1996: 1470, 1997: 1471, 1998: 1472, 1999: 1473, 2000: 1474, 2001: 1475, 2002: 1476, 2003: 1477, 2004: 1478, 2005: 1479, 2006: 1480, 2007: 1481, 2008: 1482, 2009: 1483, 2010: 1484, 2011: 1485, 2012: 1486, 2013: 1487, 2014: 1488, 2015: 1489, 2016: 1490, 2017: 1491, 2018: 1492, 2019: 1493, 2020: 1494, 2021: 1495, 2022: 1496, 2023: 1497, 2024: 1498, 2025: 1499, 2026: 1500, 2027: 1501, 2028: 1502, 2032: 1503, 2033: 1504, 2034: 1505, 2035: 1506, 2036: 1507, 2037: 1508, 2038: 1509, 2040: 1510, 2041: 1511, 2042: 1512, 2043: 1513, 2044: 1514, 2046: 1515, 2048: 1516, 2050: 1517, 2051: 1518, 2052: 1519, 2053: 1520, 2054: 1521, 2055: 1522, 2056: 1523, 2057: 1524, 2058: 1525, 2059: 1526, 2060: 1527, 2064: 1528, 2065: 1529, 2066: 1530, 2067: 1531, 2068: 1532, 2069: 1533, 2070: 1534, 2071: 1535, 2072: 1536, 2073: 1537, 2074: 1538, 2075: 1539, 2076: 1540, 2077: 1541, 2078: 1542, 2080: 1543, 2081: 1544, 2082: 1545, 2083: 1546, 2084: 1547, 2085: 1548, 2087: 1549, 2088: 1550, 2089: 1551, 2090: 1552, 2091: 1553, 2092: 1554, 2093: 1555, 2094: 1556, 2095: 1557, 2096: 1558, 2097: 1559, 2098: 1560, 2099: 1561, 2100: 1562, 2102: 1563, 2103: 1564, 2104: 1565, 2105: 1566, 2106: 1567, 2107: 1568, 2108: 1569, 2109: 1570, 2110: 1571, 2111: 1572, 2112: 1573, 2114: 1574, 2115: 1575, 2116: 1576, 2117: 1577, 2118: 1578, 2119: 1579, 2120: 1580, 2121: 1581, 2122: 1582, 2123: 1583, 2124: 1584, 2125: 1585, 2126: 1586, 2130: 1587, 2131: 1588, 2132: 1589, 2133: 1590, 2134: 1591, 2135: 1592, 2136: 1593, 2137: 1594, 2138: 1595, 2139: 1596, 2140: 1597, 2141: 1598, 2142: 1599, 2143: 1600, 2144: 1601, 2145: 1602, 2146: 1603, 2147: 1604, 2148: 1605, 2149: 1606, 2150: 1607, 2151: 1608, 2152: 1609, 2153: 1610, 2154: 1611, 2155: 1612, 2156: 1613, 2159: 1614, 2160: 1615, 2161: 1616, 2162: 1617, 2163: 1618, 2164: 1619, 2165: 1620, 2166: 1621, 2167: 1622, 2169: 1623, 2170: 1624, 2171: 1625, 2172: 1626, 2174: 1627, 2175: 1628, 2176: 1629, 2177: 1630, 2178: 1631, 2179: 1632, 2180: 1633, 2181: 1634, 2182: 1635, 2183: 1636, 2184: 1637, 2185: 1638, 2186: 1639, 2187: 1640, 2188: 1641, 2190: 1642, 2193: 1643, 2194: 1644, 2195: 1645, 2196: 1646, 2201: 1647, 2202: 1648, 2203: 1649, 2204: 1650, 2205: 1651, 2206: 1652, 2207: 1653, 2208: 1654, 2210: 1655, 2211: 1656, 2212: 1657, 2226: 1658, 2227: 1659, 2231: 1660, 2232: 1661, 2236: 1662, 2239: 1663, 2240: 1664, 2241: 1665, 2243: 1666, 2244: 1667, 2245: 1668, 2247: 1669, 2248: 1670, 2249: 1671, 2252: 1672, 2253: 1673, 2255: 1674, 2256: 1675, 2257: 1676, 2259: 1677, 2260: 1678, 2261: 1679, 2262: 1680, 2263: 1681, 2264: 1682, 2265: 1683, 2266: 1684, 2267: 1685, 2268: 1686, 2269: 1687, 2271: 1688, 2272: 1689, 2273: 1690, 2275: 1691, 2278: 1692, 2279: 1693, 2280: 1694, 2281: 1695, 2282: 1696, 2283: 1697, 2286: 1698, 2287: 1699, 2288: 1700, 2289: 1701, 2290: 1702, 2291: 1703, 2292: 1704, 2294: 1705, 2295: 1706, 2296: 1707, 2297: 1708, 2298: 1709, 2300: 1710, 2301: 1711, 2302: 1712, 2303: 1713, 2304: 1714, 2306: 1715, 2307: 1716, 2310: 1717, 2311: 1718, 2312: 1719, 2313: 1720, 2314: 1721, 2315: 1722, 2316: 1723, 2318: 1724, 2320: 1725, 2321: 1726, 2322: 1727, 2323: 1728, 2324: 1729, 2325: 1730, 2327: 1731, 2328: 1732, 2329: 1733, 2330: 1734, 2331: 1735, 2332: 1736, 2333: 1737, 2334: 1738, 2335: 1739, 2336: 1740, 2337: 1741, 2338: 1742, 2339: 1743, 2340: 1744, 2342: 1745, 2344: 1746, 2346: 1747, 2347: 1748, 2348: 1749, 2349: 1750, 2350: 1751, 2351: 1752, 2352: 1753, 2353: 1754, 2354: 1755, 2355: 1756, 2356: 1757, 2357: 1758, 2358: 1759, 2359: 1760, 2360: 1761, 2361: 1762, 2362: 1763, 2363: 1764, 2364: 1765, 2365: 1766, 2366: 1767, 2367: 1768, 2368: 1769, 2369: 1770, 2370: 1771, 2371: 1772, 2372: 1773, 2373: 1774, 2374: 1775, 2375: 1776, 2376: 1777, 2377: 1778, 2378: 1779, 2379: 1780, 2380: 1781, 2381: 1782, 2382: 1783, 2383: 1784, 2384: 1785, 2385: 1786, 2387: 1787, 2388: 1788, 2389: 1789, 2390: 1790, 2391: 1791, 2392: 1792, 2393: 1793, 2394: 1794, 2395: 1795, 2396: 1796, 2398: 1797, 2399: 1798, 2400: 1799, 2401: 1800, 2402: 1801, 2403: 1802, 2404: 1803, 2405: 1804, 2406: 1805, 2407: 1806, 2408: 1807, 2409: 1808, 2410: 1809, 2411: 1810, 2412: 1811, 2413: 1812, 2414: 1813, 2415: 1814, 2416: 1815, 2417: 1816, 2418: 1817, 2419: 1818, 2420: 1819, 2421: 1820, 2422: 1821, 2423: 1822, 2424: 1823, 2425: 1824, 2427: 1825, 2428: 1826, 2429: 1827, 2430: 1828, 2431: 1829, 2432: 1830, 2433: 1831, 2435: 1832, 2436: 1833, 2439: 1834, 2442: 1835, 2443: 1836, 2445: 1837, 2446: 1838, 2447: 1839, 2448: 1840, 2450: 1841, 2451: 1842, 2453: 1843, 2454: 1844, 2455: 1845, 2456: 1846, 2457: 1847, 2458: 1848, 2459: 1849, 2460: 1850, 2462: 1851, 2463: 1852, 2465: 1853, 2467: 1854, 2468: 1855, 2469: 1856, 2470: 1857, 2471: 1858, 2472: 1859, 2473: 1860, 2474: 1861, 2475: 1862, 2476: 1863, 2477: 1864, 2478: 1865, 2481: 1866, 2482: 1867, 2483: 1868, 2485: 1869, 2488: 1870, 2490: 1871, 2491: 1872, 2492: 1873, 2493: 1874, 2494: 1875, 2495: 1876, 2496: 1877, 2497: 1878, 2498: 1879, 2500: 1880, 2501: 1881, 2502: 1882, 2503: 1883, 2504: 1884, 2505: 1885, 2506: 1886, 2511: 1887, 2512: 1888, 2513: 1889, 2514: 1890, 2515: 1891, 2516: 1892, 2517: 1893, 2518: 1894, 2520: 1895, 2521: 1896, 2522: 1897, 2523: 1898, 2524: 1899, 2525: 1900, 2526: 1901, 2527: 1902, 2528: 1903, 2529: 1904, 2530: 1905, 2531: 1906, 2532: 1907, 2533: 1908, 2534: 1909, 2535: 1910, 2537: 1911, 2538: 1912, 2539: 1913, 2540: 1914, 2541: 1915, 2542: 1916, 2544: 1917, 2546: 1918, 2548: 1919, 2549: 1920, 2550: 1921, 2551: 1922, 2552: 1923, 2553: 1924, 2554: 1925, 2555: 1926, 2557: 1927, 2558: 1928, 2559: 1929, 2560: 1930, 2561: 1931, 2563: 1932, 2565: 1933, 2566: 1934, 2567: 1935, 2568: 1936, 2570: 1937, 2571: 1938, 2572: 1939, 2573: 1940, 2574: 1941, 2575: 1942, 2577: 1943, 2579: 1944, 2580: 1945, 2581: 1946, 2582: 1947, 2583: 1948, 2585: 1949, 2586: 1950, 2587: 1951, 2589: 1952, 2590: 1953, 2593: 1954, 2594: 1955, 2596: 1956, 2597: 1957, 2598: 1958, 2599: 1959, 2600: 1960, 2605: 1961, 2606: 1962, 2607: 1963, 2609: 1964, 2611: 1965, 2612: 1966, 2613: 1967, 2614: 1968, 2615: 1969, 2616: 1970, 2617: 1971, 2618: 1972, 2620: 1973, 2622: 1974, 2623: 1975, 2624: 1976, 2625: 1977, 2628: 1978, 2629: 1979, 2630: 1980, 2632: 1981, 2633: 1982, 2634: 1983, 2639: 1984, 2640: 1985, 2641: 1986, 2642: 1987, 2643: 1988, 2644: 1989, 2648: 1990, 2651: 1991, 2652: 1992, 2654: 1993, 2655: 1994, 2656: 1995, 2657: 1996, 2659: 1997, 2660: 1998, 2661: 1999, 2662: 2000, 2664: 2001, 2665: 2002, 2668: 2003, 2669: 2004, 2670: 2005, 2671: 2006, 2672: 2007, 2674: 2008, 2676: 2009, 2677: 2010, 2681: 2011, 2682: 2012, 2683: 2013, 2686: 2014, 2687: 2015, 2688: 2016, 2690: 2017, 2691: 2018, 2692: 2019, 2693: 2020, 2694: 2021, 2695: 2022, 2696: 2023, 2697: 2024, 2698: 2025, 2699: 2026, 2700: 2027, 2701: 2028, 2702: 2029, 2706: 2030, 2707: 2031, 2708: 2032, 2709: 2033, 2710: 2034, 2712: 2035, 2713: 2036, 2716: 2037, 2717: 2038, 2718: 2039, 2719: 2040, 2720: 2041, 2722: 2042, 2723: 2043, 2724: 2044, 2725: 2045, 2726: 2046, 2727: 2047, 2728: 2048, 2729: 2049, 2730: 2050, 2731: 2051, 2732: 2052, 2733: 2053, 2734: 2054, 2735: 2055, 2736: 2056, 2737: 2057, 2738: 2058, 2739: 2059, 2740: 2060, 2741: 2061, 2742: 2062, 2743: 2063, 2744: 2064, 2745: 2065, 2746: 2066, 2747: 2067, 2748: 2068, 2749: 2069, 2750: 2070, 2751: 2071, 2752: 2072, 2754: 2073, 2757: 2074, 2759: 2075, 2761: 2076, 2762: 2077, 2763: 2078, 2764: 2079, 2765: 2080, 2766: 2081, 2769: 2082, 2770: 2083, 2771: 2084, 2772: 2085, 2774: 2086, 2775: 2087, 2779: 2088, 2782: 2089, 2784: 2090, 2786: 2091, 2787: 2092, 2788: 2093, 2789: 2094, 2790: 2095, 2791: 2096, 2792: 2097, 2793: 2098, 2794: 2099, 2795: 2100, 2796: 2101, 2797: 2102, 2798: 2103, 2799: 2104, 2800: 2105, 2801: 2106, 2802: 2107, 2803: 2108, 2804: 2109, 2805: 2110, 2806: 2111, 2807: 2112, 2808: 2113, 2810: 2114, 2812: 2115, 2813: 2116, 2815: 2117, 2816: 2118, 2817: 2119, 2818: 2120, 2819: 2121, 2820: 2122, 2822: 2123, 2824: 2124, 2826: 2125, 2827: 2126, 2828: 2127, 2829: 2128, 2835: 2129, 2836: 2130, 2837: 2131, 2839: 2132, 2840: 2133, 2841: 2134, 2843: 2135, 2844: 2136, 2846: 2137, 2847: 2138, 2848: 2139, 2851: 2140, 2852: 2141, 2856: 2142, 2857: 2143, 2858: 2144, 2859: 2145, 2860: 2146, 2861: 2147, 2862: 2148, 2863: 2149, 2865: 2150, 2866: 2151, 2867: 2152, 2868: 2153, 2870: 2154, 2871: 2155, 2872: 2156, 2874: 2157, 2875: 2158, 2876: 2159, 2877: 2160, 2878: 2161, 2879: 2162, 2880: 2163, 2881: 2164, 2882: 2165, 2883: 2166, 2884: 2167, 2885: 2168, 2886: 2169, 2887: 2170, 2888: 2171, 2889: 2172, 2890: 2173, 2891: 2174, 2892: 2175, 2893: 2176, 2894: 2177, 2896: 2178, 2897: 2179, 2898: 2180, 2899: 2181, 2900: 2182, 2901: 2183, 2902: 2184, 2903: 2185, 2905: 2186, 2906: 2187, 2907: 2188, 2908: 2189, 2912: 2190, 2915: 2191, 2916: 2192, 2917: 2193, 2918: 2194, 2919: 2195, 2921: 2196, 2922: 2197, 2924: 2198, 2925: 2199, 2926: 2200, 2927: 2201, 2928: 2202, 2929: 2203, 2930: 2204, 2931: 2205, 2932: 2206, 2935: 2207, 2936: 2208, 2937: 2209, 2940: 2210, 2941: 2211, 2942: 2212, 2943: 2213, 2944: 2214, 2946: 2215, 2947: 2216, 2948: 2217, 2949: 2218, 2950: 2219, 2951: 2220, 2952: 2221, 2953: 2222, 2956: 2223, 2959: 2224, 2961: 2225, 2962: 2226, 2964: 2227, 2965: 2228, 2966: 2229, 2967: 2230, 2968: 2231, 2969: 2232, 2970: 2233, 2971: 2234, 2972: 2235, 2973: 2236, 2974: 2237, 2975: 2238, 2976: 2239, 2977: 2240, 2978: 2241, 2979: 2242, 2982: 2243, 2983: 2244, 2984: 2245, 2985: 2246, 2986: 2247, 2987: 2248, 2988: 2249, 2989: 2250, 2990: 2251, 2991: 2252, 2992: 2253, 2993: 2254, 2995: 2255, 2996: 2256, 2997: 2257, 3000: 2258, 3002: 2259, 3003: 2260, 3004: 2261, 3005: 2262, 3006: 2263, 3007: 2264, 3008: 2265, 3010: 2266, 3011: 2267, 3013: 2268, 3014: 2269, 3015: 2270, 3016: 2271, 3017: 2272, 3018: 2273, 3019: 2274, 3020: 2275, 3021: 2276, 3022: 2277, 3024: 2278, 3028: 2279, 3029: 2280, 3030: 2281, 3031: 2282, 3032: 2283, 3033: 2284, 3034: 2285, 3035: 2286, 3036: 2287, 3037: 2288, 3038: 2289, 3039: 2290, 3040: 2291, 3041: 2292, 3042: 2293, 3043: 2294, 3044: 2295, 3045: 2296, 3046: 2297, 3048: 2298, 3051: 2299, 3052: 2300, 3053: 2301, 3054: 2302, 3055: 2303, 3056: 2304, 3057: 2305, 3060: 2306, 3061: 2307, 3062: 2308, 3063: 2309, 3064: 2310, 3066: 2311, 3067: 2312, 3068: 2313, 3070: 2314, 3071: 2315, 3072: 2316, 3073: 2317, 3074: 2318, 3075: 2319, 3076: 2320, 3077: 2321, 3078: 2322, 3079: 2323, 3081: 2324, 3082: 2325, 3083: 2326, 3086: 2327, 3087: 2328, 3088: 2329, 3089: 2330, 3090: 2331, 3091: 2332, 3093: 2333, 3094: 2334, 3095: 2335, 3096: 2336, 3097: 2337, 3098: 2338, 3099: 2339, 3100: 2340, 3101: 2341, 3102: 2342, 3103: 2343, 3104: 2344, 3105: 2345, 3106: 2346, 3107: 2347, 3108: 2348, 3109: 2349, 3111: 2350, 3112: 2351, 3113: 2352, 3114: 2353, 3115: 2354, 3117: 2355, 3120: 2356, 3125: 2357, 3127: 2358, 3129: 2359, 3130: 2360, 3132: 2361, 3134: 2362, 3135: 2363, 3138: 2364, 3141: 2365, 3142: 2366, 3143: 2367, 3145: 2368, 3146: 2369, 3147: 2370, 3148: 2371, 3150: 2372, 3152: 2373, 3153: 2374, 3155: 2375, 3156: 2376, 3157: 2377, 3158: 2378, 3159: 2379, 3160: 2380, 3161: 2381, 3162: 2382, 3163: 2383, 3165: 2384, 3167: 2385, 3168: 2386, 3169: 2387, 3171: 2388, 3173: 2389, 3174: 2390, 3175: 2391, 3176: 2392, 3177: 2393, 3178: 2394, 3179: 2395, 3181: 2396, 3182: 2397, 3183: 2398, 3185: 2399, 3186: 2400, 3189: 2401, 3190: 2402, 3192: 2403, 3194: 2404, 3196: 2405, 3197: 2406, 3198: 2407, 3200: 2408, 3201: 2409, 3203: 2410, 3204: 2411, 3206: 2412, 3208: 2413, 3210: 2414, 3211: 2415, 3213: 2416, 3214: 2417, 3217: 2418, 3219: 2419, 3221: 2420, 3223: 2421, 3224: 2422, 3225: 2423, 3230: 2424, 3235: 2425, 3238: 2426, 3240: 2427, 3241: 2428, 3243: 2429, 3244: 2430, 3246: 2431, 3247: 2432, 3248: 2433, 3249: 2434, 3250: 2435, 3251: 2436, 3252: 2437, 3253: 2438, 3254: 2439, 3255: 2440, 3256: 2441, 3257: 2442, 3258: 2443, 3259: 2444, 3260: 2445, 3261: 2446, 3262: 2447, 3263: 2448, 3264: 2449, 3265: 2450, 3266: 2451, 3267: 2452, 3268: 2453, 3269: 2454, 3270: 2455, 3271: 2456, 3272: 2457, 3273: 2458, 3274: 2459, 3275: 2460, 3276: 2461, 3280: 2462, 3281: 2463, 3283: 2464, 3284: 2465, 3285: 2466, 3286: 2467, 3287: 2468, 3289: 2469, 3294: 2470, 3295: 2471, 3296: 2472, 3298: 2473, 3299: 2474, 3300: 2475, 3301: 2476, 3302: 2477, 3303: 2478, 3306: 2479, 3307: 2480, 3308: 2481, 3310: 2482, 3313: 2483, 3315: 2484, 3316: 2485, 3317: 2486, 3323: 2487, 3324: 2488, 3325: 2489, 3326: 2490, 3327: 2491, 3328: 2492, 3329: 2493, 3330: 2494, 3331: 2495, 3334: 2496, 3341: 2497, 3342: 2498, 3344: 2499, 3345: 2500, 3347: 2501, 3350: 2502, 3353: 2503, 3354: 2504, 3355: 2505, 3357: 2506, 3358: 2507, 3359: 2508, 3360: 2509, 3361: 2510, 3362: 2511, 3363: 2512, 3364: 2513, 3365: 2514, 3368: 2515, 3370: 2516, 3372: 2517, 3374: 2518, 3378: 2519, 3379: 2520, 3384: 2521, 3385: 2522, 3386: 2523, 3387: 2524, 3388: 2525, 3389: 2526, 3390: 2527, 3391: 2528, 3392: 2529, 3393: 2530, 3394: 2531, 3395: 2532, 3396: 2533, 3397: 2534, 3398: 2535, 3400: 2536, 3401: 2537, 3402: 2538, 3403: 2539, 3404: 2540, 3405: 2541, 3406: 2542, 3408: 2543, 3409: 2544, 3410: 2545, 3412: 2546, 3414: 2547, 3415: 2548, 3417: 2549, 3418: 2550, 3420: 2551, 3421: 2552, 3422: 2553, 3423: 2554, 3424: 2555, 3425: 2556, 3426: 2557, 3428: 2558, 3429: 2559, 3430: 2560, 3431: 2561, 3432: 2562, 3433: 2563, 3434: 2564, 3435: 2565, 3436: 2566, 3438: 2567, 3439: 2568, 3440: 2569, 3441: 2570, 3442: 2571, 3444: 2572, 3445: 2573, 3446: 2574, 3447: 2575, 3448: 2576, 3449: 2577, 3450: 2578, 3451: 2579, 3452: 2580, 3453: 2581, 3454: 2582, 3455: 2583, 3459: 2584, 3461: 2585, 3462: 2586, 3466: 2587, 3467: 2588, 3468: 2589, 3469: 2590, 3470: 2591, 3471: 2592, 3473: 2593, 3474: 2594, 3475: 2595, 3476: 2596, 3477: 2597, 3478: 2598, 3479: 2599, 3480: 2600, 3481: 2601, 3483: 2602, 3484: 2603, 3489: 2604, 3492: 2605, 3494: 2606, 3496: 2607, 3497: 2608, 3498: 2609, 3499: 2610, 3500: 2611, 3501: 2612, 3502: 2613, 3503: 2614, 3504: 2615, 3505: 2616, 3506: 2617, 3507: 2618, 3508: 2619, 3510: 2620, 3511: 2621, 3512: 2622, 3513: 2623, 3514: 2624, 3515: 2625, 3516: 2626, 3519: 2627, 3521: 2628, 3524: 2629, 3525: 2630, 3526: 2631, 3527: 2632, 3528: 2633, 3529: 2634, 3531: 2635, 3534: 2636, 3535: 2637, 3536: 2638, 3537: 2639, 3538: 2640, 3539: 2641, 3543: 2642, 3544: 2643, 3545: 2644, 3546: 2645, 3548: 2646, 3549: 2647, 3550: 2648, 3551: 2649, 3552: 2650, 3553: 2651, 3554: 2652, 3555: 2653, 3556: 2654, 3557: 2655, 3559: 2656, 3563: 2657, 3564: 2658, 3565: 2659, 3566: 2660, 3567: 2661, 3568: 2662, 3569: 2663, 3571: 2664, 3572: 2665, 3573: 2666, 3574: 2667, 3576: 2668, 3577: 2669, 3578: 2670, 3580: 2671, 3581: 2672, 3584: 2673, 3586: 2674, 3587: 2675, 3590: 2676, 3591: 2677, 3592: 2678, 3593: 2679, 3594: 2680, 3596: 2681, 3597: 2682, 3598: 2683, 3599: 2684, 3604: 2685, 3606: 2686, 3608: 2687, 3609: 2688, 3614: 2689, 3615: 2690, 3616: 2691, 3617: 2692, 3618: 2693, 3619: 2694, 3622: 2695, 3623: 2696, 3624: 2697, 3625: 2698, 3626: 2699, 3627: 2700, 3628: 2701, 3629: 2702, 3632: 2703, 3633: 2704, 3634: 2705, 3635: 2706, 3637: 2707, 3638: 2708, 3639: 2709, 3640: 2710, 3641: 2711, 3643: 2712, 3646: 2713, 3648: 2714, 3649: 2715, 3652: 2716, 3653: 2717, 3654: 2718, 3655: 2719, 3658: 2720, 3660: 2721, 3661: 2722, 3662: 2723, 3663: 2724, 3664: 2725, 3667: 2726, 3668: 2727, 3669: 2728, 3671: 2729, 3672: 2730, 3673: 2731, 3674: 2732, 3675: 2733, 3676: 2734, 3677: 2735, 3678: 2736, 3679: 2737, 3680: 2738, 3681: 2739, 3682: 2740, 3683: 2741, 3684: 2742, 3685: 2743, 3686: 2744, 3687: 2745, 3688: 2746, 3689: 2747, 3690: 2748, 3691: 2749, 3692: 2750, 3693: 2751, 3694: 2752, 3695: 2753, 3696: 2754, 3697: 2755, 3698: 2756, 3699: 2757, 3700: 2758, 3701: 2759, 3702: 2760, 3703: 2761, 3704: 2762, 3705: 2763, 3706: 2764, 3707: 2765, 3708: 2766, 3709: 2767, 3710: 2768, 3711: 2769, 3712: 2770, 3713: 2771, 3714: 2772, 3715: 2773, 3716: 2774, 3717: 2775, 3719: 2776, 3720: 2777, 3723: 2778, 3724: 2779, 3725: 2780, 3726: 2781, 3727: 2782, 3728: 2783, 3729: 2784, 3730: 2785, 3731: 2786, 3732: 2787, 3733: 2788, 3735: 2789, 3736: 2790, 3737: 2791, 3738: 2792, 3739: 2793, 3740: 2794, 3741: 2795, 3742: 2796, 3743: 2797, 3744: 2798, 3745: 2799, 3746: 2800, 3747: 2801, 3751: 2802, 3752: 2803, 3753: 2804, 3754: 2805, 3755: 2806, 3756: 2807, 3758: 2808, 3760: 2809, 3761: 2810, 3763: 2811, 3764: 2812, 3765: 2813, 3766: 2814, 3767: 2815, 3768: 2816, 3769: 2817, 3770: 2818, 3771: 2819, 3773: 2820, 3774: 2821, 3783: 2822, 3784: 2823, 3785: 2824, 3786: 2825, 3787: 2826, 3788: 2827, 3789: 2828, 3790: 2829, 3791: 2830, 3792: 2831, 3793: 2832, 3794: 2833, 3795: 2834, 3797: 2835, 3798: 2836, 3799: 2837, 3801: 2838, 3802: 2839, 3806: 2840, 3807: 2841, 3808: 2842, 3809: 2843, 3810: 2844, 3811: 2845, 3812: 2846, 3813: 2847, 3814: 2848, 3816: 2849, 3819: 2850, 3821: 2851, 3822: 2852, 3823: 2853, 3824: 2854, 3825: 2855, 3826: 2856, 3827: 2857, 3830: 2858, 3831: 2859, 3832: 2860, 3833: 2861, 3834: 2862, 3835: 2863, 3836: 2864, 3837: 2865, 3838: 2866, 3839: 2867, 3840: 2868, 3841: 2869, 3843: 2870, 3844: 2871, 3845: 2872, 3846: 2873, 3847: 2874, 3849: 2875, 3851: 2876, 3852: 2877, 3855: 2878, 3857: 2879, 3858: 2880, 3859: 2881, 3861: 2882, 3862: 2883, 3863: 2884, 3864: 2885, 3865: 2886, 3868: 2887, 3869: 2888, 3870: 2889, 3871: 2890, 3872: 2891, 3873: 2892, 3877: 2893, 3879: 2894, 3882: 2895, 3884: 2896, 3888: 2897, 3889: 2898, 3893: 2899, 3894: 2900, 3895: 2901, 3896: 2902, 3897: 2903, 3898: 2904, 3899: 2905, 3900: 2906, 3901: 2907, 3906: 2908, 3908: 2909, 3909: 2910, 3910: 2911, 3911: 2912, 3912: 2913, 3914: 2914, 3915: 2915, 3916: 2916, 3917: 2917, 3918: 2918, 3919: 2919, 3920: 2920, 3922: 2921, 3925: 2922, 3926: 2923, 3927: 2924, 3928: 2925, 3929: 2926, 3930: 2927, 3932: 2928, 3933: 2929, 3937: 2930, 3938: 2931, 3939: 2932, 3940: 2933, 3941: 2934, 3942: 2935, 3943: 2936, 3945: 2937, 3946: 2938, 3947: 2939, 3948: 2940, 3949: 2941, 3950: 2942, 3951: 2943, 3952: 2944, 3953: 2945, 3955: 2946, 3957: 2947, 3958: 2948, 3959: 2949, 3962: 2950, 3963: 2951, 3964: 2952, 3965: 2953, 3966: 2954, 3967: 2955, 3968: 2956, 3969: 2957, 3971: 2958, 3972: 2959, 3973: 2960, 3974: 2961, 3977: 2962, 3978: 2963, 3979: 2964, 3980: 2965, 3981: 2966, 3983: 2967, 3984: 2968, 3985: 2969, 3986: 2970, 3987: 2971, 3988: 2972, 3989: 2973, 3990: 2974, 3991: 2975, 3992: 2976, 3993: 2977, 3994: 2978, 3996: 2979, 3997: 2980, 3998: 2981, 3999: 2982, 4000: 2983, 4002: 2984, 4003: 2985, 4005: 2986, 4006: 2987, 4007: 2988, 4008: 2989, 4009: 2990, 4010: 2991, 4011: 2992, 4012: 2993, 4014: 2994, 4015: 2995, 4016: 2996, 4017: 2997, 4018: 2998, 4019: 2999, 4020: 3000, 4021: 3001, 4022: 3002, 4023: 3003, 4024: 3004, 4025: 3005, 4027: 3006, 4029: 3007, 4030: 3008, 4031: 3009, 4032: 3010, 4033: 3011, 4034: 3012, 4035: 3013, 4036: 3014, 4037: 3015, 4039: 3016, 4040: 3017, 4041: 3018, 4042: 3019, 4043: 3020, 4046: 3021, 4047: 3022, 4051: 3023, 4052: 3024, 4053: 3025, 4054: 3026, 4055: 3027, 4056: 3028, 4061: 3029, 4062: 3030, 4063: 3031, 4064: 3032, 4065: 3033, 4066: 3034, 4067: 3035, 4068: 3036, 4069: 3037, 4074: 3038, 4077: 3039, 4078: 3040, 4079: 3041, 4080: 3042, 4081: 3043, 4082: 3044, 4083: 3045, 4084: 3046, 4085: 3047, 4086: 3048, 4089: 3049, 4090: 3050, 4091: 3051, 4092: 3052, 4093: 3053, 4102: 3054, 4103: 3055, 4104: 3056, 4105: 3057, 4109: 3058, 4110: 3059, 4111: 3060, 4113: 3061, 4115: 3062, 4116: 3063, 4117: 3064, 4121: 3065, 4123: 3066, 4124: 3067, 4125: 3068, 4126: 3069, 4127: 3070, 4128: 3071, 4129: 3072, 4130: 3073, 4131: 3074, 4132: 3075, 4133: 3076, 4135: 3077, 4138: 3078, 4139: 3079, 4141: 3080, 4142: 3081, 4143: 3082, 4144: 3083, 4146: 3084, 4147: 3085, 4148: 3086, 4149: 3087, 4152: 3088, 4153: 3089, 4154: 3090, 4155: 3091, 4156: 3092, 4157: 3093, 4158: 3094, 4159: 3095, 4160: 3096, 4161: 3097, 4164: 3098, 4166: 3099, 4167: 3100, 4168: 3101, 4171: 3102, 4174: 3103, 4175: 3104, 4178: 3105, 4180: 3106, 4181: 3107, 4184: 3108, 4187: 3109, 4189: 3110, 4190: 3111, 4191: 3112, 4193: 3113, 4195: 3114, 4197: 3115, 4198: 3116, 4200: 3117, 4202: 3118, 4203: 3119, 4204: 3120, 4205: 3121, 4207: 3122, 4210: 3123, 4211: 3124, 4212: 3125, 4214: 3126, 4215: 3127, 4217: 3128, 4218: 3129, 4219: 3130, 4220: 3131, 4221: 3132, 4223: 3133, 4224: 3134, 4225: 3135, 4226: 3136, 4228: 3137, 4229: 3138, 4231: 3139, 4232: 3140, 4233: 3141, 4234: 3142, 4235: 3143, 4236: 3144, 4237: 3145, 4238: 3146, 4239: 3147, 4240: 3148, 4241: 3149, 4242: 3150, 4243: 3151, 4246: 3152, 4247: 3153, 4248: 3154, 4251: 3155, 4252: 3156, 4254: 3157, 4255: 3158, 4256: 3159, 4259: 3160, 4260: 3161, 4262: 3162, 4263: 3163, 4265: 3164, 4267: 3165, 4268: 3166, 4270: 3167, 4273: 3168, 4275: 3169, 4276: 3170, 4278: 3171, 4280: 3172, 4282: 3173, 4284: 3174, 4285: 3175, 4289: 3176, 4290: 3177, 4291: 3178, 4292: 3179, 4293: 3180, 4294: 3181, 4296: 3182, 4297: 3183, 4298: 3184, 4299: 3185, 4300: 3186, 4304: 3187, 4305: 3188, 4306: 3189, 4307: 3190, 4308: 3191, 4310: 3192, 4312: 3193, 4313: 3194, 4316: 3195, 4317: 3196, 4321: 3197, 4322: 3198, 4323: 3199, 4325: 3200, 4326: 3201, 4327: 3202, 4329: 3203, 4333: 3204, 4334: 3205, 4337: 3206, 4338: 3207, 4339: 3208, 4340: 3209, 4342: 3210, 4343: 3211, 4344: 3212, 4345: 3213, 4347: 3214, 4349: 3215, 4350: 3216, 4351: 3217, 4353: 3218, 4354: 3219, 4355: 3220, 4356: 3221, 4357: 3222, 4359: 3223, 4361: 3224, 4366: 3225, 4367: 3226, 4368: 3227, 4369: 3228, 4370: 3229, 4371: 3230, 4372: 3231, 4373: 3232, 4374: 3233, 4378: 3234, 4380: 3235, 4381: 3236, 4383: 3237, 4384: 3238, 4386: 3239, 4387: 3240, 4388: 3241, 4389: 3242, 4390: 3243, 4392: 3244, 4393: 3245, 4394: 3246, 4395: 3247, 4396: 3248, 4397: 3249, 4399: 3250, 4402: 3251, 4403: 3252, 4404: 3253, 4406: 3254, 4407: 3255, 4408: 3256, 4409: 3257, 4410: 3258, 4412: 3259, 4419: 3260, 4420: 3261, 4422: 3262, 4424: 3263, 4426: 3264, 4427: 3265, 4428: 3266, 4429: 3267, 4432: 3268, 4433: 3269, 4434: 3270, 4436: 3271, 4437: 3272, 4438: 3273, 4439: 3274, 4440: 3275, 4441: 3276, 4442: 3277, 4443: 3278, 4444: 3279, 4445: 3280, 4446: 3281, 4447: 3282, 4448: 3283, 4449: 3284, 4450: 3285, 4451: 3286, 4452: 3287, 4453: 3288, 4454: 3289, 4458: 3290, 4459: 3291, 4462: 3292, 4463: 3293, 4464: 3294, 4465: 3295, 4466: 3296, 4467: 3297, 4470: 3298, 4471: 3299, 4473: 3300, 4474: 3301, 4475: 3302, 4476: 3303, 4477: 3304, 4478: 3305, 4480: 3306, 4482: 3307, 4483: 3308, 4484: 3309, 4487: 3310, 4488: 3311, 4489: 3312, 4490: 3313, 4492: 3314, 4495: 3315, 4496: 3316, 4497: 3317, 4498: 3318, 4499: 3319, 4500: 3320, 4502: 3321, 4504: 3322, 4505: 3323, 4506: 3324, 4508: 3325, 4509: 3326, 4511: 3327, 4516: 3328, 4517: 3329, 4518: 3330, 4519: 3331, 4520: 3332, 4521: 3333, 4522: 3334, 4523: 3335, 4524: 3336, 4526: 3337, 4527: 3338, 4529: 3339, 4531: 3340, 4533: 3341, 4534: 3342, 4535: 3343, 4537: 3344, 4541: 3345, 4544: 3346, 4545: 3347, 4546: 3348, 4552: 3349, 4553: 3350, 4557: 3351, 4558: 3352, 4562: 3353, 4563: 3354, 4564: 3355, 4565: 3356, 4568: 3357, 4571: 3358, 4572: 3359, 4573: 3360, 4574: 3361, 4577: 3362, 4578: 3363, 4580: 3364, 4581: 3365, 4583: 3366, 4584: 3367, 4585: 3368, 4587: 3369, 4589: 3370, 4591: 3371, 4593: 3372, 4594: 3373, 4595: 3374, 4597: 3375, 4599: 3376, 4600: 3377, 4602: 3378, 4603: 3379, 4605: 3380, 4608: 3381, 4610: 3382, 4611: 3383, 4612: 3384, 4613: 3385, 4614: 3386, 4615: 3387, 4616: 3388, 4617: 3389, 4619: 3390, 4621: 3391, 4622: 3392, 4623: 3393, 4624: 3394, 4625: 3395, 4626: 3396, 4628: 3397, 4629: 3398, 4630: 3399, 4632: 3400, 4634: 3401, 4635: 3402, 4636: 3403, 4638: 3404, 4639: 3405, 4640: 3406, 4641: 3407, 4642: 3408, 4643: 3409, 4644: 3410, 4645: 3411, 4646: 3412, 4649: 3413, 4653: 3414, 4654: 3415, 4658: 3416, 4660: 3417, 4661: 3418, 4662: 3419, 4663: 3420, 4666: 3421, 4670: 3422, 4671: 3423, 4672: 3424, 4673: 3425, 4675: 3426, 4676: 3427, 4677: 3428, 4678: 3429, 4679: 3430, 4681: 3431, 4683: 3432, 4686: 3433, 4687: 3434, 4688: 3435, 4689: 3436, 4690: 3437, 4695: 3438, 4697: 3439, 4699: 3440, 4700: 3441, 4701: 3442, 4703: 3443, 4704: 3444, 4705: 3445, 4708: 3446, 4709: 3447, 4710: 3448, 4711: 3449, 4713: 3450, 4714: 3451, 4715: 3452, 4717: 3453, 4718: 3454, 4719: 3455, 4720: 3456, 4721: 3457, 4722: 3458, 4723: 3459, 4725: 3460, 4727: 3461, 4728: 3462, 4732: 3463, 4733: 3464, 4734: 3465, 4735: 3466, 4736: 3467, 4738: 3468, 4740: 3469, 4741: 3470, 4743: 3471, 4744: 3472, 4745: 3473, 4748: 3474, 4749: 3475, 4750: 3476, 4754: 3477, 4756: 3478, 4757: 3479, 4759: 3480, 4765: 3481, 4766: 3482, 4769: 3483, 4770: 3484, 4771: 3485, 4772: 3486, 4773: 3487, 4774: 3488, 4775: 3489, 4776: 3490, 4777: 3491, 4780: 3492, 4782: 3493, 4783: 3494, 4784: 3495, 4785: 3496, 4786: 3497, 4787: 3498, 4788: 3499, 4789: 3500, 4792: 3501, 4794: 3502, 4795: 3503, 4796: 3504, 4798: 3505, 4799: 3506, 4800: 3507, 4801: 3508, 4802: 3509, 4803: 3510, 4804: 3511, 4808: 3512, 4809: 3513, 4810: 3514, 4811: 3515, 4812: 3516, 4813: 3517, 4814: 3518, 4815: 3519, 4816: 3520, 4818: 3521, 4821: 3522, 4822: 3523, 4823: 3524, 4825: 3525, 4826: 3526, 4827: 3527, 4828: 3528, 4830: 3529, 4831: 3530, 4833: 3531, 4835: 3532, 4836: 3533, 4840: 3534, 4844: 3535, 4845: 3536, 4846: 3537, 4847: 3538, 4848: 3539, 4849: 3540, 4850: 3541, 4852: 3542, 4855: 3543, 4857: 3544, 4860: 3545, 4862: 3546, 4863: 3547, 4864: 3548, 4865: 3549, 4866: 3550, 4867: 3551, 4871: 3552, 4873: 3553, 4874: 3554, 4876: 3555, 4877: 3556, 4878: 3557, 4879: 3558, 4880: 3559, 4881: 3560, 4883: 3561, 4885: 3562, 4886: 3563, 4887: 3564, 4888: 3565, 4889: 3566, 4890: 3567, 4893: 3568, 4896: 3569, 4898: 3570, 4899: 3571, 4900: 3572, 4901: 3573, 4902: 3574, 4903: 3575, 4909: 3576, 4911: 3577, 4912: 3578, 4914: 3579, 4915: 3580, 4916: 3581, 4917: 3582, 4919: 3583, 4920: 3584, 4921: 3585, 4925: 3586, 4926: 3587, 4927: 3588, 4928: 3589, 4929: 3590, 4932: 3591, 4936: 3592, 4939: 3593, 4941: 3594, 4942: 3595, 4945: 3596, 4946: 3597, 4947: 3598, 4951: 3599, 4952: 3600, 4953: 3601, 4954: 3602, 4956: 3603, 4957: 3604, 4958: 3605, 4959: 3606, 4961: 3607, 4962: 3608, 4963: 3609, 4964: 3610, 4965: 3611, 4966: 3612, 4967: 3613, 4969: 3614, 4970: 3615, 4971: 3616, 4973: 3617, 4974: 3618, 4975: 3619, 4976: 3620, 4977: 3621, 4978: 3622, 4979: 3623, 4980: 3624, 4981: 3625, 4985: 3626, 4987: 3627, 4988: 3628, 4989: 3629, 4990: 3630, 4991: 3631, 4992: 3632, 4993: 3633, 4994: 3634, 4995: 3635, 4998: 3636, 5004: 3637, 5007: 3638, 5008: 3639, 5009: 3640, 5010: 3641, 5011: 3642, 5012: 3643, 5013: 3644, 5014: 3645, 5015: 3646, 5016: 3647, 5021: 3648, 5023: 3649, 5025: 3650, 5026: 3651, 5027: 3652, 5028: 3653, 5033: 3654, 5034: 3655, 5039: 3656, 5040: 3657, 5041: 3658, 5046: 3659, 5047: 3660, 5048: 3661, 5049: 3662, 5051: 3663, 5053: 3664, 5054: 3665, 5055: 3666, 5059: 3667, 5060: 3668, 5062: 3669, 5063: 3670, 5064: 3671, 5065: 3672, 5066: 3673, 5068: 3674, 5069: 3675, 5071: 3676, 5072: 3677, 5073: 3678, 5074: 3679, 5075: 3680, 5076: 3681, 5077: 3682, 5080: 3683, 5081: 3684, 5083: 3685, 5088: 3686, 5092: 3687, 5093: 3688, 5094: 3689, 5095: 3690, 5096: 3691, 5099: 3692, 5102: 3693, 5103: 3694, 5105: 3695, 5106: 3696, 5107: 3697, 5108: 3698, 5109: 3699, 5110: 3700, 5111: 3701, 5112: 3702, 5113: 3703, 5114: 3704, 5116: 3705, 5120: 3706, 5121: 3707, 5127: 3708, 5128: 3709, 5131: 3710, 5134: 3711, 5135: 3712, 5136: 3713, 5137: 3714, 5139: 3715, 5146: 3716, 5147: 3717, 5151: 3718, 5152: 3719, 5155: 3720, 5156: 3721, 5159: 3722, 5165: 3723, 5168: 3724, 5170: 3725, 5171: 3726, 5177: 3727, 5178: 3728, 5181: 3729, 5187: 3730, 5189: 3731, 5197: 3732, 5198: 3733, 5202: 3734, 5205: 3735, 5209: 3736, 5212: 3737, 5213: 3738, 5214: 3739, 5218: 3740, 5219: 3741, 5220: 3742, 5221: 3743, 5222: 3744, 5224: 3745, 5225: 3746, 5226: 3747, 5231: 3748, 5237: 3749, 5238: 3750, 5239: 3751, 5240: 3752, 5241: 3753, 5244: 3754, 5246: 3755, 5247: 3756, 5248: 3757, 5250: 3758, 5254: 3759, 5255: 3760, 5256: 3761, 5258: 3762, 5264: 3763, 5265: 3764, 5266: 3765, 5267: 3766, 5268: 3767, 5269: 3768, 5272: 3769, 5275: 3770, 5278: 3771, 5279: 3772, 5282: 3773, 5283: 3774, 5284: 3775, 5285: 3776, 5288: 3777, 5291: 3778, 5292: 3779, 5293: 3780, 5294: 3781, 5296: 3782, 5297: 3783, 5298: 3784, 5299: 3785, 5300: 3786, 5303: 3787, 5304: 3788, 5307: 3789, 5308: 3790, 5309: 3791, 5311: 3792, 5312: 3793, 5313: 3794, 5316: 3795, 5319: 3796, 5321: 3797, 5322: 3798, 5323: 3799, 5324: 3800, 5325: 3801, 5328: 3802, 5329: 3803, 5333: 3804, 5334: 3805, 5337: 3806, 5338: 3807, 5339: 3808, 5341: 3809, 5345: 3810, 5346: 3811, 5347: 3812, 5348: 3813, 5349: 3814, 5353: 3815, 5356: 3816, 5357: 3817, 5358: 3818, 5361: 3819, 5363: 3820, 5364: 3821, 5365: 3822, 5372: 3823, 5373: 3824, 5375: 3825, 5377: 3826, 5378: 3827, 5379: 3828, 5380: 3829, 5382: 3830, 5384: 3831, 5385: 3832, 5387: 3833, 5388: 3834, 5389: 3835, 5390: 3836, 5391: 3837, 5397: 3838, 5398: 3839, 5400: 3840, 5401: 3841, 5404: 3842, 5409: 3843, 5410: 3844, 5414: 3845, 5415: 3846, 5416: 3847, 5417: 3848, 5418: 3849, 5419: 3850, 5420: 3851, 5421: 3852, 5422: 3853, 5423: 3854, 5424: 3855, 5425: 3856, 5427: 3857, 5428: 3858, 5433: 3859, 5434: 3860, 5435: 3861, 5437: 3862, 5438: 3863, 5440: 3864, 5442: 3865, 5443: 3866, 5444: 3867, 5445: 3868, 5446: 3869, 5447: 3870, 5448: 3871, 5449: 3872, 5450: 3873, 5451: 3874, 5452: 3875, 5454: 3876, 5455: 3877, 5456: 3878, 5458: 3879, 5459: 3880, 5460: 3881, 5461: 3882, 5462: 3883, 5463: 3884, 5464: 3885, 5465: 3886, 5466: 3887, 5468: 3888, 5470: 3889, 5471: 3890, 5472: 3891, 5475: 3892, 5476: 3893, 5477: 3894, 5478: 3895, 5479: 3896, 5480: 3897, 5481: 3898, 5483: 3899, 5485: 3900, 5486: 3901, 5489: 3902, 5490: 3903, 5493: 3904, 5497: 3905, 5498: 3906, 5499: 3907, 5500: 3908, 5501: 3909, 5502: 3910, 5503: 3911, 5504: 3912, 5505: 3913, 5506: 3914, 5507: 3915, 5508: 3916, 5512: 3917, 5513: 3918, 5515: 3919, 5521: 3920, 5522: 3921, 5523: 3922, 5524: 3923, 5525: 3924, 5527: 3925, 5528: 3926, 5529: 3927, 5530: 3928, 5531: 3929, 5532: 3930, 5537: 3931, 5538: 3932, 5539: 3933, 5540: 3934, 5541: 3935, 5543: 3936, 5544: 3937, 5548: 3938, 5550: 3939, 5553: 3940, 5556: 3941, 5560: 3942, 5562: 3943, 5563: 3944, 5564: 3945, 5568: 3946, 5569: 3947, 5570: 3948, 5572: 3949, 5573: 3950, 5574: 3951, 5575: 3952, 5577: 3953, 5580: 3954, 5581: 3955, 5582: 3956, 5584: 3957, 5585: 3958, 5588: 3959, 5589: 3960, 5590: 3961, 5591: 3962, 5596: 3963, 5597: 3964, 5601: 3965, 5602: 3966, 5603: 3967, 5604: 3968, 5607: 3969, 5608: 3970, 5609: 3971, 5610: 3972, 5611: 3973, 5612: 3974, 5613: 3975, 5614: 3976, 5615: 3977, 5617: 3978, 5618: 3979, 5619: 3980, 5620: 3981, 5621: 3982, 5625: 3983, 5628: 3984, 5629: 3985, 5630: 3986, 5632: 3987, 5633: 3988, 5635: 3989, 5636: 3990, 5637: 3991, 5638: 3992, 5640: 3993, 5643: 3994, 5644: 3995, 5646: 3996, 5649: 3997, 5650: 3998, 5651: 3999, 5657: 4000, 5663: 4001, 5664: 4002, 5665: 4003, 5666: 4004, 5667: 4005, 5668: 4006, 5669: 4007, 5670: 4008, 5672: 4009, 5673: 4010, 5675: 4011, 5678: 4012, 5679: 4013, 5680: 4014, 5682: 4015, 5684: 4016, 5685: 4017, 5688: 4018, 5689: 4019, 5690: 4020, 5693: 4021, 5694: 4022, 5696: 4023, 5699: 4024, 5700: 4025, 5704: 4026, 5706: 4027, 5707: 4028, 5710: 4029, 5712: 4030, 5720: 4031, 5723: 4032, 5733: 4033, 5735: 4034, 5736: 4035, 5741: 4036, 5742: 4037, 5745: 4038, 5746: 4039, 5747: 4040, 5749: 4041, 5752: 4042, 5755: 4043, 5764: 4044, 5767: 4045, 5768: 4046, 5771: 4047, 5772: 4048, 5773: 4049, 5779: 4050, 5780: 4051, 5782: 4052, 5784: 4053, 5785: 4054, 5786: 4055, 5787: 4056, 5788: 4057, 5791: 4058, 5792: 4059, 5796: 4060, 5797: 4061, 5799: 4062, 5801: 4063, 5802: 4064, 5803: 4065, 5804: 4066, 5809: 4067, 5810: 4068, 5812: 4069, 5816: 4070, 5817: 4071, 5818: 4072, 5820: 4073, 5826: 4074, 5828: 4075, 5829: 4076, 5833: 4077, 5836: 4078, 5838: 4079, 5839: 4080, 5840: 4081, 5841: 4082, 5843: 4083, 5847: 4084, 5849: 4085, 5853: 4086, 5856: 4087, 5863: 4088, 5867: 4089, 5872: 4090, 5873: 4091, 5874: 4092, 5875: 4093, 5876: 4094, 5878: 4095, 5879: 4096, 5880: 4097, 5881: 4098, 5882: 4099, 5883: 4100, 5884: 4101, 5888: 4102, 5889: 4103, 5890: 4104, 5891: 4105, 5893: 4106, 5896: 4107, 5899: 4108, 5900: 4109, 5901: 4110, 5902: 4111, 5903: 4112, 5909: 4113, 5912: 4114, 5915: 4115, 5919: 4116, 5922: 4117, 5925: 4118, 5927: 4119, 5929: 4120, 5932: 4121, 5938: 4122, 5941: 4123, 5942: 4124, 5943: 4125, 5944: 4126, 5945: 4127, 5947: 4128, 5949: 4129, 5951: 4130, 5952: 4131, 5953: 4132, 5954: 4133, 5955: 4134, 5956: 4135, 5957: 4136, 5959: 4137, 5961: 4138, 5962: 4139, 5963: 4140, 5965: 4141, 5968: 4142, 5969: 4143, 5970: 4144, 5971: 4145, 5974: 4146, 5975: 4147, 5979: 4148, 5980: 4149, 5984: 4150, 5986: 4151, 5988: 4152, 5989: 4153, 5990: 4154, 5991: 4155, 5992: 4156, 5993: 4157, 5994: 4158, 5995: 4159, 5999: 4160, 6001: 4161, 6002: 4162, 6003: 4163, 6005: 4164, 6006: 4165, 6009: 4166, 6012: 4167, 6013: 4168, 6014: 4169, 6016: 4170, 6020: 4171, 6021: 4172, 6022: 4173, 6023: 4174, 6025: 4175, 6027: 4176, 6030: 4177, 6031: 4178, 6033: 4179, 6035: 4180, 6036: 4181, 6039: 4182, 6040: 4183, 6041: 4184, 6042: 4185, 6044: 4186, 6049: 4187, 6051: 4188, 6055: 4189, 6057: 4190, 6058: 4191, 6059: 4192, 6060: 4193, 6062: 4194, 6063: 4195, 6064: 4196, 6067: 4197, 6070: 4198, 6078: 4199, 6086: 4200, 6090: 4201, 6093: 4202, 6095: 4203, 6100: 4204, 6101: 4205, 6104: 4206, 6107: 4207, 6111: 4208, 6116: 4209, 6119: 4210, 6122: 4211, 6123: 4212, 6124: 4213, 6125: 4214, 6140: 4215, 6141: 4216, 6143: 4217, 6145: 4218, 6148: 4219, 6153: 4220, 6155: 4221, 6156: 4222, 6157: 4223, 6158: 4224, 6159: 4225, 6162: 4226, 6163: 4227, 6166: 4228, 6169: 4229, 6170: 4230, 6178: 4231, 6181: 4232, 6182: 4233, 6183: 4234, 6184: 4235, 6185: 4236, 6186: 4237, 6187: 4238, 6188: 4239, 6192: 4240, 6193: 4241, 6195: 4242, 6196: 4243, 6197: 4244, 6201: 4245, 6202: 4246, 6203: 4247, 6204: 4248, 6210: 4249, 6212: 4250, 6213: 4251, 6214: 4252, 6215: 4253, 6216: 4254, 6217: 4255, 6218: 4256, 6219: 4257, 6220: 4258, 6222: 4259, 6223: 4260, 6225: 4261, 6228: 4262, 6232: 4263, 6234: 4264, 6235: 4265, 6237: 4266, 6238: 4267, 6239: 4268, 6241: 4269, 6242: 4270, 6244: 4271, 6245: 4272, 6246: 4273, 6249: 4274, 6250: 4275, 6251: 4276, 6252: 4277, 6254: 4278, 6263: 4279, 6264: 4280, 6265: 4281, 6266: 4282, 6267: 4283, 6268: 4284, 6269: 4285, 6270: 4286, 6271: 4287, 6273: 4288, 6279: 4289, 6280: 4290, 6281: 4291, 6283: 4292, 6285: 4293, 6286: 4294, 6287: 4295, 6288: 4296, 6289: 4297, 6290: 4298, 6291: 4299, 6294: 4300, 6295: 4301, 6296: 4302, 6297: 4303, 6298: 4304, 6299: 4305, 6300: 4306, 6301: 4307, 6302: 4308, 6303: 4309, 6305: 4310, 6306: 4311, 6308: 4312, 6310: 4313, 6313: 4314, 6314: 4315, 6315: 4316, 6316: 4317, 6319: 4318, 6320: 4319, 6322: 4320, 6323: 4321, 6324: 4322, 6327: 4323, 6329: 4324, 6330: 4325, 6331: 4326, 6332: 4327, 6333: 4328, 6334: 4329, 6335: 4330, 6336: 4331, 6337: 4332, 6338: 4333, 6339: 4334, 6341: 4335, 6342: 4336, 6344: 4337, 6345: 4338, 6347: 4339, 6348: 4340, 6349: 4341, 6350: 4342, 6357: 4343, 6358: 4344, 6365: 4345, 6367: 4346, 6368: 4347, 6370: 4348, 6371: 4349, 6373: 4350, 6374: 4351, 6375: 4352, 6376: 4353, 6377: 4354, 6378: 4355, 6379: 4356, 6380: 4357, 6382: 4358, 6383: 4359, 6385: 4360, 6386: 4361, 6387: 4362, 6390: 4363, 6395: 4364, 6400: 4365, 6402: 4366, 6405: 4367, 6407: 4368, 6408: 4369, 6410: 4370, 6412: 4371, 6415: 4372, 6417: 4373, 6422: 4374, 6424: 4375, 6425: 4376, 6427: 4377, 6428: 4378, 6429: 4379, 6433: 4380, 6434: 4381, 6436: 4382, 6440: 4383, 6442: 4384, 6448: 4385, 6449: 4386, 6452: 4387, 6453: 4388, 6454: 4389, 6460: 4390, 6461: 4391, 6464: 4392, 6465: 4393, 6466: 4394, 6476: 4395, 6477: 4396, 6480: 4397, 6482: 4398, 6483: 4399, 6484: 4400, 6493: 4401, 6502: 4402, 6503: 4403, 6506: 4404, 6509: 4405, 6510: 4406, 6514: 4407, 6515: 4408, 6516: 4409, 6517: 4410, 6523: 4411, 6527: 4412, 6528: 4413, 6530: 4414, 6533: 4415, 6534: 4416, 6535: 4417, 6536: 4418, 6537: 4419, 6538: 4420, 6539: 4421, 6541: 4422, 6542: 4423, 6545: 4424, 6547: 4425, 6548: 4426, 6549: 4427, 6550: 4428, 6551: 4429, 6552: 4430, 6553: 4431, 6554: 4432, 6557: 4433, 6558: 4434, 6559: 4435, 6560: 4436, 6561: 4437, 6563: 4438, 6564: 4439, 6565: 4440, 6566: 4441, 6567: 4442, 6568: 4443, 6571: 4444, 6572: 4445, 6573: 4446, 6574: 4447, 6577: 4448, 6578: 4449, 6579: 4450, 6581: 4451, 6582: 4452, 6583: 4453, 6584: 4454, 6586: 4455, 6587: 4456, 6588: 4457, 6591: 4458, 6592: 4459, 6593: 4460, 6595: 4461, 6596: 4462, 6597: 4463, 6598: 4464, 6603: 4465, 6604: 4466, 6609: 4467, 6611: 4468, 6612: 4469, 6614: 4470, 6615: 4471, 6616: 4472, 6617: 4473, 6618: 4474, 6619: 4475, 6620: 4476, 6624: 4477, 6625: 4478, 6628: 4479, 6629: 4480, 6631: 4481, 6636: 4482, 6638: 4483, 6639: 4484, 6643: 4485, 6644: 4486, 6645: 4487, 6650: 4488, 6654: 4489, 6656: 4490, 6658: 4491, 6659: 4492, 6660: 4493, 6662: 4494, 6663: 4495, 6664: 4496, 6665: 4497, 6666: 4498, 6667: 4499, 6669: 4500, 6670: 4501, 6671: 4502, 6678: 4503, 6679: 4504, 6684: 4505, 6686: 4506, 6687: 4507, 6688: 4508, 6689: 4509, 6691: 4510, 6692: 4511, 6695: 4512, 6696: 4513, 6699: 4514, 6702: 4515, 6705: 4516, 6706: 4517, 6707: 4518, 6708: 4519, 6709: 4520, 6710: 4521, 6711: 4522, 6713: 4523, 6715: 4524, 6718: 4525, 6720: 4526, 6721: 4527, 6722: 4528, 6723: 4529, 6724: 4530, 6731: 4531, 6732: 4532, 6734: 4533, 6744: 4534, 6746: 4535, 6748: 4536, 6750: 4537, 6751: 4538, 6752: 4539, 6753: 4540, 6754: 4541, 6755: 4542, 6760: 4543, 6763: 4544, 6764: 4545, 6765: 4546, 6768: 4547, 6769: 4548, 6770: 4549, 6772: 4550, 6773: 4551, 6774: 4552, 6775: 4553, 6776: 4554, 6777: 4555, 6780: 4556, 6782: 4557, 6783: 4558, 6785: 4559, 6786: 4560, 6787: 4561, 6788: 4562, 6789: 4563, 6790: 4564, 6791: 4565, 6793: 4566, 6794: 4567, 6796: 4568, 6797: 4569, 6798: 4570, 6800: 4571, 6803: 4572, 6804: 4573, 6807: 4574, 6808: 4575, 6809: 4576, 6810: 4577, 6811: 4578, 6812: 4579, 6814: 4580, 6816: 4581, 6817: 4582, 6818: 4583, 6820: 4584, 6821: 4585, 6827: 4586, 6832: 4587, 6835: 4588, 6836: 4589, 6837: 4590, 6850: 4591, 6851: 4592, 6852: 4593, 6853: 4594, 6856: 4595, 6857: 4596, 6858: 4597, 6860: 4598, 6862: 4599, 6863: 4600, 6867: 4601, 6868: 4602, 6869: 4603, 6870: 4604, 6872: 4605, 6873: 4606, 6874: 4607, 6879: 4608, 6880: 4609, 6881: 4610, 6882: 4611, 6883: 4612, 6884: 4613, 6885: 4614, 6886: 4615, 6887: 4616, 6888: 4617, 6889: 4618, 6890: 4619, 6893: 4620, 6898: 4621, 6899: 4622, 6902: 4623, 6909: 4624, 6911: 4625, 6912: 4626, 6918: 4627, 6920: 4628, 6927: 4629, 6932: 4630, 6934: 4631, 6935: 4632, 6936: 4633, 6938: 4634, 6939: 4635, 6942: 4636, 6944: 4637, 6945: 4638, 6946: 4639, 6947: 4640, 6948: 4641, 6949: 4642, 6950: 4643, 6951: 4644, 6952: 4645, 6953: 4646, 6954: 4647, 6957: 4648, 6958: 4649, 6959: 4650, 6961: 4651, 6962: 4652, 6963: 4653, 6965: 4654, 6966: 4655, 6967: 4656, 6969: 4657, 6970: 4658, 6971: 4659, 6973: 4660, 6974: 4661, 6975: 4662, 6978: 4663, 6979: 4664, 6981: 4665, 6982: 4666, 6983: 4667, 6984: 4668, 6985: 4669, 6986: 4670, 6987: 4671, 6989: 4672, 6990: 4673, 6992: 4674, 6993: 4675, 6994: 4676, 6996: 4677, 6997: 4678, 6998: 4679, 6999: 4680, 7000: 4681, 7001: 4682, 7004: 4683, 7005: 4684, 7007: 4685, 7008: 4686, 7009: 4687, 7010: 4688, 7012: 4689, 7013: 4690, 7015: 4691, 7016: 4692, 7017: 4693, 7018: 4694, 7019: 4695, 7022: 4696, 7023: 4697, 7024: 4698, 7025: 4699, 7026: 4700, 7027: 4701, 7028: 4702, 7030: 4703, 7031: 4704, 7032: 4705, 7033: 4706, 7034: 4707, 7036: 4708, 7037: 4709, 7038: 4710, 7040: 4711, 7041: 4712, 7044: 4713, 7045: 4714, 7046: 4715, 7047: 4716, 7048: 4717, 7049: 4718, 7050: 4719, 7051: 4720, 7052: 4721, 7053: 4722, 7054: 4723, 7055: 4724, 7056: 4725, 7057: 4726, 7058: 4727, 7059: 4728, 7060: 4729, 7061: 4730, 7062: 4731, 7063: 4732, 7064: 4733, 7065: 4734, 7067: 4735, 7069: 4736, 7070: 4737, 7071: 4738, 7072: 4739, 7073: 4740, 7074: 4741, 7075: 4742, 7076: 4743, 7078: 4744, 7079: 4745, 7080: 4746, 7082: 4747, 7083: 4748, 7084: 4749, 7085: 4750, 7086: 4751, 7087: 4752, 7088: 4753, 7089: 4754, 7090: 4755, 7091: 4756, 7092: 4757, 7093: 4758, 7096: 4759, 7099: 4760, 7101: 4761, 7102: 4762, 7103: 4763, 7104: 4764, 7107: 4765, 7108: 4766, 7109: 4767, 7114: 4768, 7115: 4769, 7116: 4770, 7117: 4771, 7118: 4772, 7121: 4773, 7122: 4774, 7123: 4775, 7124: 4776, 7125: 4777, 7127: 4778, 7131: 4779, 7132: 4780, 7134: 4781, 7137: 4782, 7139: 4783, 7141: 4784, 7142: 4785, 7143: 4786, 7147: 4787, 7149: 4788, 7150: 4789, 7151: 4790, 7153: 4791, 7154: 4792, 7155: 4793, 7156: 4794, 7158: 4795, 7160: 4796, 7161: 4797, 7162: 4798, 7163: 4799, 7164: 4800, 7165: 4801, 7167: 4802, 7169: 4803, 7171: 4804, 7173: 4805, 7175: 4806, 7176: 4807, 7177: 4808, 7178: 4809, 7179: 4810, 7181: 4811, 7184: 4812, 7190: 4813, 7191: 4814, 7192: 4815, 7193: 4816, 7202: 4817, 7205: 4818, 7206: 4819, 7208: 4820, 7209: 4821, 7211: 4822, 7212: 4823, 7215: 4824, 7216: 4825, 7217: 4826, 7218: 4827, 7219: 4828, 7222: 4829, 7223: 4830, 7225: 4831, 7228: 4832, 7234: 4833, 7235: 4834, 7236: 4835, 7238: 4836, 7243: 4837, 7245: 4838, 7247: 4839, 7248: 4840, 7251: 4841, 7254: 4842, 7255: 4843, 7256: 4844, 7257: 4845, 7258: 4846, 7259: 4847, 7260: 4848, 7261: 4849, 7262: 4850, 7263: 4851, 7264: 4852, 7265: 4853, 7266: 4854, 7282: 4855, 7285: 4856, 7292: 4857, 7293: 4858, 7294: 4859, 7297: 4860, 7299: 4861, 7301: 4862, 7302: 4863, 7303: 4864, 7304: 4865, 7305: 4866, 7306: 4867, 7307: 4868, 7308: 4869, 7310: 4870, 7311: 4871, 7312: 4872, 7315: 4873, 7316: 4874, 7317: 4875, 7318: 4876, 7319: 4877, 7320: 4878, 7321: 4879, 7323: 4880, 7324: 4881, 7325: 4882, 7326: 4883, 7327: 4884, 7328: 4885, 7333: 4886, 7335: 4887, 7336: 4888, 7340: 4889, 7344: 4890, 7345: 4891, 7346: 4892, 7347: 4893, 7348: 4894, 7349: 4895, 7352: 4896, 7354: 4897, 7357: 4898, 7360: 4899, 7361: 4900, 7362: 4901, 7364: 4902, 7366: 4903, 7367: 4904, 7368: 4905, 7369: 4906, 7371: 4907, 7372: 4908, 7373: 4909, 7375: 4910, 7376: 4911, 7377: 4912, 7379: 4913, 7380: 4914, 7381: 4915, 7382: 4916, 7386: 4917, 7387: 4918, 7390: 4919, 7394: 4920, 7395: 4921, 7396: 4922, 7411: 4923, 7414: 4924, 7419: 4925, 7438: 4926, 7439: 4927, 7440: 4928, 7443: 4929, 7444: 4930, 7445: 4931, 7448: 4932, 7449: 4933, 7450: 4934, 7451: 4935, 7454: 4936, 7455: 4937, 7457: 4938, 7458: 4939, 7459: 4940, 7460: 4941, 7477: 4942, 7479: 4943, 7481: 4944, 7482: 4945, 7486: 4946, 7487: 4947, 7492: 4948, 7493: 4949, 7541: 4950, 7560: 4951, 7561: 4952, 7562: 4953, 7564: 4954, 7566: 4955, 7569: 4956, 7570: 4957, 7572: 4958, 7573: 4959, 7579: 4960, 7581: 4961, 7584: 4962, 7587: 4963, 7614: 4964, 7615: 4965, 7616: 4966, 7618: 4967, 7619: 4968, 7620: 4969, 7624: 4970, 7636: 4971, 7646: 4972, 7647: 4973, 7649: 4974, 7650: 4975, 7657: 4976, 7669: 4977, 7698: 4978, 7700: 4979, 7701: 4980, 7702: 4981, 7704: 4982, 7705: 4983, 7706: 4984, 7707: 4985, 7708: 4986, 7713: 4987, 7714: 4988, 7716: 4989, 7720: 4990, 7727: 4991, 7728: 4992, 7730: 4993, 7742: 4994, 7743: 4995, 7745: 4996, 7748: 4997, 7749: 4998, 7753: 4999, 7756: 5000, 7757: 5001, 7762: 5002, 7766: 5003, 7767: 5004, 7772: 5005, 7773: 5006, 7781: 5007, 7782: 5008, 7786: 5009, 7789: 5010, 7802: 5011, 7810: 5012, 7811: 5013, 7812: 5014, 7815: 5015, 7820: 5016, 7822: 5017, 7826: 5018, 7831: 5019, 7832: 5020, 7833: 5021, 7834: 5022, 7835: 5023, 7839: 5024, 7840: 5025, 7841: 5026, 7842: 5027, 7843: 5028, 7844: 5029, 7845: 5030, 7846: 5031, 7879: 5032, 7882: 5033, 7883: 5034, 7884: 5035, 7888: 5036, 7889: 5037, 7891: 5038, 7894: 5039, 7895: 5040, 7896: 5041, 7899: 5042, 7900: 5043, 7916: 5044, 7920: 5045, 7924: 5046, 7925: 5047, 7926: 5048, 7930: 5049, 7932: 5050, 7934: 5051, 7936: 5052, 7937: 5053, 7938: 5054, 7939: 5055, 7940: 5056, 7943: 5057, 7944: 5058, 7951: 5059, 7976: 5060, 7979: 5061, 7980: 5062, 7981: 5063, 7982: 5064, 7983: 5065, 7984: 5066, 7986: 5067, 7987: 5068, 7988: 5069, 7991: 5070, 7993: 5071, 8008: 5072, 8010: 5073, 8011: 5074, 8012: 5075, 8014: 5076, 8015: 5077, 8016: 5078, 8019: 5079, 8024: 5080, 8025: 5081, 8033: 5082, 8035: 5083, 8042: 5084, 8043: 5085, 8044: 5086, 8045: 5087, 8057: 5088, 8092: 5089, 8093: 5090, 8094: 5091, 8117: 5092, 8118: 5093, 8119: 5094, 8121: 5095, 8125: 5096, 8126: 5097, 8128: 5098, 8131: 5099, 8132: 5100, 8136: 5101, 8137: 5102, 8138: 5103, 8142: 5104, 8143: 5105, 8147: 5106, 8153: 5107, 8154: 5108, 8157: 5109, 8158: 5110, 8167: 5111, 8169: 5112, 8183: 5113, 8188: 5114, 8189: 5115, 8190: 5116, 8191: 5117, 8195: 5118, 8196: 5119, 8197: 5120, 8199: 5121, 8207: 5122, 8225: 5123, 8228: 5124, 8232: 5125, 8235: 5126, 8236: 5127, 8238: 5128, 8239: 5129, 8241: 5130, 8253: 5131, 8254: 5132, 8261: 5133, 8264: 5134, 8266: 5135, 8268: 5136, 8270: 5137, 8275: 5138, 8290: 5139, 8293: 5140, 8302: 5141, 8327: 5142, 8331: 5143, 8335: 5144, 8336: 5145, 8337: 5146, 8338: 5147, 8340: 5148, 8341: 5149, 8360: 5150, 8361: 5151, 8362: 5152, 8363: 5153, 8364: 5154, 8366: 5155, 8368: 5156, 8369: 5157, 8370: 5158, 8371: 5159, 8372: 5160, 8373: 5161, 8375: 5162, 8376: 5163, 8378: 5164, 8379: 5165, 8380: 5166, 8382: 5167, 8383: 5168, 8385: 5169, 8387: 5170, 8391: 5171, 8402: 5172, 8405: 5173, 8410: 5174, 8424: 5175, 8425: 5176, 8426: 5177, 8427: 5178, 8446: 5179, 8447: 5180, 8450: 5181, 8451: 5182, 8454: 5183, 8456: 5184, 8458: 5185, 8459: 5186, 8461: 5187, 8462: 5188, 8463: 5189, 8464: 5190, 8465: 5191, 8477: 5192, 8481: 5193, 8482: 5194, 8484: 5195, 8487: 5196, 8491: 5197, 8492: 5198, 8493: 5199, 8494: 5200, 8495: 5201, 8500: 5202, 8501: 5203, 8502: 5204, 8506: 5205, 8507: 5206, 8511: 5207, 8512: 5208, 8518: 5209, 8521: 5210, 8526: 5211, 8528: 5212, 8529: 5213, 8530: 5214, 8531: 5215, 8532: 5216, 8533: 5217, 8534: 5218, 8535: 5219, 8542: 5220, 8571: 5221, 8574: 5222, 8575: 5223, 8577: 5224, 8578: 5225, 8580: 5226, 8581: 5227, 8582: 5228, 8587: 5229, 8588: 5230, 8591: 5231, 8593: 5232, 8596: 5233, 8600: 5234, 8601: 5235, 8604: 5236, 8605: 5237, 8607: 5238, 8609: 5239, 8610: 5240, 8611: 5241, 8614: 5242, 8617: 5243, 8620: 5244, 8622: 5245, 8623: 5246, 8626: 5247, 8632: 5248, 8633: 5249, 8636: 5250, 8638: 5251, 8640: 5252, 8641: 5253, 8643: 5254, 8644: 5255, 8645: 5256, 8650: 5257, 8656: 5258, 8665: 5259, 8666: 5260, 8667: 5261, 8670: 5262, 8677: 5263, 8684: 5264, 8685: 5265, 8690: 5266, 8695: 5267, 8711: 5268, 8712: 5269, 8713: 5270, 8714: 5271, 8718: 5272, 8720: 5273, 8724: 5274, 8727: 5275, 8730: 5276, 8738: 5277, 8743: 5278, 8748: 5279, 8753: 5280, 8754: 5281, 8755: 5282, 8771: 5283, 8772: 5284, 8773: 5285, 8774: 5286, 8778: 5287, 8781: 5288, 8782: 5289, 8783: 5290, 8784: 5291, 8795: 5292, 8796: 5293, 8798: 5294, 8799: 5295, 8800: 5296, 8804: 5297, 8807: 5298, 8808: 5299, 8809: 5300, 8810: 5301, 8813: 5302, 8814: 5303, 8815: 5304, 8827: 5305, 8830: 5306, 8831: 5307, 8832: 5308, 8833: 5309, 8835: 5310, 8836: 5311, 8838: 5312, 8840: 5313, 8844: 5314, 8848: 5315, 8860: 5316, 8861: 5317, 8864: 5318, 8865: 5319, 8866: 5320, 8869: 5321, 8870: 5322, 8873: 5323, 8874: 5324, 8875: 5325, 8879: 5326, 8880: 5327, 8894: 5328, 8905: 5329, 8906: 5330, 8907: 5331, 8908: 5332, 8910: 5333, 8911: 5334, 8912: 5335, 8914: 5336, 8915: 5337, 8916: 5338, 8917: 5339, 8918: 5340, 8919: 5341, 8920: 5342, 8921: 5343, 8928: 5344, 8933: 5345, 8934: 5346, 8937: 5347, 8938: 5348, 8939: 5349, 8943: 5350, 8946: 5351, 8947: 5352, 8948: 5353, 8949: 5354, 8950: 5355, 8951: 5356, 8952: 5357, 8954: 5358, 8955: 5359, 8957: 5360, 8958: 5361, 8959: 5362, 8961: 5363, 8964: 5364, 8965: 5365, 8966: 5366, 8967: 5367, 8968: 5368, 8969: 5369, 8970: 5370, 8972: 5371, 8973: 5372, 8974: 5373, 8977: 5374, 8978: 5375, 8979: 5376, 8981: 5377, 8982: 5378, 8983: 5379, 8984: 5380, 8985: 5381, 8987: 5382, 8989: 5383, 8998: 5384, 9004: 5385, 9005: 5386, 9008: 5387, 9010: 5388, 9018: 5389, 25746: 5390, 25750: 5391, 25752: 5392, 25753: 5393, 25757: 5394, 25769: 5395, 25771: 5396, 25773: 5397, 25782: 5398, 25788: 5399, 25795: 5400, 25797: 5401, 25805: 5402, 25825: 5403, 25826: 5404, 25827: 5405, 25833: 5406, 25834: 5407, 25841: 5408, 25850: 5409, 25856: 5410, 25865: 5411, 25870: 5412, 25886: 5413, 25887: 5414, 25898: 5415, 25905: 5416, 25906: 5417, 25923: 5418, 25927: 5419, 25937: 5420, 25940: 5421, 25946: 5422, 25947: 5423, 25952: 5424, 25959: 5425, 25962: 5426, 25963: 5427, 25996: 5428, 25999: 5429, 26002: 5430, 26003: 5431, 26038: 5432, 26048: 5433, 26052: 5434, 26059: 5435, 26073: 5436, 26078: 5437, 26082: 5438, 26084: 5439, 26095: 5440, 26116: 5441, 26122: 5442, 26124: 5443, 26131: 5444, 26133: 5445, 26142: 5446, 26147: 5447, 26150: 5448, 26151: 5449, 26152: 5450, 26158: 5451, 26159: 5452, 26169: 5453, 26171: 5454, 26172: 5455, 26176: 5456, 26183: 5457, 26184: 5458, 26195: 5459, 26198: 5460, 26225: 5461, 26231: 5462, 26236: 5463, 26237: 5464, 26242: 5465, 26249: 5466, 26258: 5467, 26265: 5468, 26283: 5469, 26285: 5470, 26303: 5471, 26308: 5472, 26313: 5473, 26317: 5474, 26322: 5475, 26326: 5476, 26340: 5477, 26344: 5478, 26347: 5479, 26350: 5480, 26357: 5481, 26359: 5482, 26365: 5483, 26366: 5484, 26375: 5485, 26386: 5486, 26391: 5487, 26394: 5488, 26399: 5489, 26401: 5490, 26409: 5491, 26413: 5492, 26422: 5493, 26429: 5494, 26444: 5495, 26453: 5496, 26464: 5497, 26467: 5498, 26471: 5499, 26472: 5500, 26492: 5501, 26498: 5502, 26504: 5503, 26510: 5504, 26523: 5505, 26524: 5506, 26527: 5507, 26528: 5508, 26539: 5509, 26542: 5510, 26547: 5511, 26554: 5512, 26555: 5513, 26562: 5514, 26564: 5515, 26567: 5516, 26578: 5517, 26585: 5518, 26587: 5519, 26590: 5520, 26593: 5521, 26599: 5522, 26603: 5523, 26606: 5524, 26612: 5525, 26614: 5526, 26622: 5527, 26629: 5528, 26630: 5529, 26631: 5530, 26645: 5531, 26649: 5532, 26662: 5533, 26676: 5534, 26680: 5535, 26681: 5536, 26686: 5537, 26693: 5538, 26694: 5539, 26695: 5540, 26696: 5541, 26700: 5542, 26701: 5543, 26704: 5544, 26712: 5545, 26713: 5546, 26717: 5547, 26726: 5548, 26729: 5549, 26732: 5550, 26736: 5551, 26741: 5552, 26743: 5553, 26745: 5554, 26750: 5555, 26761: 5556, 26764: 5557, 26765: 5558, 26776: 5559, 26777: 5560, 26778: 5561, 26782: 5562, 26791: 5563, 26792: 5564, 26796: 5565, 26797: 5566, 26810: 5567, 26812: 5568, 26819: 5569, 26828: 5570, 26838: 5571, 26840: 5572, 26849: 5573, 26854: 5574, 26861: 5575, 26865: 5576, 26870: 5577, 26871: 5578, 26875: 5579, 26887: 5580, 26900: 5581, 26901: 5582, 26903: 5583, 26913: 5584, 26928: 5585, 26940: 5586, 26947: 5587, 26958: 5588, 26965: 5589, 26974: 5590, 26985: 5591, 26999: 5592, 27002: 5593, 27003: 5594, 27006: 5595, 27008: 5596, 27020: 5597, 27022: 5598, 27032: 5599, 27036: 5600, 27074: 5601, 27075: 5602, 27105: 5603, 27124: 5604, 27134: 5605, 27140: 5606, 27155: 5607, 27156: 5608, 27176: 5609, 27178: 5610, 27186: 5611, 27193: 5612, 27246: 5613, 27251: 5614, 27255: 5615, 27266: 5616, 27306: 5617, 27311: 5618, 27317: 5619, 27320: 5620, 27328: 5621, 27329: 5622, 27368: 5623, 27369: 5624, 27370: 5625, 27373: 5626, 27397: 5627, 27408: 5628, 27416: 5629, 27420: 5630, 27426: 5631, 27434: 5632, 27441: 5633, 27450: 5634, 27473: 5635, 27478: 5636, 27480: 5637, 27482: 5638, 27491: 5639, 27513: 5640, 27523: 5641, 27537: 5642, 27539: 5643, 27549: 5644, 27555: 5645, 27563: 5646, 27584: 5647, 27592: 5648, 27595: 5649, 27604: 5650, 27611: 5651, 27618: 5652, 27619: 5653, 27627: 5654, 27644: 5655, 27660: 5656, 27664: 5657, 27667: 5658, 27674: 5659, 27683: 5660, 27685: 5661, 27689: 5662, 27692: 5663, 27695: 5664, 27704: 5665, 27705: 5666, 27706: 5667, 27708: 5668, 27716: 5669, 27721: 5670, 27722: 5671, 27728: 5672, 27731: 5673, 27741: 5674, 27744: 5675, 27746: 5676, 27751: 5677, 27762: 5678, 27768: 5679, 27769: 5680, 27772: 5681, 27773: 5682, 27776: 5683, 27778: 5684, 27784: 5685, 27788: 5686, 27790: 5687, 27793: 5688, 27801: 5689, 27802: 5690, 27803: 5691, 27808: 5692, 27815: 5693, 27816: 5694, 27820: 5695, 27821: 5696, 27822: 5697, 27826: 5698, 27829: 5699, 27830: 5700, 27831: 5701, 27834: 5702, 27837: 5703, 27838: 5704, 27839: 5705, 27846: 5706, 27850: 5707, 27865: 5708, 27866: 5709, 27869: 5710, 27873: 5711, 27878: 5712, 27879: 5713, 27882: 5714, 27899: 5715, 27904: 5716, 27905: 5717, 27912: 5718, 30707: 5719, 30745: 5720, 30749: 5721, 30793: 5722, 30803: 5723, 30810: 5724, 30812: 5725, 30816: 5726, 30818: 5727, 30820: 5728, 30822: 5729, 30825: 5730, 30846: 5731, 30848: 5732, 30850: 5733, 30883: 5734, 30890: 5735, 30894: 5736, 30898: 5737, 30994: 5738, 31000: 5739, 31030: 5740, 31038: 5741, 31049: 5742, 31083: 5743, 31086: 5744, 31101: 5745, 31114: 5746, 31116: 5747, 31123: 5748, 31150: 5749, 31162: 5750, 31184: 5751, 31193: 5752, 31221: 5753, 31223: 5754, 31225: 5755, 31260: 5756, 31297: 5757, 31309: 5758, 31364: 5759, 31367: 5760, 31410: 5761, 31420: 5762, 31422: 5763, 31424: 5764, 31427: 5765, 31431: 5766, 31433: 5767, 31435: 5768, 31437: 5769, 31445: 5770, 31522: 5771, 31545: 5772, 31553: 5773, 31590: 5774, 31610: 5775, 31617: 5776, 31658: 5777, 31660: 5778, 31664: 5779, 31685: 5780, 31692: 5781, 31694: 5782, 31696: 5783, 31698: 5784, 31700: 5785, 31702: 5786, 31737: 5787, 31804: 5788, 31851: 5789, 31867: 5790, 31878: 5791, 31903: 5792, 31909: 5793, 31921: 5794, 31923: 5795, 31925: 5796, 31952: 5797, 31973: 5798, 32009: 5799, 32011: 5800, 32017: 5801, 32019: 5802, 32022: 5803, 32029: 5804, 32031: 5805, 32058: 5806, 32060: 5807, 32116: 5808, 32139: 5809, 32179: 5810, 32213: 5811, 32234: 5812, 32243: 5813, 32289: 5814, 32291: 5815, 32294: 5816, 32296: 5817, 32298: 5818, 32300: 5819, 32302: 5820, 32314: 5821, 32387: 5822, 32392: 5823, 32440: 5824, 32442: 5825, 32456: 5826, 32460: 5827, 32469: 5828, 32511: 5829, 32515: 5830, 32554: 5831, 32582: 5832, 32584: 5833, 32587: 5834, 32589: 5835, 32596: 5836, 32598: 5837, 32600: 5838, 32620: 5839, 32632: 5840, 32649: 5841, 32657: 5842, 32659: 5843, 32666: 5844, 32728: 5845, 32743: 5846, 32770: 5847, 32799: 5848, 32862: 5849, 32875: 5850, 32892: 5851, 32898: 5852, 32906: 5853, 32914: 5854, 32917: 5855, 33004: 5856, 33085: 5857, 33090: 5858, 33124: 5859, 33126: 5860, 33132: 5861, 33138: 5862, 33145: 5863, 33148: 5864, 33154: 5865, 33158: 5866, 33162: 5867, 33164: 5868, 33166: 5869, 33171: 5870, 33188: 5871, 33201: 5872, 33237: 5873, 33294: 5874, 33310: 5875, 33312: 5876, 33421: 5877, 33435: 5878, 33437: 5879, 33493: 5880, 33495: 5881, 33499: 5882, 33558: 5883, 33564: 5884, 33615: 5885, 33629: 5886, 33639: 5887, 33644: 5888, 33646: 5889, 33649: 5890, 33660: 5891, 33669: 5892, 33672: 5893, 33677: 5894, 33679: 5895, 33681: 5896, 33683: 5897, 33725: 5898, 33779: 5899, 33781: 5900, 33794: 5901, 33801: 5902, 33815: 5903, 33826: 5904, 33830: 5905, 33834: 5906, 33836: 5907, 33838: 5908, 33880: 5909, 33893: 5910, 33896: 5911, 33903: 5912, 33966: 5913, 34018: 5914, 34048: 5915, 34072: 5916, 34129: 5917, 34143: 5918, 34148: 5919, 34150: 5920, 34153: 5921, 34162: 5922, 34164: 5923, 34271: 5924, 34292: 5925, 34312: 5926, 34319: 5927, 34321: 5928, 34323: 5929, 34326: 5930, 34330: 5931, 34332: 5932, 34334: 5933, 34336: 5934, 34338: 5935, 34359: 5936, 34397: 5937, 34405: 5938, 34437: 5939, 34450: 5940, 34520: 5941, 34523: 5942, 34528: 5943, 34530: 5944, 34532: 5945, 34534: 5946, 34536: 5947, 34540: 5948, 34542: 5949, 34800: 5950, 34811: 5951, 35015: 5952, 35347: 5953, 35807: 5954, 35836: 5955, 35957: 5956, 36276: 5957, 36289: 5958, 36363: 5959, 36397: 5960, 36401: 5961, 36477: 5962, 36509: 5963, 36517: 5964, 36519: 5965, 36525: 5966, 36527: 5967, 36529: 5968, 36533: 5969, 36535: 5970, 36537: 5971, 36708: 5972, 36850: 5973, 36931: 5974, 37211: 5975, 37240: 5976, 37380: 5977, 37382: 5978, 37384: 5979, 37386: 5980, 37444: 5981, 37475: 5982, 37477: 5983, 37495: 5984, 37545: 5985, 37720: 5986, 37727: 5987, 37729: 5988, 37731: 5989, 37733: 5990, 37736: 5991, 37739: 5992, 37741: 5993, 37830: 5994, 37844: 5995, 37853: 5996, 37857: 5997, 38038: 5998, 38061: 5999, 38095: 6000, 38159: 6001, 38164: 6002, 38198: 6003, 38294: 6004, 38304: 6005, 38388: 6006, 38583: 6007, 38798: 6008, 38886: 6009, 38992: 6010, 39183: 6011, 39231: 6012, 39234: 6013, 39292: 6014, 39307: 6015, 39381: 6016, 39400: 6017, 39414: 6018, 39427: 6019, 39435: 6020, 39444: 6021, 39446: 6022, 39449: 6023, 39516: 6024, 39715: 6025, 39801: 6026, 39869: 6027, 40148: 6028, 40278: 6029, 40339: 6030, 40412: 6031, 40414: 6032, 40478: 6033, 40491: 6034, 40578: 6035, 40581: 6036, 40583: 6037, 40597: 6038, 40614: 6039, 40617: 6040, 40629: 6041, 40697: 6042, 40723: 6043, 40732: 6044, 40815: 6045, 40819: 6046, 40826: 6047, 40851: 6048, 40870: 6049, 40946: 6050, 40955: 6051, 40959: 6052, 40962: 6053, 40966: 6054, 41014: 6055, 41285: 6056, 41527: 6057, 41566: 6058, 41569: 6059, 41571: 6060, 41573: 6061, 41617: 6062, 41627: 6063, 41712: 6064, 41716: 6065, 41724: 6066, 41769: 6067, 41828: 6068, 41863: 6069, 41997: 6070, 42002: 6071, 42004: 6072, 42007: 6073, 42009: 6074, 42011: 6075, 42013: 6076, 42015: 6077, 42018: 6078, 42176: 6079, 42191: 6080, 42285: 6081, 42418: 6082, 42422: 6083, 42556: 6084, 42559: 6085, 42602: 6086, 42632: 6087, 42638: 6088, 42718: 6089, 42723: 6090, 42725: 6091, 42728: 6092, 42730: 6093, 42732: 6094, 42734: 6095, 42738: 6096, 42740: 6097, 42761: 6098, 42943: 6099, 42946: 6100, 43289: 6101, 43333: 6102, 43376: 6103, 43396: 6104, 43419: 6105, 43460: 6106, 43549: 6107, 43556: 6108, 43558: 6109, 43560: 6110, 43677: 6111, 43679: 6112, 43684: 6113, 43708: 6114, 43744: 6115, 43836: 6116, 43869: 6117, 43871: 6118, 43904: 6119, 43908: 6120, 43912: 6121, 43914: 6122, 43917: 6123, 43919: 6124, 43921: 6125, 43928: 6126, 43930: 6127, 43932: 6128, 43936: 6129, 44004: 6130, 44020: 6131, 44022: 6132, 44189: 6133, 44191: 6134, 44193: 6135, 44195: 6136, 44197: 6137, 44199: 6138, 44204: 6139, 44225: 6140, 44238: 6141, 44241: 6142, 44243: 6143, 44301: 6144, 44397: 6145, 44399: 6146, 44511: 6147, 44555: 6148, 44597: 6149, 44613: 6150, 44633: 6151, 44657: 6152, 44665: 6153, 44694: 6154, 44709: 6155, 44719: 6156, 44731: 6157, 44759: 6158, 44761: 6159, 44773: 6160, 44777: 6161, 44788: 6162, 44828: 6163, 44840: 6164, 44849: 6165, 44851: 6166, 44864: 6167, 44889: 6168, 44929: 6169, 44931: 6170, 44937: 6171, 44943: 6172, 44972: 6173, 44974: 6174, 45028: 6175, 45062: 6176, 45074: 6177, 45081: 6178, 45106: 6179, 45175: 6180, 45183: 6181, 45186: 6182, 45208: 6183, 45210: 6184, 45221: 6185, 45361: 6186, 45382: 6187, 45431: 6188, 45440: 6189, 45442: 6190, 45447: 6191, 45499: 6192, 45501: 6193, 45503: 6194, 45517: 6195, 45635: 6196, 45648: 6197, 45658: 6198, 45662: 6199, 45666: 6200, 45668: 6201, 45672: 6202, 45720: 6203, 45722: 6204, 45726: 6205, 45728: 6206, 45730: 6207, 45732: 6208, 45880: 6209, 45928: 6210, 45950: 6211, 45969: 6212, 46062: 6213, 46105: 6214, 46231: 6215, 46322: 6216, 46335: 6217, 46337: 6218, 46347: 6219, 46367: 6220, 46530: 6221, 46559: 6222, 46572: 6223, 46574: 6224, 46578: 6225, 46664: 6226, 46723: 6227, 46772: 6228, 46850: 6229, 46855: 6230, 46862: 6231, 46865: 6232, 46948: 6233, 46965: 6234, 46967: 6235, 46970: 6236, 46972: 6237, 46974: 6238, 46976: 6239, 47044: 6240, 47099: 6241, 47122: 6242, 47124: 6243, 47200: 6244, 47202: 6245, 47254: 6246, 47261: 6247, 47382: 6248, 47384: 6249, 47404: 6250, 47423: 6251, 47446: 6252, 47465: 6253, 47491: 6254, 47516: 6255, 47518: 6256, 47538: 6257, 47566: 6258, 47610: 6259, 47629: 6260, 47640: 6261, 47644: 6262, 47646: 6263, 47721: 6264, 47725: 6265, 47736: 6266, 47774: 6267, 47793: 6268, 47810: 6269, 47894: 6270, 47937: 6271, 47950: 6272, 47952: 6273, 47970: 6274, 47978: 6275, 47997: 6276, 47999: 6277, 48001: 6278, 48032: 6279, 48043: 6280, 48045: 6281, 48082: 6282, 48142: 6283, 48150: 6284, 48161: 6285, 48214: 6286, 48262: 6287, 48304: 6288, 48319: 6289, 48322: 6290, 48326: 6291, 48342: 6292, 48385: 6293, 48394: 6294, 48412: 6295, 48414: 6296, 48416: 6297, 48516: 6298, 48518: 6299, 48520: 6300, 48560: 6301, 48593: 6302, 48596: 6303, 48598: 6304, 48638: 6305, 48649: 6306, 48678: 6307, 48696: 6308, 48698: 6309, 48738: 6310, 48741: 6311, 48744: 6312, 48774: 6313, 48780: 6314, 48783: 6315, 48872: 6316, 48877: 6317, 48879: 6318, 48883: 6319, 48982: 6320, 48997: 6321, 49013: 6322, 49110: 6323, 49130: 6324, 49132: 6325, 49220: 6326, 49263: 6327, 49265: 6328, 49272: 6329, 49274: 6330, 49276: 6331, 49278: 6332, 49280: 6333, 49284: 6334, 49286: 6335, 49314: 6336, 49347: 6337, 49389: 6338, 49396: 6339, 49524: 6340, 49530: 6341, 49647: 6342, 49649: 6343, 49651: 6344, 49666: 6345, 49688: 6346, 49735: 6347, 49772: 6348, 49793: 6349, 49822: 6350, 49824: 6351, 49910: 6352, 49917: 6353, 49932: 6354, 49957: 6355, 49961: 6356, 50003: 6357, 50005: 6358, 50064: 6359, 50068: 6360, 50147: 6361, 50158: 6362, 50160: 6363, 50189: 6364, 50274: 6365, 50354: 6366, 50356: 6367, 50440: 6368, 50442: 6369, 50445: 6370, 50514: 6371, 50601: 6372, 50610: 6373, 50613: 6374, 50658: 6375, 50685: 6376, 50740: 6377, 50792: 6378, 50794: 6379, 50796: 6380, 50798: 6381, 50800: 6382, 50802: 6383, 50804: 6384, 50806: 6385, 50842: 6386, 50851: 6387, 50872: 6388, 50912: 6389, 50923: 6390, 50942: 6391, 50954: 6392, 50999: 6393, 51024: 6394, 51037: 6395, 51077: 6396, 51080: 6397, 51082: 6398, 51084: 6399, 51086: 6400, 51088: 6401, 51091: 6402, 51167: 6403, 51174: 6404, 51255: 6405, 51314: 6406, 51357: 6407, 51412: 6408, 51471: 6409, 51498: 6410, 51540: 6411, 51545: 6412, 51562: 6413, 51573: 6414, 51575: 6415, 51662: 6416, 51666: 6417, 51694: 6418, 51698: 6419, 51705: 6420, 51709: 6421, 51834: 6422, 51884: 6423, 51903: 6424, 51925: 6425, 51927: 6426, 51931: 6427, 51933: 6428, 51935: 6429, 51937: 6430, 51939: 6431, 52042: 6432, 52241: 6433, 52245: 6434, 52279: 6435, 52281: 6436, 52283: 6437, 52287: 6438, 52299: 6439, 52319: 6440, 52328: 6441, 52375: 6442, 52435: 6443, 52458: 6444, 52462: 6445, 52579: 6446, 52604: 6447, 52644: 6448, 52668: 6449, 52694: 6450, 52712: 6451, 52715: 6452, 52722: 6453, 52724: 6454, 52730: 6455, 52767: 6456, 52784: 6457, 52831: 6458, 52867: 6459, 52885: 6460, 52950: 6461, 52952: 6462, 52967: 6463, 52973: 6464, 52975: 6465, 53000: 6466, 53022: 6467, 53024: 6468, 53121: 6469, 53123: 6470, 53125: 6471, 53127: 6472, 53129: 6473, 53138: 6474, 53140: 6475, 53143: 6476, 53161: 6477, 53280: 6478, 53318: 6479, 53322: 6480, 53326: 6481, 53355: 6482, 53435: 6483, 53447: 6484, 53450: 6485, 53453: 6486, 53460: 6487, 53464: 6488, 53466: 6489, 53468: 6490, 53519: 6491, 53550: 6492, 53574: 6493, 53578: 6494, 53808: 6495, 53883: 6496, 53894: 6497, 53921: 6498, 53953: 6499, 53956: 6500, 53972: 6501, 53974: 6502, 53993: 6503, 53996: 6504, 54001: 6505, 54004: 6506, 54116: 6507, 54121: 6508, 54185: 6509, 54190: 6510, 54256: 6511, 54259: 6512, 54272: 6513, 54274: 6514, 54276: 6515, 54281: 6516, 54286: 6517, 54354: 6518, 54372: 6519, 54503: 6520, 54617: 6521, 54648: 6522, 54686: 6523, 54732: 6524, 54734: 6525, 54736: 6526, 54745: 6527, 54768: 6528, 54771: 6529, 54780: 6530, 54785: 6531, 54787: 6532, 54796: 6533, 54881: 6534, 54908: 6535, 54910: 6536, 54934: 6537, 54962: 6538, 54995: 6539, 54997: 6540, 54999: 6541, 55020: 6542, 55036: 6543, 55052: 6544, 55061: 6545, 55067: 6546, 55069: 6547, 55071: 6548, 55080: 6549, 55094: 6550, 55110: 6551, 55112: 6552, 55116: 6553, 55118: 6554, 55156: 6555, 55167: 6556, 55190: 6557, 55205: 6558, 55207: 6559, 55232: 6560, 55241: 6561, 55245: 6562, 55247: 6563, 55250: 6564, 55253: 6565, 55259: 6566, 55261: 6567, 55267: 6568, 55269: 6569, 55272: 6570, 55274: 6571, 55276: 6572, 55278: 6573, 55280: 6574, 55282: 6575, 55290: 6576, 55292: 6577, 55294: 6578, 55363: 6579, 55391: 6580, 55442: 6581, 55444: 6582, 55451: 6583, 55492: 6584, 55553: 6585, 55555: 6586, 55577: 6587, 55620: 6588, 55687: 6589, 55721: 6590, 55729: 6591, 55765: 6592, 55768: 6593, 55805: 6594, 55814: 6595, 55820: 6596, 55830: 6597, 55844: 6598, 55854: 6599, 55872: 6600, 55908: 6601, 55946: 6602, 55995: 6603, 56003: 6604, 56012: 6605, 56022: 6606, 56060: 6607, 56145: 6608, 56152: 6609, 56156: 6610, 56169: 6611, 56171: 6612, 56174: 6613, 56176: 6614, 56251: 6615, 56274: 6616, 56286: 6617, 56333: 6618, 56336: 6619, 56339: 6620, 56367: 6621, 56379: 6622, 56389: 6623, 56563: 6624, 56587: 6625, 56607: 6626, 56620: 6627, 56715: 6628, 56757: 6629, 56775: 6630, 56782: 6631, 56788: 6632, 56801: 6633, 56805: 6634, 56837: 6635, 56869: 6636, 56908: 6637, 56915: 6638, 56921: 6639, 56941: 6640, 56949: 6641, 57147: 6642, 57183: 6643, 57243: 6644, 57274: 6645, 57326: 6646, 57368: 6647, 57421: 6648, 57499: 6649, 57502: 6650, 57504: 6651, 57522: 6652, 57526: 6653, 57528: 6654, 57532: 6655, 57536: 6656, 57637: 6657, 57640: 6658, 57669: 6659, 57772: 6660, 57843: 6661, 57910: 6662, 57951: 6663, 58025: 6664, 58047: 6665, 58078: 6666, 58103: 6667, 58105: 6668, 58107: 6669, 58154: 6670, 58156: 6671, 58162: 6672, 58191: 6673, 58287: 6674, 58291: 6675, 58293: 6676, 58295: 6677, 58297: 6678, 58299: 6679, 58301: 6680, 58303: 6681, 58306: 6682, 58309: 6683, 58315: 6684, 58332: 6685, 58347: 6686, 58351: 6687, 58376: 6688, 58404: 6689, 58425: 6690, 58492: 6691, 58554: 6692, 58559: 6693, 58627: 6694, 58655: 6695, 58783: 6696, 58803: 6697, 58806: 6698, 58826: 6699, 58839: 6700, 58842: 6701, 58870: 6702, 58876: 6703, 58879: 6704, 58964: 6705, 58972: 6706, 58975: 6707, 58998: 6708, 59014: 6709, 59016: 6710, 59018: 6711, 59022: 6712, 59026: 6713, 59037: 6714, 59103: 6715, 59118: 6716, 59126: 6717, 59129: 6718, 59131: 6719, 59141: 6720, 59143: 6721, 59220: 6722, 59258: 6723, 59295: 6724, 59306: 6725, 59315: 6726, 59333: 6727, 59336: 6728, 59369: 6729, 59387: 6730, 59421: 6731, 59429: 6732, 59440: 6733, 59501: 6734, 59549: 6735, 59604: 6736, 59615: 6737, 59667: 6738, 59725: 6739, 59727: 6740, 59731: 6741, 59738: 6742, 59784: 6743, 59810: 6744, 59814: 6745, 59900: 6746, 59915: 6747, 59947: 6748, 59985: 6749, 59995: 6750, 60030: 6751, 60037: 6752, 60040: 6753, 60046: 6754, 60069: 6755, 60072: 6756, 60074: 6757, 60126: 6758, 60128: 6759, 60141: 6760, 60161: 6761, 60289: 6762, 60291: 6763, 60293: 6764, 60303: 6765, 60333: 6766, 60363: 6767, 60365: 6768, 60389: 6769, 60397: 6770, 60408: 6771, 60471: 6772, 60487: 6773, 60514: 6774, 60516: 6775, 60522: 6776, 60538: 6777, 60647: 6778, 60674: 6779, 60684: 6780, 60735: 6781, 60737: 6782, 60753: 6783, 60756: 6784, 60760: 6785, 60766: 6786, 60803: 6787, 60818: 6788, 60832: 6789, 60857: 6790, 60885: 6791, 60894: 6792, 60904: 6793, 60937: 6794, 60941: 6795, 60943: 6796, 60950: 6797, 60979: 6798, 61011: 6799, 61024: 6800, 61026: 6801, 61071: 6802, 61073: 6803, 61123: 6804, 61132: 6805, 61160: 6806, 61167: 6807, 61210: 6808, 61236: 6809, 61240: 6810, 61246: 6811, 61248: 6812, 61250: 6813, 61255: 6814, 61257: 6815, 61262: 6816, 61289: 6817, 61319: 6818, 61323: 6819, 61348: 6820, 61350: 6821, 61352: 6822, 61394: 6823, 61401: 6824, 61406: 6825, 61465: 6826, 61628: 6827, 61692: 6828, 61697: 6829, 61705: 6830, 61729: 6831, 61818: 6832, 61986: 6833, 62008: 6834, 62081: 6835, 62113: 6836, 62155: 6837, 62208: 6838, 62250: 6839, 62293: 6840, 62299: 6841, 62336: 6842, 62344: 6843, 62374: 6844, 62376: 6845, 62383: 6846, 62394: 6847, 62434: 6848, 62437: 6849, 62439: 6850, 62511: 6851, 62553: 6852, 62586: 6853, 62644: 6854, 62662: 6855, 62718: 6856, 62792: 6857, 62799: 6858, 62834: 6859, 62836: 6860, 62849: 6861, 62956: 6862, 62970: 6863, 62999: 6864, 63033: 6865, 63062: 6866, 63072: 6867, 63082: 6868, 63113: 6869, 63131: 6870, 63179: 6871, 63222: 6872, 63239: 6873, 63276: 6874, 63312: 6875, 63393: 6876, 63433: 6877, 63436: 6878, 63479: 6879, 63515: 6880, 63540: 6881, 63768: 6882, 63808: 6883, 63826: 6884, 63853: 6885, 63859: 6886, 63876: 6887, 63992: 6888, 64010: 6889, 64030: 6890, 64032: 6891, 64034: 6892, 64114: 6893, 64116: 6894, 64167: 6895, 64197: 6896, 64231: 6897, 64249: 6898, 64278: 6899, 64285: 6900, 64497: 6901, 64499: 6902, 64501: 6903, 64575: 6904, 64614: 6905, 64620: 6906, 64622: 6907, 64695: 6908, 64716: 6909, 64839: 6910, 64957: 6911, 64969: 6912, 64983: 6913, 64993: 6914, 64997: 6915, 65037: 6916, 65088: 6917, 65126: 6918, 65130: 6919, 65133: 6920, 65135: 6921, 65188: 6922, 65193: 6923, 65216: 6924, 65225: 6925, 65230: 6926, 65261: 6927, 65350: 6928, 65359: 6929, 65514: 6930, 65577: 6931, 65585: 6932, 65588: 6933, 65596: 6934, 65601: 6935, 65631: 6936, 65642: 6937, 65651: 6938, 65682: 6939, 65685: 6940, 65738: 6941, 65740: 6942, 65802: 6943, 65810: 6944, 65882: 6945, 65982: 6946, 66090: 6947, 66097: 6948, 66171: 6949, 66198: 6950, 66203: 6951, 66240: 6952, 66297: 6953, 66310: 6954, 66320: 6955, 66335: 6956, 66371: 6957, 66427: 6958, 66509: 6959, 66511: 6960, 66544: 6961, 66665: 6962, 66744: 6963, 66783: 6964, 66785: 6965, 66798: 6966, 66915: 6967, 66934: 6968, 66943: 6969, 67087: 6970, 67168: 6971, 67186: 6972, 67193: 6973, 67197: 6974, 67255: 6975, 67267: 6976, 67295: 6977, 67361: 6978, 67408: 6979, 67508: 6980, 67534: 6981, 67618: 6982, 67665: 6983, 67695: 6984, 67734: 6985, 67788: 6986, 67799: 6987, 67888: 6988, 67923: 6989, 67997: 6990, 68073: 6991, 68135: 6992, 68157: 6993, 68159: 6994, 68194: 6995, 68205: 6996, 68237: 6997, 68269: 6998, 68319: 6999, 68347: 7000, 68358: 7001, 68444: 7002, 68480: 7003, 68486: 7004, 68522: 7005, 68536: 7006, 68552: 7007, 68554: 7008, 68597: 7009, 68600: 7010, 68650: 7011, 68659: 7012, 68791: 7013, 68793: 7014, 68835: 7015, 68848: 7016, 68872: 7017, 68886: 7018, 68932: 7019, 68945: 7020, 68952: 7021, 68954: 7022, 68959: 7023, 69069: 7024, 69118: 7025, 69122: 7026, 69131: 7027, 69134: 7028, 69140: 7029, 69211: 7030, 69224: 7031, 69227: 7032, 69251: 7033, 69275: 7034, 69278: 7035, 69304: 7036, 69306: 7037, 69394: 7038, 69406: 7039, 69436: 7040, 69453: 7041, 69469: 7042, 69481: 7043, 69495: 7044, 69516: 7045, 69524: 7046, 69526: 7047, 69529: 7048, 69604: 7049, 69606: 7050, 69640: 7051, 69644: 7052, 69654: 7053, 69685: 7054, 69712: 7055, 69720: 7056, 69746: 7057, 69757: 7058, 69784: 7059, 69805: 7060, 69844: 7061, 69849: 7062, 69860: 7063, 69904: 7064, 69951: 7065, 69953: 7066, 69988: 7067, 70015: 7068, 70159: 7069, 70183: 7070, 70206: 7071, 70208: 7072, 70286: 7073, 70293: 7074, 70301: 7075, 70305: 7076, 70334: 7077, 70336: 7078, 70361: 7079, 70451: 7080, 70492: 7081, 70521: 7082, 70533: 7083, 70545: 7084, 70565: 7085, 70599: 7086, 70637: 7087, 70641: 7088, 70663: 7089, 70687: 7090, 70697: 7091, 70703: 7092, 70708: 7093, 70728: 7094, 70862: 7095, 70932: 7096, 70946: 7097, 70984: 7098, 70990: 7099, 70994: 7100, 71033: 7101, 71057: 7102, 71106: 7103, 71108: 7104, 71129: 7105, 71131: 7106, 71135: 7107, 71147: 7108, 71156: 7109, 71160: 7110, 71205: 7111, 71211: 7112, 71248: 7113, 71252: 7114, 71254: 7115, 71264: 7116, 71268: 7117, 71282: 7118, 71302: 7119, 71304: 7120, 71327: 7121, 71341: 7122, 71379: 7123, 71429: 7124, 71438: 7125, 71453: 7126, 71462: 7127, 71464: 7128, 71466: 7129, 71468: 7130, 71484: 7131, 71494: 7132, 71500: 7133, 71518: 7134, 71520: 7135, 71530: 7136, 71535: 7137, 71550: 7138, 71579: 7139, 71619: 7140, 71640: 7141, 71668: 7142, 71732: 7143, 71745: 7144, 71810: 7145, 71823: 7146, 71838: 7147, 71867: 7148, 71899: 7149, 71902: 7150, 71910: 7151, 71970: 7152, 71999: 7153, 72011: 7154, 72104: 7155, 72129: 7156, 72142: 7157, 72165: 7158, 72167: 7159, 72171: 7160, 72178: 7161, 72224: 7162, 72226: 7163, 72294: 7164, 72308: 7165, 72330: 7166, 72356: 7167, 72378: 7168, 72395: 7169, 72405: 7170, 72407: 7171, 72424: 7172, 72479: 7173, 72489: 7174, 72554: 7175, 72591: 7176, 72601: 7177, 72603: 7178, 72605: 7179, 72624: 7180, 72641: 7181, 72692: 7182, 72694: 7183, 72696: 7184, 72701: 7185, 72714: 7186, 72720: 7187, 72731: 7188, 72733: 7189, 72737: 7190, 72874: 7191, 72919: 7192, 72921: 7193, 72982: 7194, 72998: 7195, 73015: 7196, 73017: 7197, 73023: 7198, 73042: 7199, 73106: 7200, 73160: 7201, 73211: 7202, 73266: 7203, 73268: 7204, 73290: 7205, 73319: 7206, 73321: 7207, 73323: 7208, 73344: 7209, 73386: 7210, 73431: 7211, 73488: 7212, 73499: 7213, 73501: 7214, 73515: 7215, 73569: 7216, 73676: 7217, 73681: 7218, 73741: 7219, 73804: 7220, 73808: 7221, 73822: 7222, 73854: 7223, 73858: 7224, 73876: 7225, 73881: 7226, 73929: 7227, 74075: 7228, 74089: 7229, 74095: 7230, 74154: 7231, 74226: 7232, 74228: 7233, 74275: 7234, 74282: 7235, 74324: 7236, 74342: 7237, 74370: 7238, 74450: 7239, 74452: 7240, 74458: 7241, 74508: 7242, 74510: 7243, 74530: 7244, 74532: 7245, 74545: 7246, 74553: 7247, 74580: 7248, 74624: 7249, 74647: 7250, 74668: 7251, 74677: 7252, 74683: 7253, 74685: 7254, 74688: 7255, 74696: 7256, 74698: 7257, 74727: 7258, 74750: 7259, 74754: 7260, 74789: 7261, 74791: 7262, 74795: 7263, 74851: 7264, 74868: 7265, 74916: 7266, 74946: 7267, 74948: 7268, 75341: 7269, 75389: 7270, 75395: 7271, 75416: 7272, 75446: 7273, 75803: 7274, 75805: 7275, 75813: 7276, 75816: 7277, 75947: 7278, 75985: 7279, 76030: 7280, 76054: 7281, 76060: 7282, 76077: 7283, 76091: 7284, 76093: 7285, 76143: 7286, 76173: 7287, 76175: 7288, 76251: 7289, 76293: 7290, 76301: 7291, 76738: 7292, 76743: 7293, 76751: 7294, 76763: 7295, 77177: 7296, 77191: 7297, 77201: 7298, 77206: 7299, 77233: 7300, 77266: 7301, 77364: 7302, 77414: 7303, 77421: 7304, 77427: 7305, 77455: 7306, 77561: 7307, 77667: 7308, 77688: 7309, 77709: 7310, 77795: 7311, 77798: 7312, 77800: 7313, 77841: 7314, 77846: 7315, 77866: 7316, 77881: 7317, 77893: 7318, 77931: 7319, 78034: 7320, 78039: 7321, 78041: 7322, 78088: 7323, 78103: 7324, 78105: 7325, 78116: 7326, 78142: 7327, 78160: 7328, 78174: 7329, 78209: 7330, 78218: 7331, 78264: 7332, 78266: 7333, 78316: 7334, 78349: 7335, 78467: 7336, 78469: 7337, 78499: 7338, 78544: 7339, 78574: 7340, 78620: 7341, 78626: 7342, 78637: 7343, 78703: 7344, 78746: 7345, 78772: 7346, 78836: 7347, 78893: 7348, 78959: 7349, 79006: 7350, 79008: 7351, 79057: 7352, 79073: 7353, 79091: 7354, 79132: 7355, 79134: 7356, 79139: 7357, 79185: 7358, 79224: 7359, 79242: 7360, 79251: 7361, 79259: 7362, 79274: 7363, 79293: 7364, 79299: 7365, 79333: 7366, 79357: 7367, 79428: 7368, 79501: 7369, 79536: 7370, 79553: 7371, 79572: 7372, 79590: 7373, 79592: 7374, 79677: 7375, 79684: 7376, 79695: 7377, 79702: 7378, 79798: 7379, 79868: 7380, 79879: 7381, 79895: 7382, 79897: 7383, 79946: 7384, 80083: 7385, 80094: 7386, 80124: 7387, 80126: 7388, 80139: 7389, 80162: 7390, 80166: 7391, 80219: 7392, 80241: 7393, 80363: 7394, 80454: 7395, 80463: 7396, 80478: 7397, 80489: 7398, 80549: 7399, 80551: 7400, 80553: 7401, 80572: 7402, 80584: 7403, 80586: 7404, 80590: 7405, 80615: 7406, 80693: 7407, 80727: 7408, 80748: 7409, 80831: 7410, 80834: 7411, 80839: 7412, 80846: 7413, 80858: 7414, 80860: 7415, 80862: 7416, 80864: 7417, 80880: 7418, 80906: 7419, 80917: 7420, 80969: 7421, 81018: 7422, 81087: 7423, 81132: 7424, 81156: 7425, 81158: 7426, 81191: 7427, 81229: 7428, 81257: 7429, 81383: 7430, 81417: 7431, 81456: 7432, 81512: 7433, 81520: 7434, 81535: 7435, 81537: 7436, 81562: 7437, 81564: 7438, 81591: 7439, 81681: 7440, 81782: 7441, 81784: 7442, 81786: 7443, 81788: 7444, 81791: 7445, 81819: 7446, 81831: 7447, 81834: 7448, 81845: 7449, 81847: 7450, 81910: 7451, 81932: 7452, 81949: 7453, 82041: 7454, 82053: 7455, 82088: 7456, 82093: 7457, 82095: 7458, 82152: 7459, 82167: 7460, 82169: 7461, 82202: 7462, 82242: 7463, 82366: 7464, 82378: 7465, 82459: 7466, 82461: 7467, 82499: 7468, 82527: 7469, 82534: 7470, 82641: 7471, 82667: 7472, 82684: 7473, 82744: 7474, 82765: 7475, 82767: 7476, 82848: 7477, 82852: 7478, 82854: 7479, 82857: 7480, 83086: 7481, 83132: 7482, 83134: 7483, 83177: 7484, 83270: 7485, 83349: 7486, 83369: 7487, 83374: 7488, 83480: 7489, 83601: 7490, 83613: 7491, 83796: 7492, 83803: 7493, 83827: 7494, 83910: 7495, 83969: 7496, 83976: 7497, 84152: 7498, 84156: 7499, 84187: 7500, 84189: 7501, 84240: 7502, 84246: 7503, 84273: 7504, 84374: 7505, 84392: 7506, 84414: 7507, 84512: 7508, 84523: 7509, 84553: 7510, 84601: 7511, 84615: 7512, 84637: 7513, 84696: 7514, 84716: 7515, 84772: 7516, 84799: 7517, 84844: 7518, 84847: 7519, 84942: 7520, 84944: 7521, 84950: 7522, 84952: 7523, 84954: 7524, 85020: 7525, 85022: 7526, 85025: 7527, 85056: 7528, 85131: 7529, 85179: 7530, 85213: 7531, 85259: 7532, 85261: 7533, 85295: 7534, 85316: 7535, 85334: 7536, 85342: 7537, 85354: 7538, 85367: 7539, 85394: 7540, 85397: 7541, 85399: 7542, 85401: 7543, 85412: 7544, 85414: 7545, 85438: 7546, 85510: 7547, 85736: 7548, 85774: 7549, 85780: 7550, 85788: 7551, 85796: 7552, 85881: 7553, 85885: 7554, 86000: 7555, 86014: 7556, 86028: 7557, 86059: 7558, 86066: 7559, 86068: 7560, 86142: 7561, 86190: 7562, 86237: 7563, 86279: 7564, 86286: 7565, 86290: 7566, 86293: 7567, 86295: 7568, 86298: 7569, 86320: 7570, 86332: 7571, 86345: 7572, 86347: 7573, 86355: 7574, 86377: 7575, 86487: 7576, 86504: 7577, 86548: 7578, 86593: 7579, 86628: 7580, 86644: 7581, 86668: 7582, 86721: 7583, 86781: 7584, 86815: 7585, 86817: 7586, 86833: 7587, 86835: 7588, 86864: 7589, 86880: 7590, 86882: 7591, 86892: 7592, 86898: 7593, 86911: 7594, 86922: 7595, 86960: 7596, 87028: 7597, 87192: 7598, 87194: 7599, 87197: 7600, 87222: 7601, 87232: 7602, 87234: 7603, 87287: 7604, 87298: 7605, 87304: 7606, 87306: 7607, 87413: 7608, 87430: 7609, 87444: 7610, 87483: 7611, 87485: 7612, 87520: 7613, 87522: 7614, 87529: 7615, 87660: 7616, 87785: 7617, 87834: 7618, 87867: 7619, 87869: 7620, 87876: 7621, 87960: 7622, 88069: 7623, 88094: 7624, 88108: 7625, 88125: 7626, 88129: 7627, 88140: 7628, 88163: 7629, 88179: 7630, 88235: 7631, 88267: 7632, 88272: 7633, 88327: 7634, 88345: 7635, 88356: 7636, 88405: 7637, 88448: 7638, 88515: 7639, 88593: 7640, 88672: 7641, 88697: 7642, 88699: 7643, 88744: 7644, 88746: 7645, 88785: 7646, 88810: 7647, 88812: 7648, 88911: 7649, 88932: 7650, 88954: 7651, 89028: 7652, 89030: 7653, 89039: 7654, 89047: 7655, 89072: 7656, 89085: 7657, 89087: 7658, 89090: 7659, 89118: 7660, 89190: 7661, 89208: 7662, 89281: 7663, 89305: 7664, 89343: 7665, 89386: 7666, 89388: 7667, 89427: 7668, 89470: 7669, 89492: 7670, 89580: 7671, 89582: 7672, 89586: 7673, 89678: 7674, 89745: 7675, 89753: 7676, 89759: 7677, 89761: 7678, 89774: 7679, 89804: 7680, 89837: 7681, 89840: 7682, 89862: 7683, 89864: 7684, 89898: 7685, 89904: 7686, 89939: 7687, 89945: 7688, 90057: 7689, 90243: 7690, 90245: 7691, 90249: 7692, 90343: 7693, 90345: 7694, 90353: 7695, 90357: 7696, 90374: 7697, 90376: 7698, 90384: 7699, 90403: 7700, 90405: 7701, 90428: 7702, 90430: 7703, 90439: 7704, 90469: 7705, 90471: 7706, 90522: 7707, 90524: 7708, 90528: 7709, 90531: 7710, 90576: 7711, 90600: 7712, 90603: 7713, 90630: 7714, 90647: 7715, 90717: 7716, 90719: 7717, 90738: 7718, 90746: 7719, 90769: 7720, 90809: 7721, 90863: 7722, 90866: 7723, 90888: 7724, 90890: 7725, 90943: 7726, 90945: 7727, 91077: 7728, 91079: 7729, 91094: 7730, 91104: 7731, 91126: 7732, 91128: 7733, 91233: 7734, 91261: 7735, 91266: 7736, 91273: 7737, 91323: 7738, 91325: 7739, 91337: 7740, 91353: 7741, 91355: 7742, 91386: 7743, 91414: 7744, 91470: 7745, 91483: 7746, 91485: 7747, 91488: 7748, 91500: 7749, 91529: 7750, 91535: 7751, 91542: 7752, 91571: 7753, 91622: 7754, 91628: 7755, 91630: 7756, 91653: 7757, 91658: 7758, 91660: 7759, 91666: 7760, 91671: 7761, 91688: 7762, 91690: 7763, 91784: 7764, 91842: 7765, 91860: 7766, 91869: 7767, 91873: 7768, 91890: 7769, 91935: 7770, 91947: 7771, 91974: 7772, 91976: 7773, 91978: 7774, 91981: 7775, 92008: 7776, 92046: 7777, 92048: 7778, 92094: 7779, 92192: 7780, 92198: 7781, 92234: 7782, 92243: 7783, 92259: 7784, 92264: 7785, 92309: 7786, 92348: 7787, 92391: 7788, 92420: 7789, 92422: 7790, 92427: 7791, 92439: 7792, 92475: 7793, 92494: 7794, 92507: 7795, 92509: 7796, 92535: 7797, 92637: 7798, 92643: 7799, 92665: 7800, 92674: 7801, 92681: 7802, 92694: 7803, 92730: 7804, 92760: 7805, 92938: 7806, 92954: 7807, 93006: 7808, 93008: 7809, 93022: 7810, 93040: 7811, 93114: 7812, 93134: 7813, 93139: 7814, 93193: 7815, 93208: 7816, 93242: 7817, 93270: 7818, 93272: 7819, 93287: 7820, 93297: 7821, 93320: 7822, 93326: 7823, 93363: 7824, 93443: 7825, 93502: 7826, 93510: 7827, 93512: 7828, 93563: 7829, 93598: 7830, 93610: 7831, 93721: 7832, 93723: 7833, 93740: 7834, 93766: 7835, 93790: 7836, 93805: 7837, 93819: 7838, 93831: 7839, 93838: 7840, 93840: 7841, 93855: 7842, 93980: 7843, 93982: 7844, 93988: 7845, 94011: 7846, 94015: 7847, 94018: 7848, 94070: 7849, 94122: 7850, 94130: 7851, 94150: 7852, 94160: 7853, 94262: 7854, 94266: 7855, 94323: 7856, 94325: 7857, 94405: 7858, 94478: 7859, 94494: 7860, 94503: 7861, 94661: 7862, 94677: 7863, 94735: 7864, 94777: 7865, 94780: 7866, 94799: 7867, 94810: 7868, 94833: 7869, 94864: 7870, 94867: 7871, 94896: 7872, 94919: 7873, 94931: 7874, 94953: 7875, 94959: 7876, 94985: 7877, 95004: 7878, 95067: 7879, 95088: 7880, 95105: 7881, 95135: 7882, 95145: 7883, 95147: 7884, 95149: 7885, 95163: 7886, 95165: 7887, 95167: 7888, 95170: 7889, 95175: 7890, 95182: 7891, 95193: 7892, 95199: 7893, 95201: 7894, 95207: 7895, 95218: 7896, 95307: 7897, 95309: 7898, 95311: 7899, 95313: 7900, 95377: 7901, 95441: 7902, 95449: 7903, 95473: 7904, 95475: 7905, 95497: 7906, 95499: 7907, 95508: 7908, 95510: 7909, 95519: 7910, 95543: 7911, 95558: 7912, 95583: 7913, 95624: 7914, 95633: 7915, 95654: 7916, 95690: 7917, 95717: 7918, 95720: 7919, 95738: 7920, 95744: 7921, 95761: 7922, 95771: 7923, 95780: 7924, 95796: 7925, 95839: 7926, 95843: 7927, 95858: 7928, 95873: 7929, 95875: 7930, 95939: 7931, 95949: 7932, 95965: 7933, 96004: 7934, 96007: 7935, 96020: 7936, 96079: 7937, 96084: 7938, 96110: 7939, 96114: 7940, 96121: 7941, 96150: 7942, 96281: 7943, 96283: 7944, 96373: 7945, 96411: 7946, 96417: 7947, 96430: 7948, 96432: 7949, 96448: 7950, 96471: 7951, 96488: 7952, 96518: 7953, 96520: 7954, 96530: 7955, 96563: 7956, 96567: 7957, 96588: 7958, 96606: 7959, 96608: 7960, 96610: 7961, 96616: 7962, 96655: 7963, 96691: 7964, 96726: 7965, 96728: 7966, 96737: 7967, 96811: 7968, 96815: 7969, 96821: 7970, 96829: 7971, 96832: 7972, 96861: 7973, 96917: 7974, 96935: 7975, 96945: 7976, 96964: 7977, 96975: 7978, 97024: 7979, 97168: 7980, 97172: 7981, 97188: 7982, 97194: 7983, 97225: 7984, 97230: 7985, 97285: 7986, 97304: 7987, 97306: 7988, 97328: 7989, 97470: 7990, 97643: 7991, 97665: 7992, 97701: 7993, 97742: 7994, 97752: 7995, 97757: 7996, 97785: 7997, 97836: 7998, 97858: 7999, 97860: 8000, 97866: 8001, 97870: 8002, 97904: 8003, 97913: 8004, 97921: 8005, 97923: 8006, 97936: 8007, 97938: 8008, 97950: 8009, 97988: 8010, 98061: 8011, 98083: 8012, 98122: 8013, 98124: 8014, 98154: 8015, 98160: 8016, 98175: 8017, 98203: 8018, 98230: 8019, 98239: 8020, 98243: 8021, 98279: 8022, 98296: 8023, 98361: 8024, 98491: 8025, 98499: 8026, 98503: 8027, 98585: 8028, 98604: 8029, 98607: 8030, 98623: 8031, 98633: 8032, 98697: 8033, 98799: 8034, 98809: 8035, 98836: 8036, 98908: 8037, 98961: 8038, 99005: 8039, 99007: 8040, 99030: 8041, 99087: 8042, 99106: 8043, 99112: 8044, 99114: 8045, 99117: 8046, 99122: 8047, 99130: 8048, 99145: 8049, 99149: 8050, 99191: 8051, 99415: 8052, 99437: 8053, 99532: 8054, 99574: 8055, 99636: 8056, 99638: 8057, 99721: 8058, 99728: 8059, 99750: 8060, 99764: 8061, 99813: 8062, 99846: 8063, 99853: 8064, 99910: 8065, 99917: 8066, 99992: 8067, 100044: 8068, 100068: 8069, 100083: 8070, 100106: 8071, 100159: 8072, 100163: 8073, 100194: 8074, 100226: 8075, 100277: 8076, 100302: 8077, 100304: 8078, 100306: 8079, 100326: 8080, 100383: 8081, 100390: 8082, 100397: 8083, 100487: 8084, 100498: 8085, 100507: 8086, 100527: 8087, 100553: 8088, 100556: 8089, 100579: 8090, 100611: 8091, 100714: 8092, 100737: 8093, 100810: 8094, 100843: 8095, 100882: 8096, 100906: 8097, 101025: 8098, 101070: 8099, 101072: 8100, 101074: 8101, 101076: 8102, 101088: 8103, 101112: 8104, 101142: 8105, 101283: 8106, 101360: 8107, 101362: 8108, 101415: 8109, 101423: 8110, 101525: 8111, 101529: 8112, 101531: 8113, 101577: 8114, 101612: 8115, 101739: 8116, 101741: 8117, 101765: 8118, 101864: 8119, 101884: 8120, 101895: 8121, 101962: 8122, 101973: 8123, 102007: 8124, 102025: 8125, 102033: 8126, 102058: 8127, 102066: 8128, 102070: 8129, 102084: 8130, 102088: 8131, 102123: 8132, 102125: 8133, 102165: 8134, 102194: 8135, 102217: 8136, 102278: 8137, 102338: 8138, 102378: 8139, 102407: 8140, 102445: 8141, 102481: 8142, 102590: 8143, 102602: 8144, 102666: 8145, 102684: 8146, 102686: 8147, 102716: 8148, 102720: 8149, 102735: 8150, 102742: 8151, 102747: 8152, 102749: 8153, 102760: 8154, 102800: 8155, 102802: 8156, 102819: 8157, 102823: 8158, 102852: 8159, 102880: 8160, 102903: 8161, 102984: 8162, 102993: 8163, 103027: 8164, 103042: 8165, 103048: 8166, 103075: 8167, 103085: 8168, 103107: 8169, 103137: 8170, 103141: 8171, 103171: 8172, 103210: 8173, 103219: 8174, 103221: 8175, 103228: 8176, 103233: 8177, 103235: 8178, 103245: 8179, 103249: 8180, 103253: 8181, 103335: 8182, 103339: 8183, 103341: 8184, 103366: 8185, 103372: 8186, 103384: 8187, 103449: 8188, 103483: 8189, 103502: 8190, 103539: 8191, 103543: 8192, 103596: 8193, 103602: 8194, 103606: 8195, 103609: 8196, 103624: 8197, 103655: 8198, 103685: 8199, 103688: 8200, 103755: 8201, 103772: 8202, 103801: 8203, 103810: 8204, 103819: 8205, 103865: 8206, 103883: 8207, 103980: 8208, 103984: 8209, 104017: 8210, 104069: 8211, 104074: 8212, 104076: 8213, 104078: 8214, 104129: 8215, 104141: 8216, 104211: 8217, 104218: 8218, 104241: 8219, 104243: 8220, 104245: 8221, 104272: 8222, 104283: 8223, 104303: 8224, 104337: 8225, 104339: 8226, 104374: 8227, 104419: 8228, 104457: 8229, 104644: 8230, 104760: 8231, 104780: 8232, 104837: 8233, 104841: 8234, 104863: 8235, 104875: 8236, 104879: 8237, 104906: 8238, 104908: 8239, 104913: 8240, 104925: 8241, 104944: 8242, 105020: 8243, 105037: 8244, 105121: 8245, 105197: 8246, 105211: 8247, 105213: 8248, 105246: 8249, 105250: 8250, 105254: 8251, 105325: 8252, 105351: 8253, 105355: 8254, 105468: 8255, 105504: 8256, 105540: 8257, 105585: 8258, 105593: 8259, 105653: 8260, 105720: 8261, 105731: 8262, 105746: 8263, 105755: 8264, 105801: 8265, 105835: 8266, 105844: 8267, 105954: 8268, 106002: 8269, 106062: 8270, 106072: 8271, 106100: 8272, 106144: 8273, 106330: 8274, 106438: 8275, 106441: 8276, 106487: 8277, 106489: 8278, 106491: 8279, 106540: 8280, 106542: 8281, 106594: 8282, 106642: 8283, 106648: 8284, 106696: 8285, 106766: 8286, 106782: 8287, 106785: 8288, 106839: 8289, 106873: 8290, 106883: 8291, 106889: 8292, 106916: 8293, 106918: 8294, 106920: 8295, 106927: 8296, 107013: 8297, 107069: 8298, 107141: 8299, 107159: 8300, 107314: 8301, 107338: 8302, 107348: 8303, 107406: 8304, 107408: 8305, 107410: 8306, 107412: 8307, 107436: 8308, 107447: 8309, 107449: 8310, 107462: 8311, 107565: 8312, 107630: 8313, 107702: 8314, 107723: 8315, 107771: 8316, 107780: 8317, 107846: 8318, 107945: 8319, 107951: 8320, 107953: 8321, 107962: 8322, 107997: 8323, 107999: 8324, 108078: 8325, 108090: 8326, 108156: 8327, 108188: 8328, 108190: 8329, 108192: 8330, 108540: 8331, 108601: 8332, 108689: 8333, 108715: 8334, 108727: 8335, 108729: 8336, 108795: 8337, 108928: 8338, 108932: 8339, 108945: 8340, 108949: 8341, 108981: 8342, 109042: 8343, 109161: 8344, 109183: 8345, 109187: 8346, 109191: 8347, 109241: 8348, 109282: 8349, 109295: 8350, 109313: 8351, 109317: 8352, 109372: 8353, 109374: 8354, 109383: 8355, 109416: 8356, 109483: 8357, 109487: 8358, 109569: 8359, 109576: 8360, 109578: 8361, 109596: 8362, 109633: 8363, 109673: 8364, 109687: 8365, 109723: 8366, 109846: 8367, 109848: 8368, 109850: 8369, 109853: 8370, 109864: 8371, 109895: 8372, 109897: 8373, 109941: 8374, 109968: 8375, 109971: 8376, 110102: 8377, 110127: 8378, 110130: 8379, 110281: 8380, 110286: 8381, 110297: 8382, 110330: 8383, 110350: 8384, 110387: 8385, 110501: 8386, 110541: 8387, 110553: 8388, 110586: 8389, 110591: 8390, 110603: 8391, 110611: 8392, 110655: 8393, 110669: 8394, 110718: 8395, 110730: 8396, 110746: 8397, 110771: 8398, 110773: 8399, 110781: 8400, 110826: 8401, 110882: 8402, 111113: 8403, 111146: 8404, 111320: 8405, 111360: 8406, 111362: 8407, 111364: 8408, 111375: 8409, 111384: 8410, 111443: 8411, 111551: 8412, 111617: 8413, 111622: 8414, 111659: 8415, 111663: 8416, 111680: 8417, 111732: 8418, 111743: 8419, 111759: 8420, 111781: 8421, 111785: 8422, 111795: 8423, 111800: 8424, 111817: 8425, 111844: 8426, 111913: 8427, 111921: 8428, 112006: 8429, 112070: 8430, 112138: 8431, 112171: 8432, 112175: 8433, 112183: 8434, 112290: 8435, 112303: 8436, 112316: 8437, 112326: 8438, 112334: 8439, 112370: 8440, 112421: 8441, 112450: 8442, 112454: 8443, 112460: 8444, 112497: 8445, 112512: 8446, 112515: 8447, 112552: 8448, 112556: 8449, 112580: 8450, 112623: 8451, 112727: 8452, 112749: 8453, 112788: 8454, 112804: 8455, 112818: 8456, 112852: 8457, 112868: 8458, 112897: 8459, 112911: 8460, 112940: 8461, 113159: 8462, 113186: 8463, 113207: 8464, 113225: 8465, 113252: 8466, 113275: 8467, 113278: 8468, 113280: 8469, 113313: 8470, 113345: 8471, 113348: 8472, 113350: 8473, 113374: 8474, 113378: 8475, 113394: 8476, 113416: 8477, 113453: 8478, 113532: 8479, 113565: 8480, 113573: 8481, 113604: 8482, 113705: 8483, 113741: 8484, 113780: 8485, 113829: 8486, 113849: 8487, 113862: 8488, 114028: 8489, 114044: 8490, 114060: 8491, 114066: 8492, 114074: 8493, 114126: 8494, 114180: 8495, 114184: 8496, 114246: 8497, 114265: 8498, 114335: 8499, 114396: 8500, 114494: 8501, 114554: 8502, 114601: 8503, 114627: 8504, 114662: 8505, 114670: 8506, 114678: 8507, 114707: 8508, 114713: 8509, 114762: 8510, 114795: 8511, 114818: 8512, 114847: 8513, 114925: 8514, 114935: 8515, 115065: 8516, 115111: 8517, 115122: 8518, 115149: 8519, 115151: 8520, 115170: 8521, 115203: 8522, 115210: 8523, 115216: 8524, 115231: 8525, 115502: 8526, 115569: 8527, 115617: 8528, 115664: 8529, 115667: 8530, 115680: 8531, 115713: 8532, 115727: 8533, 115819: 8534, 115828: 8535, 115877: 8536, 115969: 8537, 116044: 8538, 116138: 8539, 116169: 8540, 116207: 8541, 116411: 8542, 116413: 8543, 116419: 8544, 116505: 8545, 116529: 8546, 116668: 8547, 116718: 8548, 116724: 8549, 116738: 8550, 116797: 8551, 116799: 8552, 116817: 8553, 116823: 8554, 116849: 8555, 116887: 8556, 116897: 8557, 116941: 8558, 116963: 8559, 116977: 8560, 116985: 8561, 117107: 8562, 117109: 8563, 117133: 8564, 117176: 8565, 117192: 8566, 117364: 8567, 117368: 8568, 117444: 8569, 117466: 8570, 117511: 8571, 117529: 8572, 117531: 8573, 117533: 8574, 117545: 8575, 117572: 8576, 117590: 8577, 117630: 8578, 117646: 8579, 117849: 8580, 117851: 8581, 117867: 8582, 117877: 8583, 117881: 8584, 117887: 8585, 117895: 8586, 117922: 8587, 118082: 8588, 118166: 8589, 118198: 8590, 118248: 8591, 118270: 8592, 118290: 8593, 118326: 8594, 118354: 8595, 118512: 8596, 118530: 8597, 118572: 8598, 118696: 8599, 118700: 8600, 118702: 8601, 118706: 8602, 118784: 8603, 118814: 8604, 118834: 8605, 118862: 8606, 118880: 8607, 118888: 8608, 118894: 8609, 118896: 8610, 118900: 8611, 118924: 8612, 118930: 8613, 118985: 8614, 118997: 8615, 119068: 8616, 119141: 8617, 119145: 8618, 119153: 8619, 119155: 8620, 119167: 8621, 119218: 8622, 119655: 8623, 119714: 8624, 119828: 8625, 119964: 8626, 120130: 8627, 120138: 8628, 120466: 8629, 120478: 8630, 120625: 8631, 120635: 8632, 120637: 8633, 120761: 8634, 120783: 8635, 120799: 8636, 120807: 8637, 120813: 8638, 120827: 8639, 120919: 8640, 121007: 8641, 121035: 8642, 121097: 8643, 121099: 8644, 121129: 8645, 121169: 8646, 121171: 8647, 121231: 8648, 121253: 8649, 121338: 8650, 121342: 8651, 121372: 8652, 121374: 8653, 121469: 8654, 121715: 8655, 121781: 8656, 122092: 8657, 122246: 8658, 122260: 8659, 122433: 8660, 122490: 8661, 122627: 8662, 122882: 8663, 122884: 8664, 122886: 8665, 122888: 8666, 122890: 8667, 122892: 8668, 122896: 8669, 122898: 8670, 122900: 8671, 122902: 8672, 122904: 8673, 122906: 8674, 122912: 8675, 122916: 8676, 122918: 8677, 122920: 8678, 122922: 8679, 122924: 8680, 122926: 8681, 122932: 8682, 123200: 8683, 123310: 8684, 123545: 8685, 123553: 8686, 123947: 8687, 124273: 8688, 124404: 8689, 124484: 8690, 124851: 8691, 124853: 8692, 124859: 8693, 125221: 8694, 125914: 8695, 125916: 8696, 125970: 8697, 125974: 8698, 126088: 8699, 126090: 8700, 126142: 8701, 126420: 8702, 126426: 8703, 126430: 8704, 126482: 8705, 126548: 8706, 126577: 8707, 126921: 8708, 127052: 8709, 127096: 8710, 127098: 8711, 127108: 8712, 127114: 8713, 127116: 8714, 127130: 8715, 127132: 8716, 127134: 8717, 127136: 8718, 127146: 8719, 127152: 8720, 127164: 8721, 127172: 8722, 127180: 8723, 127184: 8724, 127194: 8725, 127198: 8726, 127202: 8727, 127204: 8728, 127212: 8729, 127298: 8730, 127319: 8731, 127323: 8732, 127390: 8733, 128087: 8734, 128089: 8735, 128097: 8736, 128099: 8737, 128197: 8738, 128360: 8739, 128366: 8740, 128488: 8741, 128512: 8742, 128520: 8743, 128542: 8744, 128592: 8745, 128594: 8746, 128620: 8747, 128695: 8748, 128736: 8749, 128832: 8750, 128838: 8751, 128842: 8752, 128852: 8753, 128900: 8754, 128902: 8755, 128908: 8756, 128914: 8757, 128944: 8758, 128968: 8759, 128975: 8760, 128991: 8761, 129011: 8762, 129229: 8763, 129250: 8764, 129313: 8765, 129333: 8766, 129354: 8767, 129397: 8768, 129428: 8769, 129514: 8770, 129657: 8771, 129659: 8772, 129737: 8773, 129779: 8774, 129937: 8775, 130050: 8776, 130052: 8777, 130073: 8778, 130083: 8779, 130087: 8780, 130444: 8781, 130450: 8782, 130452: 8783, 130482: 8784, 130490: 8785, 130498: 8786, 130518: 8787, 130520: 8788, 130576: 8789, 130578: 8790, 130634: 8791, 130686: 8792, 130840: 8793, 130842: 8794, 130970: 8795, 130976: 8796, 130978: 8797, 131013: 8798, 131023: 8799, 131098: 8800, 131104: 8801, 131130: 8802, 131237: 8803, 131439: 8804, 131480: 8805, 131578: 8806, 131610: 8807, 131656: 8808, 131714: 8809, 131724: 8810, 131739: 8811, 131749: 8812, 131796: 8813, 131826: 8814, 131920: 8815, 131934: 8816, 132046: 8817, 132084: 8818, 132153: 8819, 132157: 8820, 132333: 8821, 132335: 8822, 132362: 8823, 132422: 8824, 132424: 8825, 132454: 8826, 132462: 8827, 132488: 8828, 132496: 8829, 132584: 8830, 132618: 8831, 132660: 8832, 132796: 8833, 132800: 8834, 132888: 8835, 133115: 8836, 133195: 8837, 133217: 8838, 133281: 8839, 133365: 8840, 133377: 8841, 133419: 8842, 133545: 8843, 133645: 8844, 133712: 8845, 133716: 8846, 133771: 8847, 133780: 8848, 133782: 8849, 133798: 8850, 133802: 8851, 133832: 8852, 133867: 8853, 133879: 8854, 134004: 8855, 134019: 8856, 134021: 8857, 134041: 8858, 134095: 8859, 134109: 8860, 134130: 8861, 134158: 8862, 134170: 8863, 134184: 8864, 134214: 8865, 134246: 8866, 134248: 8867, 134252: 8868, 134326: 8869, 134334: 8870, 134368: 8871, 134393: 8872, 134515: 8873, 134524: 8874, 134528: 8875, 134775: 8876, 134783: 8877, 134796: 8878, 134808: 8879, 134847: 8880, 134849: 8881, 134853: 8882, 134859: 8883, 134861: 8884, 134881: 8885, 135133: 8886, 135137: 8887, 135143: 8888, 135198: 8889, 135216: 8890, 135288: 8891, 135436: 8892, 135456: 8893, 135518: 8894, 135532: 8895, 135534: 8896, 135536: 8897, 135567: 8898, 135569: 8899, 135777: 8900, 135787: 8901, 135803: 8902, 135815: 8903, 135861: 8904, 135885: 8905, 135887: 8906, 135937: 8907, 136012: 8908, 136016: 8909, 136018: 8910, 136020: 8911, 136024: 8912, 136297: 8913, 136305: 8914, 136341: 8915, 136353: 8916, 136355: 8917, 136359: 8918, 136443: 8919, 136445: 8920, 136447: 8921, 136449: 8922, 136469: 8923, 136471: 8924, 136503: 8925, 136511: 8926, 136540: 8927, 136556: 8928, 136562: 8929, 136564: 8930, 136598: 8931, 136602: 8932, 136654: 8933, 136664: 8934, 136666: 8935, 136778: 8936, 136786: 8937, 136800: 8938, 136816: 8939, 136834: 8940, 136838: 8941, 136840: 8942, 136850: 8943, 136859: 8944, 136864: 8945, 136912: 8946, 136958: 8947, 137218: 8948, 137337: 8949, 137345: 8950, 137517: 8951, 137595: 8952, 137857: 8953, 137859: 8954, 137863: 8955, 138036: 8956, 138186: 8957, 138204: 8958, 138208: 8959, 138210: 8960, 138396: 8961, 138546: 8962, 138610: 8963, 138632: 8964, 138702: 8965, 138798: 8966, 138835: 8967, 138966: 8968, 139052: 8969, 139130: 8970, 139157: 8971, 139385: 8972, 139415: 8973, 139511: 8974, 139640: 8975, 139642: 8976, 139644: 8977, 139655: 8978, 139717: 8979, 139747: 8980, 139855: 8981, 139857: 8982, 139859: 8983, 139915: 8984, 139994: 8985, 140016: 8986, 140038: 8987, 140110: 8988, 140133: 8989, 140162: 8990, 140174: 8991, 140237: 8992, 140247: 8993, 140265: 8994, 140267: 8995, 140289: 8996, 140301: 8997, 140359: 8998, 140481: 8999, 140523: 9000, 140525: 9001, 140541: 9002, 140561: 9003, 140627: 9004, 140711: 9005, 140715: 9006, 140725: 9007, 140737: 9008, 140816: 9009, 140850: 9010, 140852: 9011, 140928: 9012, 140956: 9013, 141004: 9014, 141131: 9015, 141400: 9016, 141408: 9017, 141422: 9018, 141513: 9019, 141544: 9020, 141646: 9021, 141668: 9022, 141688: 9023, 141718: 9024, 141749: 9025, 141799: 9026, 141810: 9027, 141816: 9028, 141818: 9029, 141820: 9030, 141830: 9031, 141836: 9032, 141844: 9033, 141846: 9034, 141866: 9035, 141890: 9036, 141928: 9037, 141994: 9038, 142020: 9039, 142056: 9040, 142074: 9041, 142115: 9042, 142196: 9043, 142222: 9044, 142366: 9045, 142372: 9046, 142420: 9047, 142422: 9048, 142424: 9049, 142444: 9050, 142448: 9051, 142456: 9052, 142488: 9053, 142507: 9054, 142509: 9055, 142536: 9056, 142550: 9057, 142558: 9058, 142598: 9059, 142602: 9060, 142831: 9061, 142961: 9062, 142997: 9063, 143001: 9064, 143031: 9065, 143245: 9066, 143255: 9067, 143257: 9068, 143355: 9069, 143365: 9070, 143367: 9071, 143385: 9072, 143410: 9073, 143458: 9074, 143472: 9075, 143511: 9076, 143525: 9077, 143559: 9078, 143859: 9079, 143896: 9080, 143969: 9081, 144210: 9082, 144222: 9083, 144262: 9084, 144352: 9085, 144478: 9086, 144522: 9087, 144606: 9088, 144620: 9089, 144714: 9090, 144716: 9091, 144734: 9092, 144976: 9093, 145080: 9094, 145150: 9095, 145283: 9096, 145418: 9097, 145491: 9098, 145724: 9099, 145745: 9100, 145839: 9101, 145935: 9102, 145951: 9103, 145994: 9104, 146024: 9105, 146028: 9106, 146210: 9107, 146244: 9108, 146309: 9109, 146656: 9110, 146662: 9111, 146682: 9112, 146684: 9113, 146688: 9114, 146730: 9115, 146986: 9116, 147002: 9117, 147142: 9118, 147196: 9119, 147250: 9120, 147282: 9121, 147286: 9122, 147300: 9123, 147326: 9124, 147328: 9125, 147330: 9126, 147372: 9127, 147374: 9128, 147376: 9129, 147378: 9130, 147380: 9131, 147382: 9132, 147384: 9133, 147410: 9134, 147657: 9135, 147662: 9136, 147936: 9137, 148166: 9138, 148172: 9139, 148238: 9140, 148424: 9141, 148482: 9142, 148592: 9143, 148626: 9144, 148632: 9145, 148652: 9146, 148667: 9147, 148671: 9148, 148675: 9149, 148709: 9150, 148775: 9151, 148881: 9152, 148888: 9153, 148956: 9154, 148978: 9155, 148982: 9156, 149011: 9157, 149144: 9158, 149146: 9159, 149330: 9160, 149334: 9161, 149350: 9162, 149352: 9163, 149354: 9164, 149380: 9165, 149406: 9166, 149508: 9167, 149566: 9168, 149590: 9169, 149612: 9170, 149830: 9171, 149902: 9172, 150254: 9173, 150401: 9174, 150548: 9175, 150554: 9176, 150596: 9177, 150604: 9178, 150696: 9179, 150993: 9180, 151311: 9181, 151315: 9182, 151317: 9183, 151455: 9184, 151479: 9185, 151501: 9186, 151557: 9187, 151559: 9188, 151653: 9189, 151687: 9190, 151695: 9191, 151739: 9192, 151745: 9193, 151759: 9194, 151763: 9195, 151769: 9196, 151777: 9197, 151781: 9198, 152037: 9199, 152063: 9200, 152065: 9201, 152071: 9202, 152077: 9203, 152079: 9204, 152081: 9205, 152083: 9206, 152085: 9207, 152091: 9208, 152105: 9209, 152173: 9210, 152270: 9211, 152284: 9212, 152372: 9213, 152591: 9214, 152658: 9215, 152711: 9216, 152970: 9217, 153070: 9218, 153236: 9219, 153386: 9220, 153408: 9221, 154065: 9222, 154358: 9223, 154975: 9224, 155064: 9225, 155168: 9226, 155288: 9227, 155358: 9228, 155509: 9229, 155589: 9230, 155659: 9231, 155743: 9232, 155774: 9233, 155812: 9234, 155820: 9235, 155892: 9236, 156025: 9237, 156371: 9238, 156387: 9239, 156553: 9240, 156605: 9241, 156607: 9242, 156609: 9243, 156675: 9244, 156706: 9245, 156726: 9246, 156781: 9247, 156783: 9248, 157108: 9249, 157110: 9250, 157122: 9251, 157130: 9252, 157172: 9253, 157200: 9254, 157270: 9255, 157296: 9256, 157312: 9257, 157340: 9258, 157369: 9259, 157407: 9260, 157432: 9261, 157699: 9262, 157775: 9263, 157865: 9264, 158022: 9265, 158027: 9266, 158035: 9267, 158238: 9268, 158254: 9269, 158388: 9270, 158398: 9271, 158402: 9272, 158528: 9273, 158721: 9274, 158783: 9275, 158813: 9276, 158830: 9277, 158842: 9278, 158872: 9279, 158874: 9280, 158882: 9281, 158956: 9282, 158966: 9283, 158972: 9284, 159061: 9285, 159069: 9286, 159077: 9287, 159093: 9288, 159161: 9289, 159193: 9290, 159403: 9291, 159415: 9292, 159441: 9293, 159510: 9294, 159690: 9295, 159717: 9296, 159755: 9297, 159779: 9298, 159811: 9299, 159817: 9300, 159849: 9301, 159858: 9302, 159976: 9303, 160080: 9304, 160271: 9305, 160289: 9306, 160341: 9307, 160400: 9308, 160422: 9309, 160438: 9310, 160440: 9311, 160527: 9312, 160563: 9313, 160565: 9314, 160567: 9315, 160569: 9316, 160571: 9317, 160573: 9318, 160644: 9319, 160646: 9320, 160684: 9321, 160718: 9322, 160730: 9323, 160836: 9324, 160848: 9325, 160872: 9326, 160954: 9327, 160978: 9328, 160980: 9329, 161008: 9330, 161024: 9331, 161032: 9332, 161044: 9333, 161127: 9334, 161131: 9335, 161290: 9336, 161354: 9337, 161580: 9338, 161582: 9339, 161594: 9340, 161634: 9341, 161830: 9342, 161918: 9343, 161922: 9344, 161966: 9345, 162082: 9346, 162344: 9347, 162350: 9348, 162414: 9349, 162478: 9350, 162578: 9351, 162590: 9352, 162598: 9353, 162600: 9354, 162602: 9355, 162606: 9356, 162828: 9357, 162968: 9358, 162982: 9359, 163056: 9360, 163072: 9361, 163112: 9362, 163134: 9363, 163386: 9364, 163527: 9365, 163639: 9366, 163645: 9367, 163653: 9368, 163809: 9369, 163925: 9370, 163937: 9371, 163981: 9372, 163985: 9373, 164179: 9374, 164200: 9375, 164226: 9376, 164280: 9377, 164367: 9378, 164375: 9379, 164540: 9380, 164647: 9381, 164655: 9382, 164707: 9383, 164753: 9384, 164881: 9385, 164909: 9386, 164917: 9387, 165075: 9388, 165101: 9389, 165103: 9390, 165139: 9391, 165343: 9392, 165347: 9393, 165483: 9394, 165489: 9395, 165529: 9396, 165549: 9397, 165551: 9398, 165635: 9399, 165639: 9400, 165645: 9401, 165671: 9402, 165843: 9403, 165947: 9404, 165959: 9405, 165969: 9406, 166015: 9407, 166024: 9408, 166183: 9409, 166203: 9410, 166291: 9411, 166461: 9412, 166492: 9413, 166526: 9414, 166528: 9415, 166534: 9416, 166558: 9417, 166568: 9418, 166635: 9419, 166643: 9420, 166705: 9421, 166946: 9422, 167018: 9423, 167036: 9424, 167064: 9425, 167296: 9426, 167370: 9427, 167380: 9428, 167538: 9429, 167570: 9430, 167634: 9431, 167706: 9432, 167732: 9433, 167746: 9434, 167772: 9435, 167790: 9436, 167854: 9437, 168026: 9438, 168090: 9439, 168144: 9440, 168174: 9441, 168218: 9442, 168248: 9443, 168250: 9444, 168252: 9445, 168254: 9446, 168266: 9447, 168326: 9448, 168350: 9449, 168358: 9450, 168366: 9451, 168418: 9452, 168456: 9453, 168492: 9454, 168608: 9455, 168612: 9456, 168632: 9457, 168712: 9458, 168846: 9459, 169034: 9460, 169180: 9461, 169670: 9462, 169904: 9463, 169912: 9464, 169958: 9465, 169982: 9466, 169984: 9467, 169992: 9468, 170289: 9469, 170297: 9470, 170355: 9471, 170357: 9472, 170399: 9473, 170401: 9474, 170411: 9475, 170551: 9476, 170597: 9477, 170697: 9478, 170705: 9479, 170777: 9480, 170813: 9481, 170817: 9482, 170827: 9483, 170837: 9484, 170875: 9485, 170897: 9486, 170907: 9487, 170937: 9488, 170939: 9489, 170945: 9490, 170957: 9491, 170993: 9492, 171011: 9493, 171023: 9494, 171251: 9495, 171495: 9496, 171631: 9497, 171695: 9498, 171701: 9499, 171749: 9500, 171751: 9501, 171759: 9502, 171763: 9503, 171765: 9504, 171811: 9505, 171867: 9506, 171891: 9507, 171917: 9508, 172013: 9509, 172215: 9510, 172229: 9511, 172233: 9512, 172253: 9513, 172321: 9514, 172461: 9515, 172497: 9516, 172547: 9517, 172577: 9518, 172583: 9519, 172585: 9520, 172587: 9521, 172589: 9522, 172591: 9523, 172637: 9524, 172705: 9525, 172793: 9526, 172825: 9527, 172875: 9528, 172881: 9529, 172887: 9530, 172909: 9531, 173145: 9532, 173197: 9533, 173205: 9534, 173209: 9535, 173235: 9536, 173253: 9537, 173255: 9538, 173291: 9539, 173307: 9540, 173317: 9541, 173351: 9542, 173355: 9543, 173535: 9544, 173619: 9545, 173751: 9546, 173873: 9547, 173925: 9548, 173941: 9549, 173963: 9550, 174045: 9551, 174053: 9552, 174055: 9553, 174141: 9554, 174403: 9555, 174479: 9556, 174551: 9557, 174681: 9558, 174727: 9559, 174737: 9560, 174815: 9561, 174909: 9562, 175197: 9563, 175199: 9564, 175293: 9565, 175303: 9566, 175387: 9567, 175397: 9568, 175401: 9569, 175431: 9570, 175435: 9571, 175475: 9572, 175485: 9573, 175569: 9574, 175577: 9575, 175585: 9576, 175661: 9577, 175693: 9578, 175705: 9579, 175707: 9580, 175743: 9581, 175781: 9582, 176051: 9583, 176101: 9584, 176329: 9585, 176371: 9586, 176389: 9587, 176413: 9588, 176415: 9589, 176419: 9590, 176423: 9591, 176579: 9592, 176601: 9593, 176621: 9594, 176751: 9595, 176805: 9596, 176935: 9597, 177185: 9598, 177285: 9599, 177593: 9600, 177615: 9601, 177763: 9602, 177765: 9603, 177939: 9604, 178061: 9605, 178111: 9606, 178129: 9607, 178323: 9608, 178613: 9609, 178615: 9610, 178827: 9611, 179053: 9612, 179073: 9613, 179119: 9614, 179133: 9615, 179135: 9616, 179211: 9617, 179401: 9618, 179427: 9619, 179491: 9620, 179511: 9621, 179709: 9622, 179749: 9623, 179813: 9624, 179815: 9625, 179817: 9626, 179819: 9627, 179953: 9628, 180031: 9629, 180045: 9630, 180095: 9631, 180231: 9632, 180263: 9633, 180265: 9634, 180297: 9635, 180497: 9636, 180777: 9637, 180985: 9638, 180987: 9639, 181065: 9640, 181139: 9641, 181315: 9642, 181413: 9643, 181659: 9644, 181719: 9645, 182293: 9646, 182297: 9647, 182299: 9648, 182639: 9649, 182715: 9650, 182727: 9651, 182731: 9652, 182749: 9653, 182793: 9654, 182823: 9655, 183011: 9656, 183197: 9657, 183199: 9658, 183227: 9659, 183295: 9660, 183301: 9661, 183317: 9662, 183611: 9663, 183635: 9664, 183897: 9665, 183911: 9666, 183959: 9667, 184015: 9668, 184053: 9669, 184245: 9670, 184253: 9671, 184257: 9672, 184349: 9673, 184471: 9674, 184641: 9675, 184721: 9676, 184791: 9677, 184931: 9678, 184987: 9679, 184997: 9680, 185029: 9681, 185031: 9682, 185033: 9683, 185135: 9684, 185435: 9685, 185473: 9686, 185585: 9687, 186587: 9688, 187031: 9689, 187541: 9690, 187593: 9691, 187595: 9692, 187717: 9693, 188189: 9694, 188301: 9695, 188675: 9696, 188751: 9697, 188797: 9698, 188833: 9699, 189043: 9700, 189111: 9701, 189333: 9702, 189381: 9703, 189547: 9704, 189713: 9705, 190183: 9706, 190207: 9707, 190209: 9708, 190213: 9709, 190215: 9710, 190219: 9711, 190221: 9712, 191005: 9713, 193565: 9714, 193567: 9715, 193571: 9716, 193573: 9717, 193579: 9718, 193581: 9719, 193583: 9720, 193585: 9721, 193587: 9722, 193609: 9723}\n",
      "There are 9724 movies, 610 users, and 100836 ratings\n"
     ]
    }
   ],
   "source": [
    "path_data = 'data/ml-latest-small/'\n",
    "ratings_s_path =  path_data+ 'ratings.csv'\n",
    "train, test, nb_users, nb_movies, user_ids_map, movie_ids_map = get_train_test_sets(ratings_s_path)\n",
    "dataset = pd.concat((train,test), axis = 0)\n",
    "\n",
    "print(\"There are %i movies, %i users, and %i ratings\" % (nb_movies, nb_users, dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [train[\"userId\"].to_numpy(), train[\"movieId\"].to_numpy()]\n",
    "y_train = train[\"rating\"].to_numpy()\n",
    "\n",
    "X_test = [test[\"userId\"].to_numpy(), test[\"movieId\"].to_numpy()]\n",
    "y_test = test[\"rating\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at the ratings distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['rating'].hist(bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset there are a lot of missing values, because not all the user/movie pairs have an associated rating. Indeed, each user rates only a few movies ! The goal of this notebook is to predict (some of) the missing user/movie ratings.\n",
    "\n",
    "Print how many movies each of the 5 first users have rated, and print the percentage of available ratings in the whole dataset (i.e. the ratio between number of ratings and all the possible users/movies combinations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rating\n",
       "userId        \n",
       "0          232\n",
       "1           29\n",
       "2           39\n",
       "3          216\n",
       "4           44"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio ratings = 1.700 %\n"
     ]
    }
   ],
   "source": [
    "#TOFILL\n",
    "display(dataset[dataset.userId < 5].groupby('userId').count()[[\"rating\"]])\n",
    "print(f\"ratio ratings = {(len(dataset)/(nb_users*nb_movies)*100):0.3f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only 1.7% of ratings that are available, which is normal as each hasn't rated all the movies. To see the dataset in a matrix form with all the missing ratings, use the `Dataframe.pivot()` function, with the `userId` as index, the `movieId` as columns, and the ratings for the `values` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>movieId</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9714</th>\n",
       "      <th>9715</th>\n",
       "      <th>9716</th>\n",
       "      <th>9717</th>\n",
       "      <th>9718</th>\n",
       "      <th>9719</th>\n",
       "      <th>9720</th>\n",
       "      <th>9721</th>\n",
       "      <th>9722</th>\n",
       "      <th>9723</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>610 rows  9724 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "movieId  0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "userId                                                               ...   \n",
       "0         4.0   NaN   4.0   NaN   NaN   4.0   NaN   NaN   NaN   NaN  ...   \n",
       "1         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "2         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "3         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "4         4.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "...       ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "605       2.5   NaN   NaN   NaN   NaN   NaN   2.5   NaN   NaN   NaN  ...   \n",
       "606       4.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "607       2.5   2.0   2.0   NaN   NaN   NaN   NaN   NaN   NaN   4.0  ...   \n",
       "608       3.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   4.0  ...   \n",
       "609       5.0   NaN   NaN   NaN   NaN   5.0   NaN   NaN   NaN   NaN  ...   \n",
       "\n",
       "movieId  9714  9715  9716  9717  9718  9719  9720  9721  9722  9723  \n",
       "userId                                                               \n",
       "0         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "1         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "2         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "3         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "4         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "...       ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "605       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "606       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "607       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "608       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "609       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[610 rows x 9724 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TOFILL\n",
    "df_pivot = dataset.pivot(index='userId',columns='movieId',values='rating')\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all the ratings of user 1. To do so, use the *movies.csv* file and your `movie_ids_map` dictionnary to find the movie title from the new movie indexes, and print the real movie title associated to each rating of user 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse mapping and import movie df\n",
    "user_ids_map_to_original = {v: k for k, v in user_ids_map.items()}\n",
    "movie_ids_map_to_original = {v: k for k, v in movie_ids_map.items()}\n",
    "df_movies = pd.read_csv(path_data + 'movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>movieName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>1</td>\n",
       "      <td>318</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Shawshank Redemption, The (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>1</td>\n",
       "      <td>333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Tommy Boy (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>1</td>\n",
       "      <td>1704</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Good Will Hunting (1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>1</td>\n",
       "      <td>3578</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Gladiator (2000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>1</td>\n",
       "      <td>6874</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Kill Bill: Vol. 1 (2003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>1</td>\n",
       "      <td>8798</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Collateral (2004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>1</td>\n",
       "      <td>46970</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Talladega Nights: The Ballad of Ricky Bobby (2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>1</td>\n",
       "      <td>48516</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Departed, The (2006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>1</td>\n",
       "      <td>58559</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Dark Knight, The (2008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>1</td>\n",
       "      <td>60756</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Step Brothers (2008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>1</td>\n",
       "      <td>68157</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Inglourious Basterds (2009)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>1</td>\n",
       "      <td>71535</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Zombieland (2009)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>1</td>\n",
       "      <td>74458</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Shutter Island (2010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>1</td>\n",
       "      <td>77455</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Exit Through the Gift Shop (2010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>1</td>\n",
       "      <td>79132</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Inception (2010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>1</td>\n",
       "      <td>80489</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Town, The (2010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>1</td>\n",
       "      <td>80906</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Inside Job (2010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>1</td>\n",
       "      <td>86345</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Louis C.K.: Hilarious (2010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1</td>\n",
       "      <td>89774</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Warrior (2011)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>1</td>\n",
       "      <td>91529</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Dark Knight Rises, The (2012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>1</td>\n",
       "      <td>91658</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Girl with the Dragon Tattoo, The (2011)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>1</td>\n",
       "      <td>99114</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Django Unchained (2012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>1</td>\n",
       "      <td>106782</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Wolf of Wall Street, The (2013)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1</td>\n",
       "      <td>109487</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Interstellar (2014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>1</td>\n",
       "      <td>112552</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Whiplash (2014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>1</td>\n",
       "      <td>114060</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The Drop (2014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>1</td>\n",
       "      <td>115713</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Ex Machina (2015)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>1</td>\n",
       "      <td>122882</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Mad Max: Fury Road (2015)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>1</td>\n",
       "      <td>131724</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The Jinx: The Life and Deaths of Robert Durst ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     userId  movieId  rating  \\\n",
       "232       1      318     3.0   \n",
       "233       1      333     4.0   \n",
       "234       1     1704     4.5   \n",
       "235       1     3578     4.0   \n",
       "236       1     6874     4.0   \n",
       "237       1     8798     3.5   \n",
       "238       1    46970     4.0   \n",
       "239       1    48516     4.0   \n",
       "240       1    58559     4.5   \n",
       "241       1    60756     5.0   \n",
       "242       1    68157     4.5   \n",
       "243       1    71535     3.0   \n",
       "244       1    74458     4.0   \n",
       "245       1    77455     3.0   \n",
       "246       1    79132     4.0   \n",
       "247       1    80489     4.5   \n",
       "248       1    80906     5.0   \n",
       "249       1    86345     4.0   \n",
       "250       1    89774     5.0   \n",
       "251       1    91529     3.5   \n",
       "252       1    91658     2.5   \n",
       "253       1    99114     3.5   \n",
       "254       1   106782     5.0   \n",
       "255       1   109487     3.0   \n",
       "256       1   112552     4.0   \n",
       "257       1   114060     2.0   \n",
       "258       1   115713     3.5   \n",
       "259       1   122882     5.0   \n",
       "260       1   131724     5.0   \n",
       "\n",
       "                                             movieName  \n",
       "232                   Shawshank Redemption, The (1994)  \n",
       "233                                   Tommy Boy (1995)  \n",
       "234                           Good Will Hunting (1997)  \n",
       "235                                   Gladiator (2000)  \n",
       "236                           Kill Bill: Vol. 1 (2003)  \n",
       "237                                  Collateral (2004)  \n",
       "238  Talladega Nights: The Ballad of Ricky Bobby (2...  \n",
       "239                               Departed, The (2006)  \n",
       "240                            Dark Knight, The (2008)  \n",
       "241                               Step Brothers (2008)  \n",
       "242                        Inglourious Basterds (2009)  \n",
       "243                                  Zombieland (2009)  \n",
       "244                              Shutter Island (2010)  \n",
       "245                  Exit Through the Gift Shop (2010)  \n",
       "246                                   Inception (2010)  \n",
       "247                                   Town, The (2010)  \n",
       "248                                  Inside Job (2010)  \n",
       "249                       Louis C.K.: Hilarious (2010)  \n",
       "250                                     Warrior (2011)  \n",
       "251                      Dark Knight Rises, The (2012)  \n",
       "252            Girl with the Dragon Tattoo, The (2011)  \n",
       "253                            Django Unchained (2012)  \n",
       "254                    Wolf of Wall Street, The (2013)  \n",
       "255                                Interstellar (2014)  \n",
       "256                                    Whiplash (2014)  \n",
       "257                                    The Drop (2014)  \n",
       "258                                  Ex Machina (2015)  \n",
       "259                          Mad Max: Fury Road (2015)  \n",
       "260  The Jinx: The Life and Deaths of Robert Durst ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = dataset[dataset.userId==1][['userId','movieId','rating']]\n",
    "df_test['movieId'] = df_test['movieId'].map(movie_ids_map_to_original)\n",
    "df_test['movieName'] = df_test['movieId'].map(lambda x : df_movies[df_movies.movieId==x]['title'].iloc[0])\n",
    "df_test.sort_values(by='movieId')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a python library made for easily designing complex models such as deep learning models, in this module we are going to use just a few features from it to implement our simple matrix factorization model, as it makes a good introduction to the library before the next module about deep learning where you will also be using Keras.\n",
    "\n",
    "The following function `get_mf_model` implements the model described in equation (2) in Koren's paper (without the $+\\lambda(\\ldots)$ part for the moment). So it basically tries to find the $p_u \\in \\mathbb{R}^k$ and $q_i \\in \\mathbb{R}^k$ vectors that minimizes the squared loss between their dot product $p_u^Tq_i$, and the observed ratings $r_{ui}$, from random initialization of $p_u$ and $q_i$. In machine learning terms, $p_u$ and $q_i$ are called the *embeddings* of the user $u$ and of the movie $i$ respectively. Their size $k$ is an hyper-parameter of the model, which is called the *rank* of the factorization.\n",
    "\n",
    "To do so, it uses the functional API from Keras (the other API proposed is the sequential one, but is not adapted for this model), you can read about it here : https://keras.io/guides/functional_api/ .\n",
    "\n",
    "Keras, unlike Numpy, uses a different progamming paradigm. Numpy uses an *imperative* programming style (like python in general), meaning that when you execute `x.dot(y)`, the dot product is actually calculated. Keras however, uses a *declarative* (also called *symbolic*) programming style, meaning that when you write `Dot()([x, y])`, you tell Keras than when you will call the *fit* function of your model in the future, you will want to do a dot product between the future values that *x* and *y* will have. And this is what Keras is about, it allows you to build your own model as a sequence of operations, describing each input and output, and then later fit it and predict with it.\n",
    "\n",
    "Let's not get in too many details, but retain that the `get_mf_model` function below is not actually executing the model, it creates it, and returns an object of the class `keras.models.Model` that has been instructed with your model operations, and this object can then be trained with the classic `fit` and `predict` functions. \n",
    "\n",
    "Read carefully the comments in the code of the function to understand the different steps in the model creation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Input, Reshape\n",
    "from keras.layers import Dot\n",
    "\n",
    "def get_mf_model(nb_users, nb_movies, k):\n",
    "    \"\"\"\n",
    "    Build a simple matrix factorization model from\n",
    "    the number of user, the number of movies, and the size of the embeddings k.\n",
    "    \n",
    "    Input:\n",
    "        nb_users : int : The number of unique users\n",
    "        nb_movies : int : The number of unique movies\n",
    "        k : int : The size of the embeddings\n",
    "        \n",
    "    Output:\n",
    "        model : keras.models.Model : A keras model that implements matrix factorization\n",
    "        \n",
    "    \"\"\"\n",
    "    dim_embedddings = k\n",
    "    \n",
    "    #Inputs:\n",
    "    #First we describe the input of the model, that is the training data that we will give it as X\n",
    "    #In our case, the input are just the user index u and the movie index i.\n",
    "    #So we declare two inputs of size one:\n",
    "    u = Input(shape=(1,), dtype='int32', name = \"u__user_id\")\n",
    "    i = Input(shape=(1,), dtype='int32', name = 'i__movie_id')\n",
    "    \n",
    "    #Then let's declare our variable, the embeddings p and q.\n",
    "    #First with the users, we declare that we have nb_users embeddings, each of size dim_embeddings.\n",
    "    #An embedding object is indexed by calling it with the index parameter like a function,\n",
    "    #so we add a `(u)` at the end to tell keras we want it to be indexed \n",
    "    #by the user ids we will pass at training time as inputs.\n",
    "    p_u = Embedding(nb_users, dim_embedddings, name=\"p_u__user_embedding\")(u)\n",
    "    \n",
    "    #Unfortunatly, when indexing an embeddings it keeps [1,k] matrix shape instead\n",
    "    #of just a [k] vector, so we have to tell Keras that we just want a vector by\n",
    "    #redefining its shape:\n",
    "    p_u = Reshape((dim_embedddings,), name=\"p_u__user_embedding_reshaped\")(p_u)\n",
    "    \n",
    "    # Same thing for the movie embeddings:\n",
    "    q_i = Embedding(nb_movies, dim_embedddings, name=\"q_i__movie_embedding\")(i)\n",
    "    q_i = Reshape((dim_embedddings,), name=\"q_i__movie_embedding_reshaped\")(q_i)\n",
    "    \n",
    "    #Then the dot product between the two indexed embeddings, \n",
    "    #we'll understand the axes = 1 part later.\n",
    "    r_hat = Dot(axes = 1)([q_i, p_u])\n",
    "\n",
    "    #We define our model by giving its input and outputs, in our case\n",
    "    #the user and movie ids will be the inputs, and the output will be\n",
    "    #the estimated rating r_hat, that is the dot product of the \n",
    "    #corresponding embeddings.\n",
    "    model = Model(inputs=[u, i], outputs=r_hat)\n",
    "    \n",
    "    #Finally, we define the loss and metric to use, in our case the mean squared error,\n",
    "    #along with the optimization method, we'll understand what is 'adam' later also.\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[\"mse\"])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30\n",
    "mf_model = get_mf_model(nb_users, nb_movies, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras allows us to have a textual overview of the model we defined with the *summary()* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
       "\n",
       " i__movie_id          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " u__user_id           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " q_i__movie_embeddi  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">291,720</span>  i__movie_id[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
       "\n",
       " p_u__user_embedding  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,300</span>  u__user_id[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
       "\n",
       " q_i__movie_embeddi  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  q_i__movie_embed \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                                                             \n",
       "\n",
       " p_u__user_embeddin  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  p_u__user_embedd \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                                                             \n",
       "\n",
       " dot (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  q_i__movie_embed \n",
       "                                                     p_u__user_embedd \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " i__movie_id          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " u__user_id           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " q_i__movie_embeddi  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m30\u001b[0m)         \u001b[38;5;34m291,720\u001b[0m  i__movie_id[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
       "\n",
       " p_u__user_embedding  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m30\u001b[0m)          \u001b[38;5;34m18,300\u001b[0m  u__user_id[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
       "\n",
       " q_i__movie_embeddi  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  q_i__movie_embed \n",
       " (\u001b[38;5;33mReshape\u001b[0m)                                                             \n",
       "\n",
       " p_u__user_embeddin  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  p_u__user_embedd \n",
       " (\u001b[38;5;33mReshape\u001b[0m)                                                             \n",
       "\n",
       " dot (\u001b[38;5;33mDot\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  q_i__movie_embed \n",
       "                                                     p_u__user_embedd \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">310,020</span> (1.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m310,020\u001b[0m (1.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">310,020</span> (1.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m310,020\u001b[0m (1.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mf_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the keras objects we defined in our model is called a *layer*, and we find them in order in the first column. The *Param #* column gives the number of trainable parameters of the layer, in our case these are just the embeddings, and they should be equal to $nb\\_users \\times k$ and $nb\\_movies \\times k$. The *Connected to* column tells for each layer which layers are inputs for this layer (you can safely ignore the `[0][0]` for this module).\n",
    "\n",
    "Finally the *Output Shape* column gives us the shape of the layer, each layer being a *tensor*. A tensor is the generalization of matrices to more than two dimensions. So a matrix is a 2D-tensor and a vector is a 1D-tensor, and each layer can be a matrix, a vector, or a higher-order tensor. The output shape we see is indeed the expected one at each layer, except there is this `None` in first dimension, why is that ?\n",
    "\n",
    "To understand it, we have to get into how Keras is actually minimizing the mean squared loss of our model. In general, when in comes to minimizing error functions on big datasets, a generic method is to use Stocastic Gradient Descent (SGD), briefly described in page 4 of Koren's article. \n",
    "\n",
    "Read about gradient descent, SGD and its variant mini-batch SGD in Chapter 4 of *Hands on ML ...* (pages 111-120):\n",
    "https://drive.google.com/file/d/1t0rc3x5YQBgLXVLET6BzR4jn5vzMI_m0/view?usp=sharing\n",
    "\n",
    "This is what Keras does when it fits the model, it initializes the $q_i$ and $p_u$ embedding vectors randomly, and then perform mini-batch SGD to find the minimum mean squared error on the training set. Since mini-batching means considering multiple training samples at the same time, Keras keeps the first dimension of each layer to stack the samples of each batch, this is why `None` is written, the actual batch_size being set at training time when calling the `fit` function. This is also why we had to set `axes=1` when calling the `Dot` layer in the `get_mf_model` function, because the first dimension (axe 0) of each layer is kept for the batches. And about the `optimizer='adam'`, it is just a variation of mini-batch SGD that is faster, we'll get into more details about SGD variations in the optional parts of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally train our matrix factorization model on our movieLens data. The `epochs` parameter controls the number of iterations of the SGD algorithm, that is the number of times it is going to pass on each training rating and update the embeddings accordingly. Let's keep it at 20 for the moment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/ml4/lib/python3.12/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['u__user_id', 'i__movie_id']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 13.3496 - mse: 13.3496\n",
      "Epoch 2/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 11.9929 - mse: 11.9929\n",
      "Epoch 3/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 5.0816 - mse: 5.0816\n",
      "Epoch 4/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.5307 - mse: 2.5307\n",
      "Epoch 5/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.7290 - mse: 1.7290\n",
      "Epoch 6/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.3340 - mse: 1.3340\n",
      "Epoch 7/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1144 - mse: 1.1144\n",
      "Epoch 8/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9720 - mse: 0.9720\n",
      "Epoch 9/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8704 - mse: 0.8704\n",
      "Epoch 10/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7928 - mse: 0.7928\n",
      "Epoch 11/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7564 - mse: 0.7564\n",
      "Epoch 12/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7148 - mse: 0.7148\n",
      "Epoch 13/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6783 - mse: 0.6783\n",
      "Epoch 14/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6516 - mse: 0.6516\n",
      "Epoch 15/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6288 - mse: 0.6288\n",
      "Epoch 16/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6057 - mse: 0.6057\n",
      "Epoch 17/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5964 - mse: 0.5964\n",
      "Epoch 18/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.5851 - mse: 0.5851\n",
      "Epoch 19/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5673 - mse: 0.5673\n",
      "Epoch 20/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5437 - mse: 0.5437\n"
     ]
    }
   ],
   "source": [
    "history = mf_model.fit(X_train, y_train, epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now try to predict the test ratings, and report our root mean squared error like in other regression problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m316/316\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step\n",
      " Test RMSE : 1.0801014839411307 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "y_pred = mf_model.predict(X_test)\n",
    "\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\" Test RMSE : %s \" % test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([333, 600, 128, ..., 410, 304, 248])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]\n",
    "# X_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get about 1.1/1.2 RMSE, we can probably do better !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding user and movie bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's enhance our matrix factorization model and add the user and movie biases to the rating estimation function as in equation (4) of Koren's paper ; except we will for the moment forget about the global bias $\\mu$ as it is not so intuitive to implement in Keras. Fill the function below to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Add\n",
    "\n",
    "def get_mf_bias_model(nb_users, nb_movies, k):\n",
    "    \"\"\"\n",
    "    Build a smatrix factorization model with user and movie biases\n",
    "    \n",
    "    Input:\n",
    "        nb_users : int : The number of unique users\n",
    "        nb_movies : int : The number of unique movies\n",
    "        k : int : The size of the embeddings\n",
    "        \n",
    "    Output:\n",
    "        model : keras.models.Model : A keras model that implements matrix factorization with biases\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    dim_embedddings = k\n",
    "    \n",
    "    # User embeddings\n",
    "    u = Input(shape=(1,), dtype='int32', name = 'u__user_id')\n",
    "    p_u = Embedding(nb_users, dim_embedddings, name=\"p_u__user_embedding\")(u)\n",
    "    p_u = Reshape((dim_embedddings,), name=\"p_u__user_embedding_reshaped\")(p_u)\n",
    "    \n",
    "    # Movie embeddings\n",
    "    i = Input(shape=(1,), dtype='int32', name = 'i__movie_id')\n",
    "    q_i = Embedding(nb_movies, dim_embedddings, name=\"q_i__movie_embedding\")(i)\n",
    "    q_i = Reshape((dim_embedddings,), name=\"q_i__movie_embedding_reshaped\")(q_i)\n",
    "    \n",
    "    #define bu\n",
    "    b_u = Embedding(nb_users, 1, name=\"b_u__user_embedding\")(u)\n",
    "    b_u = Reshape((1,), name=\"b_u__user_embedding_reshaped\")(b_u)\n",
    "    #define bi\n",
    "    b_i = Embedding(nb_movies, 1, name=\"b_i__user_embedding\")(i)\n",
    "    b_i = Reshape((1,), name=\"b_i__user_embedding_reshaped\")(b_i)\n",
    "\n",
    "    #Objective function r(hat)\n",
    "    d = Dot(axes = 1)([p_u, q_i]) \n",
    "    bias = Add()([b_i, b_u])\n",
    "    r_hat = Add()([d, bias]) \n",
    "\n",
    "    #We define our model by giving its input and outputs\n",
    "    model = Model(inputs=[u, i], outputs=r_hat)\n",
    "    \n",
    "    #Finally, we define the loss and metric to use, in our case the mean squared error,\n",
    "    #along with the optimization method, we'll understand what is 'adam' later also.\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[\"mse\"])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_bias_model = get_mf_bias_model(nb_users, nb_movies, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
       "\n",
       " u__user_id           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " i__movie_id          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " p_u__user_embedding  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,300</span>  u__user_id[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
       "\n",
       " q_i__movie_embeddi  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">291,720</span>  i__movie_id[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
       "\n",
       " b_i__user_embedding  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">9,724</span>  i__movie_id[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
       "\n",
       " b_u__user_embedding  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">610</span>  u__user_id[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
       "\n",
       " p_u__user_embeddin  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  p_u__user_embedd \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                                                             \n",
       "\n",
       " q_i__movie_embeddi  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  q_i__movie_embed \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                                                             \n",
       "\n",
       " b_i__user_embeddin  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  b_i__user_embedd \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                                                             \n",
       "\n",
       " b_u__user_embeddin  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  b_u__user_embedd \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                                                             \n",
       "\n",
       " dot_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  p_u__user_embedd \n",
       "                                                     q_i__movie_embed \n",
       "\n",
       " add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  b_i__user_embedd \n",
       "                                                     b_u__user_embedd \n",
       "\n",
       " add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dot_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
       "                                                     add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " u__user_id           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " i__movie_id          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " p_u__user_embedding  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m30\u001b[0m)          \u001b[38;5;34m18,300\u001b[0m  u__user_id[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
       "\n",
       " q_i__movie_embeddi  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m30\u001b[0m)         \u001b[38;5;34m291,720\u001b[0m  i__movie_id[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
       "\n",
       " b_i__user_embedding  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)            \u001b[38;5;34m9,724\u001b[0m  i__movie_id[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
       "\n",
       " b_u__user_embedding  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)              \u001b[38;5;34m610\u001b[0m  u__user_id[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
       "\n",
       " p_u__user_embeddin  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  p_u__user_embedd \n",
       " (\u001b[38;5;33mReshape\u001b[0m)                                                             \n",
       "\n",
       " q_i__movie_embeddi  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  q_i__movie_embed \n",
       " (\u001b[38;5;33mReshape\u001b[0m)                                                             \n",
       "\n",
       " b_i__user_embeddin  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  b_i__user_embedd \n",
       " (\u001b[38;5;33mReshape\u001b[0m)                                                             \n",
       "\n",
       " b_u__user_embeddin  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  b_u__user_embedd \n",
       " (\u001b[38;5;33mReshape\u001b[0m)                                                             \n",
       "\n",
       " dot_1 (\u001b[38;5;33mDot\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  p_u__user_embedd \n",
       "                                                     q_i__movie_embed \n",
       "\n",
       " add (\u001b[38;5;33mAdd\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  b_i__user_embedd \n",
       "                                                     b_u__user_embedd \n",
       "\n",
       " add_1 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  dot_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
       "                                                     add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,354</span> (1.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m320,354\u001b[0m (1.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,354</span> (1.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m320,354\u001b[0m (1.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mf_bias_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/ml4/lib/python3.12/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['u__user_id', 'i__movie_id']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 12.8012 - mse: 12.8012\n",
      "Epoch 2/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 10.0666 - mse: 10.0666\n",
      "Epoch 3/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.4546 - mse: 4.4546\n",
      "Epoch 4/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.0915 - mse: 2.0915\n",
      "Epoch 5/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.3982 - mse: 1.3982\n",
      "Epoch 6/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0782 - mse: 1.0782\n",
      "Epoch 7/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9167 - mse: 0.9167\n",
      "Epoch 8/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7986 - mse: 0.7986\n",
      "Epoch 9/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7202 - mse: 0.7202\n",
      "Epoch 10/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6643 - mse: 0.6643\n",
      "Epoch 11/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6150 - mse: 0.6150\n",
      "Epoch 12/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5891 - mse: 0.5891\n",
      "Epoch 13/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5594 - mse: 0.5594\n",
      "Epoch 14/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5353 - mse: 0.5353\n",
      "Epoch 15/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5177 - mse: 0.5177\n",
      "Epoch 16/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4974 - mse: 0.4974\n",
      "Epoch 17/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4821 - mse: 0.4821\n",
      "Epoch 18/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4709 - mse: 0.4709\n",
      "Epoch 19/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4512 - mse: 0.4512\n",
      "Epoch 20/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4347 - mse: 0.4347\n"
     ]
    }
   ],
   "source": [
    "history = mf_bias_model.fit(X_train, y_train, epochs=20, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m316/316\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      " Test RMSE : 0.9975148865575351 \n"
     ]
    }
   ],
   "source": [
    "y_pred = mf_bias_model.predict(X_test)\n",
    "\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\" Test RMSE : %s \" % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a lower RMSE, about 1.0/1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the moment we have omitted the regularization of the embeddings and bias parameters, as described in equation (5) of Koren's paper. We are now going to add them to the model, have a look at https://keras.io/layers/embeddings/ and https://keras.io/regularizers/ to see how to do this with keras. Fill the function below to implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "def get_mf_bias_l2_reg_model(nb_users, nb_movies, k, lambda_):\n",
    "    \"\"\"\n",
    "    Build a smatrix factorization model with user and movie biases, and L2 regularization\n",
    "    \n",
    "    Input:\n",
    "        nb_users : int : The number of unique users\n",
    "        nb_movies : int : The number of unique movies\n",
    "        k : int : The size of the embeddings\n",
    "        lambda_ : regularisation factor\n",
    "        \n",
    "    Output:\n",
    "        model : keras.models.Model : A keras model that implements matrix factorization with biases\n",
    "            and L2 regularization\n",
    "        \n",
    "    \"\"\"\n",
    "    dim_embedddings = k\n",
    "    \n",
    "    # adding regularization\n",
    "    reg = regularizers.l2(lambda_)\n",
    "    \n",
    "    # User embeddings\n",
    "    u = Input(shape=(1,), dtype='int32', name = 'u__user_id')\n",
    "    p_u = Embedding(nb_users, dim_embedddings, name=\"p_u__user_embedding\", embeddings_regularizer=reg)(u)\n",
    "    p_u = Reshape((dim_embedddings,), name=\"p_u__user_embedding_reshaped\")(p_u)\n",
    "    \n",
    "    # Movie embeddings\n",
    "    i = Input(shape=(1,), dtype='int32', name = 'i__movie_id')\n",
    "    q_i = Embedding(nb_movies, dim_embedddings, name=\"q_i__movie_embedding\", embeddings_regularizer=reg)(i)\n",
    "    q_i = Reshape((dim_embedddings,), name=\"q_i__movie_embedding_reshaped\")(q_i)\n",
    "\n",
    "    #user bias\n",
    "    b_u = Embedding(nb_users, 1, name=\"b_u__user_embedding\", embeddings_regularizer=reg)(u)\n",
    "    b_u = Reshape((1,), name=\"b_u__user_embedding_reshaped\")(b_u)\n",
    "    #movie bias\n",
    "    b_i = Embedding(nb_movies, 1, name=\"b_i__user_embedding\", embeddings_regularizer=reg)(i)\n",
    "    b_i = Reshape((1,), name=\"b_i__user_embedding_reshaped\")(b_i)\n",
    "    \n",
    "    #Objective function r(hat)\n",
    "    r_hat = Add()([Dot(axes = 1)([p_u, q_i]), Add()([b_i, b_u])]) \n",
    "\n",
    "    #We define our model by giving its input and outputs\n",
    "    model = Model(inputs=[u, i], outputs=r_hat)\n",
    "    \n",
    "    #Finally, we define the loss and metric to use, in our case the mean squared error,\n",
    "    #along with the optimization method, we'll understand what is 'adam' later also.\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[\"mse\"])\n",
    "    \n",
    "    return model    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.00001\n",
    "mf_bias_reg_model = get_mf_bias_l2_reg_model(nb_users, nb_movies, k, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
       "\n",
       " u__user_id           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " i__movie_id          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " p_u__user_embedding  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,300</span>  u__user_id[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
       "\n",
       " q_i__movie_embeddi  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">291,720</span>  i__movie_id[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
       "\n",
       " b_i__user_embedding  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">9,724</span>  i__movie_id[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
       "\n",
       " b_u__user_embedding  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">610</span>  u__user_id[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
       "\n",
       " p_u__user_embeddin  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  p_u__user_embedd \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                                                             \n",
       "\n",
       " q_i__movie_embeddi  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  q_i__movie_embed \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                                                             \n",
       "\n",
       " b_i__user_embeddin  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  b_i__user_embedd \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                                                             \n",
       "\n",
       " b_u__user_embeddin  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  b_u__user_embedd \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                                                             \n",
       "\n",
       " dot_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  p_u__user_embedd \n",
       "                                                     q_i__movie_embed \n",
       "\n",
       " add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  b_i__user_embedd \n",
       "                                                     b_u__user_embedd \n",
       "\n",
       " add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dot_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
       "                                                     add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " u__user_id           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " i__movie_id          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " p_u__user_embedding  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m30\u001b[0m)          \u001b[38;5;34m18,300\u001b[0m  u__user_id[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
       "\n",
       " q_i__movie_embeddi  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m30\u001b[0m)         \u001b[38;5;34m291,720\u001b[0m  i__movie_id[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
       "\n",
       " b_i__user_embedding  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)            \u001b[38;5;34m9,724\u001b[0m  i__movie_id[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
       "\n",
       " b_u__user_embedding  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)              \u001b[38;5;34m610\u001b[0m  u__user_id[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
       "\n",
       " p_u__user_embeddin  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  p_u__user_embedd \n",
       " (\u001b[38;5;33mReshape\u001b[0m)                                                             \n",
       "\n",
       " q_i__movie_embeddi  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  q_i__movie_embed \n",
       " (\u001b[38;5;33mReshape\u001b[0m)                                                             \n",
       "\n",
       " b_i__user_embeddin  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  b_i__user_embedd \n",
       " (\u001b[38;5;33mReshape\u001b[0m)                                                             \n",
       "\n",
       " b_u__user_embeddin  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  b_u__user_embedd \n",
       " (\u001b[38;5;33mReshape\u001b[0m)                                                             \n",
       "\n",
       " dot_2 (\u001b[38;5;33mDot\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  p_u__user_embedd \n",
       "                                                     q_i__movie_embed \n",
       "\n",
       " add_3 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  b_i__user_embedd \n",
       "                                                     b_u__user_embedd \n",
       "\n",
       " add_2 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  dot_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
       "                                                     add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,354</span> (1.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m320,354\u001b[0m (1.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,354</span> (1.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m320,354\u001b[0m (1.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mf_bias_reg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/ml4/lib/python3.12/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['u__user_id', 'i__movie_id']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 13.0457 - mse: 13.0438\n",
      "Epoch 2/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 10.8548 - mse: 10.8504\n",
      "Epoch 3/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.9099 - mse: 4.8829\n",
      "Epoch 4/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.2383 - mse: 2.1850\n",
      "Epoch 5/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.5829 - mse: 1.5134\n",
      "Epoch 6/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2733 - mse: 1.1919\n",
      "Epoch 7/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1174 - mse: 1.0267\n",
      "Epoch 8/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0197 - mse: 0.9214\n",
      "Epoch 9/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9519 - mse: 0.8473\n",
      "Epoch 10/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9103 - mse: 0.8004\n",
      "Epoch 11/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8667 - mse: 0.7524\n",
      "Epoch 12/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8385 - mse: 0.7203\n",
      "Epoch 13/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8201 - mse: 0.6985\n",
      "Epoch 14/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8053 - mse: 0.6808\n",
      "Epoch 15/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7897 - mse: 0.6628\n",
      "Epoch 16/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7824 - mse: 0.6534\n",
      "Epoch 17/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7716 - mse: 0.6408\n",
      "Epoch 18/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7541 - mse: 0.6216\n",
      "Epoch 19/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7420 - mse: 0.6081\n",
      "Epoch 20/20\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7314 - mse: 0.5963\n"
     ]
    }
   ],
   "source": [
    "history = mf_bias_reg_model.fit(X_train, y_train, epochs=20, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m316/316\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      " Test RMSE : 1.015759195845514 \n"
     ]
    }
   ],
   "source": [
    "y_pred = mf_bias_reg_model.predict(X_test)\n",
    "\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\" Test RMSE : %s \" % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might get a slightly worse (higher) RMSE, because adding regularization makes the optimization process more complex, and it probably requires more than 20 epochs to properly converge. But in the end its gonna yield better results with more iterations, so let's change this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of setting manually the maximum number of epochs, we prefer to use *early stopping*. When training with early stopping, keras keeps a given validation set though the parameter `validation_split`, on which it is going to monitor a performance measure you give it (here the `mse`) at every epoch, and continue optimization while the mse on the validation set keeps going down, and stops it when it goes back up. This mechanism is an easy way to avoid over-fitting, you can read more about it there : https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "\n",
    "In general when using early stopping we setup a high number of maximum epochs, that is never reach because the optimization is stopped by early stopping first :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/ml4/lib/python3.12/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['u__user_id', 'i__movie_id']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 13.0151 - mse: 13.0132 - val_loss: 11.9999 - val_mse: 11.9981\n",
      "Epoch 2/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 11.2958 - mse: 11.2925 - val_loss: 7.8890 - val_mse: 7.8758\n",
      "Epoch 3/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6.2990 - mse: 6.2794 - val_loss: 3.4102 - val_mse: 3.3705\n",
      "Epoch 4/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.7119 - mse: 2.6666 - val_loss: 2.2315 - val_mse: 2.1723\n",
      "Epoch 5/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.7674 - mse: 1.7047 - val_loss: 1.7933 - val_mse: 1.7210\n",
      "Epoch 6/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3894 - mse: 1.3143 - val_loss: 1.5651 - val_mse: 1.4825\n",
      "Epoch 7/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1722 - mse: 1.0874 - val_loss: 1.4307 - val_mse: 1.3397\n",
      "Epoch 8/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0439 - mse: 0.9512 - val_loss: 1.3472 - val_mse: 1.2494\n",
      "Epoch 9/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9571 - mse: 0.8577 - val_loss: 1.2912 - val_mse: 1.1876\n",
      "Epoch 10/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9022 - mse: 0.7973 - val_loss: 1.2558 - val_mse: 1.1472\n",
      "Epoch 11/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8466 - mse: 0.7370 - val_loss: 1.2303 - val_mse: 1.1175\n",
      "Epoch 12/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8140 - mse: 0.7004 - val_loss: 1.2135 - val_mse: 1.0973\n",
      "Epoch 13/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7857 - mse: 0.6687 - val_loss: 1.2004 - val_mse: 1.0810\n",
      "Epoch 14/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7741 - mse: 0.6541 - val_loss: 1.1930 - val_mse: 1.0712\n",
      "Epoch 15/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7482 - mse: 0.6258 - val_loss: 1.1865 - val_mse: 1.0624\n",
      "Epoch 16/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7407 - mse: 0.6160 - val_loss: 1.1823 - val_mse: 1.0562\n",
      "Epoch 17/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7214 - mse: 0.5949 - val_loss: 1.1801 - val_mse: 1.0524\n",
      "Epoch 18/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7083 - mse: 0.5803 - val_loss: 1.1796 - val_mse: 1.0505\n",
      "Epoch 19/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6930 - mse: 0.5635 - val_loss: 1.1775 - val_mse: 1.0472\n",
      "Epoch 20/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6903 - mse: 0.5597 - val_loss: 1.1770 - val_mse: 1.0455\n",
      "Epoch 21/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6824 - mse: 0.5506 - val_loss: 1.1766 - val_mse: 1.0442\n",
      "Epoch 22/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6752 - mse: 0.5426 - val_loss: 1.1768 - val_mse: 1.0436\n",
      "Epoch 23/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6597 - mse: 0.5263 - val_loss: 1.1764 - val_mse: 1.0425\n",
      "Epoch 24/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6540 - mse: 0.5199 - val_loss: 1.1759 - val_mse: 1.0414\n",
      "Epoch 25/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6344 - mse: 0.4998 - val_loss: 1.1773 - val_mse: 1.0421\n",
      "Epoch 26/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6333 - mse: 0.4980 - val_loss: 1.1778 - val_mse: 1.0421\n",
      "Epoch 27/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6104 - mse: 0.4746 - val_loss: 1.1773 - val_mse: 1.0411\n",
      "Epoch 28/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6086 - mse: 0.4724 - val_loss: 1.1786 - val_mse: 1.0420\n",
      "Epoch 29/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.5973 - mse: 0.4606 - val_loss: 1.1784 - val_mse: 1.0413\n",
      "Epoch 30/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.5918 - mse: 0.4547 - val_loss: 1.1824 - val_mse: 1.0449\n",
      "Epoch 31/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.5814 - mse: 0.4438 - val_loss: 1.1826 - val_mse: 1.0447\n",
      "Epoch 32/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.5772 - mse: 0.4393 - val_loss: 1.1855 - val_mse: 1.0471\n",
      "Epoch 32: early stopping\n",
      "Restoring model weights from the end of the best epoch: 27.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "mf_bias_reg_model = get_mf_bias_l2_reg_model(nb_users, nb_movies, k, lambda_)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_mse', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "history = mf_bias_reg_model.fit(X_train, y_train, epochs=500, batch_size=512, validation_split=0.1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the training stops before 500 epochs, when the validation MSE stops decreasing during 5 consecutive epochs (the patience value = 5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m316/316\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970us/step\n",
      " Test RMSE : 1.0269147361784732 \n"
     ]
    }
   ],
   "source": [
    "y_pred = mf_bias_reg_model.predict(X_test)\n",
    "\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\" Test RMSE : %s \" % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search embedding size and regularization factor with early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for the moment we didn't grid search our model hyper-parameters, such as `k` and `lambda_`. There exists some scikit-learn wrappers for keras models in order to use scikit grid search functions, unfortunately they only work with single input keras models, which is not our case as we have two inputs: the user and the movie indexes.\n",
    "\n",
    "So let's implement your own grid search function for the two parameters `k` and `lambda_`. With big enough datasets, it is not necessary to do a cross-validation for each hyper-parameter combination, and we can simply split the training set into a sub-training set and a validation set to test our hyper-parameters. It does work because the validation set is big enough to see enough data variations, and with very big datasets, it is anyway not possible anymore to do a full cross-validation as it takes too much time to train. \n",
    "\n",
    "Fill in the `grid_search` function below and use early stopping with a validation split (just like above), and retrieve the validation RMSE (you can get the MSE from the `history` variable that is returned by the `fit` method (and then take the `sqrt` of that)) for all the hyper-parameter combinations from the `param_grid` dictionary of hyper-parameter values. Call the `get_model_function` parameter (yes, you can pass functions as parameters!) to generate each model, and return the hyper-parameters that give the lowest RMSE on the 10% validation set, the RMSE value, and the best corresponding trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def grid_search(data, param_grid, get_model_function, nb_users, nb_movies, validation_size = 0.1):\n",
    "    \"\"\"\n",
    "    Performs a grid search over the \n",
    "    \n",
    "    Input:\n",
    "        data : DataFrame : The training set to be split between training and validation sets\n",
    "        param_grid : dict : Dictionary containing the values of the hyper-parameters to grid-search\n",
    "        get_model_function : function : A function that returns the keras model to grid-search\n",
    "        nb_users : int : The number of unique users\n",
    "        nb_movies : int : The number of unique movies\n",
    "        validation_size : float : Proportion of the validation set\n",
    "        \n",
    "    Output:\n",
    "        best_params : dict : A dictionary of the best hyper-parameters values\n",
    "        best_score : float : The validation RMSE corresponding to the best\n",
    "        best_model : keras.Model : The model trained with the best hyper-parameters\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    best_score = np.inf\n",
    "    best_params = {}\n",
    "    best_model = None\n",
    "    \n",
    "    #TOFILL\n",
    "    # Get the keys and the values as lists\n",
    "    params_keys = list(param_grid.keys())\n",
    "    params_values = list(param_grid.values())\n",
    "    # X = np.to_numpdata[[\"userId\",\"movieId\"]]\n",
    "    # y = data[\"rating\"]\n",
    "    # train,valid = train_test_split(data,train_size=1-validation_size)\n",
    "    X_train = [data[\"userId\"].to_numpy(), data[\"movieId\"].to_numpy()]\n",
    "    y_train = data[\"rating\"].to_numpy()\n",
    "    # print(len(X_train), type(X_train), len(y_train), type(y_train),)\n",
    "    # print(len(X_train[0]))\n",
    "    # Generate all combinations of parameter values\n",
    "    for combination in itertools.product(*params_values):\n",
    "        params = dict(zip(params_keys, combination))\n",
    "        k, lambda_ = combination\n",
    "        model = get_model_function(nb_users,nb_movies,k,lambda_)\n",
    "        early_stopping = EarlyStopping(monitor='val_mse', patience=5, verbose=1, restore_best_weights=True)\n",
    "        history=model.fit(X_train, y_train, epochs=500, batch_size=512, validation_split=validation_size, callbacks=[early_stopping])\n",
    "        y_pred = model.predict(X_train)\n",
    "        valid_rmse = sqrt(np.min(history.history['val_mse']))\n",
    "        print(f\"best valid RMSE : {valid_rmse},\\t\\t\\t params = {params}\")\n",
    "        if valid_rmse < best_score:\n",
    "            print(f\"-------------------------\\nNew best RMSE {valid_rmse} < {best_score},\\nparams = {params}\")\n",
    "            best_score = valid_rmse\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "    return best_params, best_score, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Functional name=functional_3, built=True>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([333, 600, 128, ..., 410, 304, 248]),\n",
       " array([5363, 8358,  838, ...,  461, 4354, 8285])]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = train\n",
    "X_train = [data[\"userId\"].to_numpy(), data[\"movieId\"].to_numpy()]\n",
    "y_train = data[\"rating\"].to_numpy()\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 12.9929 - mse: 12.9804 - val_loss: 12.0729 - val_mse: 12.0624\n",
      "Epoch 2/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.5704 - mse: 11.5466 - val_loss: 9.5244 - val_mse: 9.4101\n",
      "Epoch 3/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.4498 - mse: 8.2736 - val_loss: 5.8790 - val_mse: 5.4854\n",
      "Epoch 4/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.0762 - mse: 4.6063 - val_loss: 4.0368 - val_mse: 3.3582\n",
      "Epoch 5/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5954 - mse: 2.8630 - val_loss: 3.4348 - val_mse: 2.5674\n",
      "Epoch 6/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.0861 - mse: 2.1848 - val_loss: 3.1695 - val_mse: 2.1808\n",
      "Epoch 7/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8471 - mse: 1.8355 - val_loss: 3.0210 - val_mse: 1.9479\n",
      "Epoch 8/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.7032 - mse: 1.6135 - val_loss: 2.9258 - val_mse: 1.7914\n",
      "Epoch 9/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6110 - mse: 1.4645 - val_loss: 2.8594 - val_mse: 1.6809\n",
      "Epoch 10/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5505 - mse: 1.3625 - val_loss: 2.8110 - val_mse: 1.6016\n",
      "Epoch 11/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4888 - mse: 1.2737 - val_loss: 2.7699 - val_mse: 1.5381\n",
      "Epoch 12/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4495 - mse: 1.2137 - val_loss: 2.7356 - val_mse: 1.4887\n",
      "Epoch 13/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4231 - mse: 1.1733 - val_loss: 2.7042 - val_mse: 1.4484\n",
      "Epoch 14/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3909 - mse: 1.1338 - val_loss: 2.6757 - val_mse: 1.4159\n",
      "Epoch 15/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.3534 - mse: 1.0930 - val_loss: 2.6481 - val_mse: 1.3882\n",
      "Epoch 16/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.3384 - mse: 1.0788 - val_loss: 2.6220 - val_mse: 1.3635\n",
      "Epoch 17/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.3072 - mse: 1.0497 - val_loss: 2.5957 - val_mse: 1.3420\n",
      "Epoch 18/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.2951 - mse: 1.0431 - val_loss: 2.5692 - val_mse: 1.3209\n",
      "Epoch 19/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.2643 - mse: 1.0186 - val_loss: 2.5443 - val_mse: 1.3040\n",
      "Epoch 20/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.2490 - mse: 1.0107 - val_loss: 2.5192 - val_mse: 1.2880\n",
      "Epoch 21/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.2183 - mse: 0.9899 - val_loss: 2.4942 - val_mse: 1.2732\n",
      "Epoch 22/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2041 - mse: 0.9859 - val_loss: 2.4685 - val_mse: 1.2571\n",
      "Epoch 23/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.1816 - mse: 0.9733 - val_loss: 2.4443 - val_mse: 1.2439\n",
      "Epoch 24/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1707 - mse: 0.9728 - val_loss: 2.4198 - val_mse: 1.2307\n",
      "Epoch 25/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1451 - mse: 0.9591 - val_loss: 2.3966 - val_mse: 1.2194\n",
      "Epoch 26/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1185 - mse: 0.9444 - val_loss: 2.3734 - val_mse: 1.2082\n",
      "Epoch 27/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.1053 - mse: 0.9430 - val_loss: 2.3505 - val_mse: 1.1962\n",
      "Epoch 28/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.0920 - mse: 0.9404 - val_loss: 2.3278 - val_mse: 1.1845\n",
      "Epoch 29/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0669 - mse: 0.9268 - val_loss: 2.3066 - val_mse: 1.1756\n",
      "Epoch 30/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.0459 - mse: 0.9178 - val_loss: 2.2852 - val_mse: 1.1653\n",
      "Epoch 31/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.0327 - mse: 0.9157 - val_loss: 2.2641 - val_mse: 1.1545\n",
      "Epoch 32/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.0113 - mse: 0.9046 - val_loss: 2.2442 - val_mse: 1.1454\n",
      "Epoch 33/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9999 - mse: 0.9035 - val_loss: 2.2249 - val_mse: 1.1365\n",
      "Epoch 34/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.9775 - mse: 0.8916 - val_loss: 2.2057 - val_mse: 1.1272\n",
      "Epoch 35/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.9706 - mse: 0.8945 - val_loss: 2.1880 - val_mse: 1.1194\n",
      "Epoch 36/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.9545 - mse: 0.8879 - val_loss: 2.1711 - val_mse: 1.1120\n",
      "Epoch 37/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9400 - mse: 0.8834 - val_loss: 2.1546 - val_mse: 1.1040\n",
      "Epoch 38/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.9306 - mse: 0.8821 - val_loss: 2.1375 - val_mse: 1.0955\n",
      "Epoch 39/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.9069 - mse: 0.8672 - val_loss: 2.1223 - val_mse: 1.0889\n",
      "Epoch 40/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.8899 - mse: 0.8584 - val_loss: 2.1077 - val_mse: 1.0822\n",
      "Epoch 41/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.8807 - mse: 0.8572 - val_loss: 2.0937 - val_mse: 1.0753\n",
      "Epoch 42/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8809 - mse: 0.8641 - val_loss: 2.0800 - val_mse: 1.0694\n",
      "Epoch 43/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8555 - mse: 0.8462 - val_loss: 2.0670 - val_mse: 1.0626\n",
      "Epoch 44/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.8569 - mse: 0.8541 - val_loss: 2.0544 - val_mse: 1.0566\n",
      "Epoch 45/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.8483 - mse: 0.8525 - val_loss: 2.0427 - val_mse: 1.0508\n",
      "Epoch 46/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8284 - mse: 0.8384 - val_loss: 2.0312 - val_mse: 1.0452\n",
      "Epoch 47/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8240 - mse: 0.8392 - val_loss: 2.0212 - val_mse: 1.0409\n",
      "Epoch 48/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.8156 - mse: 0.8366 - val_loss: 2.0108 - val_mse: 1.0353\n",
      "Epoch 49/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.8083 - mse: 0.8342 - val_loss: 2.0019 - val_mse: 1.0317\n",
      "Epoch 50/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.8038 - mse: 0.8341 - val_loss: 1.9917 - val_mse: 1.0261\n",
      "Epoch 51/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8024 - mse: 0.8383 - val_loss: 1.9836 - val_mse: 1.0219\n",
      "Epoch 52/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.7868 - mse: 0.8263 - val_loss: 1.9748 - val_mse: 1.0176\n",
      "Epoch 53/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.7800 - mse: 0.8237 - val_loss: 1.9671 - val_mse: 1.0138\n",
      "Epoch 54/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.7735 - mse: 0.8208 - val_loss: 1.9598 - val_mse: 1.0097\n",
      "Epoch 55/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.7710 - mse: 0.8216 - val_loss: 1.9524 - val_mse: 1.0058\n",
      "Epoch 56/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7618 - mse: 0.8156 - val_loss: 1.9455 - val_mse: 1.0025\n",
      "Epoch 57/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.7515 - mse: 0.8090 - val_loss: 1.9395 - val_mse: 0.9994\n",
      "Epoch 58/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.7441 - mse: 0.8044 - val_loss: 1.9328 - val_mse: 0.9951\n",
      "Epoch 59/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.7469 - mse: 0.8100 - val_loss: 1.9272 - val_mse: 0.9915\n",
      "Epoch 60/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.7436 - mse: 0.8084 - val_loss: 1.9221 - val_mse: 0.9888\n",
      "Epoch 61/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7316 - mse: 0.7992 - val_loss: 1.9169 - val_mse: 0.9859\n",
      "Epoch 62/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7363 - mse: 0.8056 - val_loss: 1.9121 - val_mse: 0.9831\n",
      "Epoch 63/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.7348 - mse: 0.8060 - val_loss: 1.9074 - val_mse: 0.9801\n",
      "Epoch 64/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.7228 - mse: 0.7961 - val_loss: 1.9026 - val_mse: 0.9772\n",
      "Epoch 65/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.7190 - mse: 0.7936 - val_loss: 1.8990 - val_mse: 0.9748\n",
      "Epoch 66/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7125 - mse: 0.7887 - val_loss: 1.8954 - val_mse: 0.9728\n",
      "Epoch 67/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.7151 - mse: 0.7921 - val_loss: 1.8918 - val_mse: 0.9706\n",
      "Epoch 68/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.7094 - mse: 0.7882 - val_loss: 1.8880 - val_mse: 0.9677\n",
      "Epoch 69/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7082 - mse: 0.7878 - val_loss: 1.8852 - val_mse: 0.9658\n",
      "Epoch 70/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6927 - mse: 0.7735 - val_loss: 1.8824 - val_mse: 0.9640\n",
      "Epoch 71/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6996 - mse: 0.7811 - val_loss: 1.8797 - val_mse: 0.9618\n",
      "Epoch 72/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6991 - mse: 0.7810 - val_loss: 1.8773 - val_mse: 0.9600\n",
      "Epoch 73/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6952 - mse: 0.7780 - val_loss: 1.8750 - val_mse: 0.9579\n",
      "Epoch 74/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6926 - mse: 0.7755 - val_loss: 1.8726 - val_mse: 0.9564\n",
      "Epoch 75/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6921 - mse: 0.7756 - val_loss: 1.8700 - val_mse: 0.9543\n",
      "Epoch 76/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6942 - mse: 0.7783 - val_loss: 1.8679 - val_mse: 0.9521\n",
      "Epoch 77/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6830 - mse: 0.7673 - val_loss: 1.8665 - val_mse: 0.9507\n",
      "Epoch 78/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6821 - mse: 0.7664 - val_loss: 1.8642 - val_mse: 0.9486\n",
      "Epoch 79/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6758 - mse: 0.7597 - val_loss: 1.8631 - val_mse: 0.9478\n",
      "Epoch 80/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6824 - mse: 0.7671 - val_loss: 1.8620 - val_mse: 0.9465\n",
      "Epoch 81/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6793 - mse: 0.7637 - val_loss: 1.8604 - val_mse: 0.9450\n",
      "Epoch 82/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6801 - mse: 0.7644 - val_loss: 1.8593 - val_mse: 0.9439\n",
      "Epoch 83/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6768 - mse: 0.7609 - val_loss: 1.8581 - val_mse: 0.9425\n",
      "Epoch 84/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6706 - mse: 0.7546 - val_loss: 1.8567 - val_mse: 0.9407\n",
      "Epoch 85/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6728 - mse: 0.7571 - val_loss: 1.8558 - val_mse: 0.9396\n",
      "Epoch 86/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6699 - mse: 0.7536 - val_loss: 1.8550 - val_mse: 0.9385\n",
      "Epoch 87/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6689 - mse: 0.7524 - val_loss: 1.8545 - val_mse: 0.9374\n",
      "Epoch 88/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6674 - mse: 0.7502 - val_loss: 1.8536 - val_mse: 0.9361\n",
      "Epoch 89/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6683 - mse: 0.7507 - val_loss: 1.8531 - val_mse: 0.9353\n",
      "Epoch 90/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6640 - mse: 0.7463 - val_loss: 1.8527 - val_mse: 0.9342\n",
      "Epoch 91/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6627 - mse: 0.7437 - val_loss: 1.8517 - val_mse: 0.9332\n",
      "Epoch 92/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6677 - mse: 0.7487 - val_loss: 1.8514 - val_mse: 0.9325\n",
      "Epoch 93/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6618 - mse: 0.7423 - val_loss: 1.8508 - val_mse: 0.9314\n",
      "Epoch 94/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6611 - mse: 0.7415 - val_loss: 1.8504 - val_mse: 0.9304\n",
      "Epoch 95/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6580 - mse: 0.7375 - val_loss: 1.8498 - val_mse: 0.9293\n",
      "Epoch 96/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6643 - mse: 0.7431 - val_loss: 1.8495 - val_mse: 0.9285\n",
      "Epoch 97/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6650 - mse: 0.7438 - val_loss: 1.8493 - val_mse: 0.9280\n",
      "Epoch 98/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6601 - mse: 0.7383 - val_loss: 1.8490 - val_mse: 0.9267\n",
      "Epoch 99/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6603 - mse: 0.7382 - val_loss: 1.8486 - val_mse: 0.9258\n",
      "Epoch 100/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6599 - mse: 0.7371 - val_loss: 1.8486 - val_mse: 0.9250\n",
      "Epoch 101/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6559 - mse: 0.7322 - val_loss: 1.8485 - val_mse: 0.9245\n",
      "Epoch 102/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6550 - mse: 0.7308 - val_loss: 1.8484 - val_mse: 0.9239\n",
      "Epoch 103/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6596 - mse: 0.7349 - val_loss: 1.8483 - val_mse: 0.9233\n",
      "Epoch 104/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6565 - mse: 0.7306 - val_loss: 1.8481 - val_mse: 0.9230\n",
      "Epoch 105/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6548 - mse: 0.7291 - val_loss: 1.8484 - val_mse: 0.9222\n",
      "Epoch 106/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6464 - mse: 0.7204 - val_loss: 1.8483 - val_mse: 0.9216\n",
      "Epoch 107/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6536 - mse: 0.7268 - val_loss: 1.8483 - val_mse: 0.9208\n",
      "Epoch 108/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6497 - mse: 0.7220 - val_loss: 1.8482 - val_mse: 0.9202\n",
      "Epoch 109/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6543 - mse: 0.7259 - val_loss: 1.8483 - val_mse: 0.9197\n",
      "Epoch 110/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6502 - mse: 0.7213 - val_loss: 1.8488 - val_mse: 0.9195\n",
      "Epoch 111/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6521 - mse: 0.7227 - val_loss: 1.8487 - val_mse: 0.9192\n",
      "Epoch 112/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6470 - mse: 0.7170 - val_loss: 1.8486 - val_mse: 0.9184\n",
      "Epoch 113/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6504 - mse: 0.7196 - val_loss: 1.8488 - val_mse: 0.9181\n",
      "Epoch 114/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6506 - mse: 0.7193 - val_loss: 1.8489 - val_mse: 0.9176\n",
      "Epoch 115/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6474 - mse: 0.7158 - val_loss: 1.8490 - val_mse: 0.9170\n",
      "Epoch 116/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6492 - mse: 0.7168 - val_loss: 1.8495 - val_mse: 0.9171\n",
      "Epoch 117/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6471 - mse: 0.7142 - val_loss: 1.8493 - val_mse: 0.9163\n",
      "Epoch 118/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6438 - mse: 0.7103 - val_loss: 1.8499 - val_mse: 0.9161\n",
      "Epoch 119/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6469 - mse: 0.7129 - val_loss: 1.8500 - val_mse: 0.9154\n",
      "Epoch 120/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6394 - mse: 0.7048 - val_loss: 1.8503 - val_mse: 0.9152\n",
      "Epoch 121/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6458 - mse: 0.7109 - val_loss: 1.8504 - val_mse: 0.9143\n",
      "Epoch 122/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6544 - mse: 0.7178 - val_loss: 1.8512 - val_mse: 0.9146\n",
      "Epoch 123/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6441 - mse: 0.7073 - val_loss: 1.8510 - val_mse: 0.9140\n",
      "Epoch 124/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6463 - mse: 0.7093 - val_loss: 1.8512 - val_mse: 0.9135\n",
      "Epoch 125/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6433 - mse: 0.7055 - val_loss: 1.8514 - val_mse: 0.9133\n",
      "Epoch 126/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6509 - mse: 0.7126 - val_loss: 1.8515 - val_mse: 0.9129\n",
      "Epoch 127/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6402 - mse: 0.7013 - val_loss: 1.8517 - val_mse: 0.9123\n",
      "Epoch 128/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6490 - mse: 0.7091 - val_loss: 1.8518 - val_mse: 0.9120\n",
      "Epoch 129/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6388 - mse: 0.6989 - val_loss: 1.8524 - val_mse: 0.9120\n",
      "Epoch 130/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6486 - mse: 0.7078 - val_loss: 1.8528 - val_mse: 0.9118\n",
      "Epoch 131/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6374 - mse: 0.6961 - val_loss: 1.8530 - val_mse: 0.9114\n",
      "Epoch 132/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6464 - mse: 0.7046 - val_loss: 1.8531 - val_mse: 0.9110\n",
      "Epoch 133/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6342 - mse: 0.6917 - val_loss: 1.8536 - val_mse: 0.9110\n",
      "Epoch 134/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6382 - mse: 0.6956 - val_loss: 1.8537 - val_mse: 0.9105\n",
      "Epoch 135/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6448 - mse: 0.7010 - val_loss: 1.8546 - val_mse: 0.9111\n",
      "Epoch 136/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6441 - mse: 0.7006 - val_loss: 1.8543 - val_mse: 0.9099\n",
      "Epoch 137/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6514 - mse: 0.7066 - val_loss: 1.8549 - val_mse: 0.9101\n",
      "Epoch 138/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6449 - mse: 0.6998 - val_loss: 1.8548 - val_mse: 0.9099\n",
      "Epoch 139/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6476 - mse: 0.7016 - val_loss: 1.8552 - val_mse: 0.9096\n",
      "Epoch 140/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6349 - mse: 0.6890 - val_loss: 1.8557 - val_mse: 0.9096\n",
      "Epoch 141/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6406 - mse: 0.6942 - val_loss: 1.8560 - val_mse: 0.9093\n",
      "Epoch 142/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6421 - mse: 0.6951 - val_loss: 1.8562 - val_mse: 0.9093\n",
      "Epoch 143/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6373 - mse: 0.6900 - val_loss: 1.8564 - val_mse: 0.9088\n",
      "Epoch 144/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6327 - mse: 0.6848 - val_loss: 1.8568 - val_mse: 0.9087\n",
      "Epoch 145/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6410 - mse: 0.6924 - val_loss: 1.8571 - val_mse: 0.9085\n",
      "Epoch 146/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6358 - mse: 0.6871 - val_loss: 1.8571 - val_mse: 0.9082\n",
      "Epoch 147/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6380 - mse: 0.6887 - val_loss: 1.8577 - val_mse: 0.9084\n",
      "Epoch 148/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6393 - mse: 0.6897 - val_loss: 1.8580 - val_mse: 0.9081\n",
      "Epoch 149/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6385 - mse: 0.6884 - val_loss: 1.8582 - val_mse: 0.9080\n",
      "Epoch 150/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.6407 - mse: 0.6901 - val_loss: 1.8588 - val_mse: 0.9080\n",
      "Epoch 151/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6437 - mse: 0.6926 - val_loss: 1.8588 - val_mse: 0.9078\n",
      "Epoch 152/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6404 - mse: 0.6894 - val_loss: 1.8589 - val_mse: 0.9074\n",
      "Epoch 153/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6422 - mse: 0.6906 - val_loss: 1.8594 - val_mse: 0.9073\n",
      "Epoch 154/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6404 - mse: 0.6880 - val_loss: 1.8598 - val_mse: 0.9073\n",
      "Epoch 155/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6404 - mse: 0.6878 - val_loss: 1.8600 - val_mse: 0.9071\n",
      "Epoch 156/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6390 - mse: 0.6863 - val_loss: 1.8601 - val_mse: 0.9070\n",
      "Epoch 157/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6448 - mse: 0.6913 - val_loss: 1.8608 - val_mse: 0.9071\n",
      "Epoch 158/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6385 - mse: 0.6851 - val_loss: 1.8607 - val_mse: 0.9069\n",
      "Epoch 159/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6429 - mse: 0.6887 - val_loss: 1.8609 - val_mse: 0.9065\n",
      "Epoch 160/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6413 - mse: 0.6869 - val_loss: 1.8612 - val_mse: 0.9065\n",
      "Epoch 161/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6282 - mse: 0.6736 - val_loss: 1.8612 - val_mse: 0.9064\n",
      "Epoch 162/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6467 - mse: 0.6914 - val_loss: 1.8615 - val_mse: 0.9064\n",
      "Epoch 163/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6420 - mse: 0.6868 - val_loss: 1.8617 - val_mse: 0.9064\n",
      "Epoch 164/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6400 - mse: 0.6843 - val_loss: 1.8622 - val_mse: 0.9066\n",
      "Epoch 165/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6386 - mse: 0.6828 - val_loss: 1.8624 - val_mse: 0.9064\n",
      "Epoch 166/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6408 - mse: 0.6845 - val_loss: 1.8624 - val_mse: 0.9060\n",
      "Epoch 167/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6331 - mse: 0.6767 - val_loss: 1.8627 - val_mse: 0.9061\n",
      "Epoch 168/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6417 - mse: 0.6847 - val_loss: 1.8630 - val_mse: 0.9063\n",
      "Epoch 169/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6337 - mse: 0.6769 - val_loss: 1.8630 - val_mse: 0.9057\n",
      "Epoch 170/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6366 - mse: 0.6792 - val_loss: 1.8632 - val_mse: 0.9056\n",
      "Epoch 171/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6375 - mse: 0.6799 - val_loss: 1.8634 - val_mse: 0.9055\n",
      "Epoch 172/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6422 - mse: 0.6839 - val_loss: 1.8635 - val_mse: 0.9055\n",
      "Epoch 173/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6411 - mse: 0.6828 - val_loss: 1.8638 - val_mse: 0.9057\n",
      "Epoch 174/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6381 - mse: 0.6798 - val_loss: 1.8639 - val_mse: 0.9057\n",
      "Epoch 175/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6392 - mse: 0.6808 - val_loss: 1.8644 - val_mse: 0.9059\n",
      "Epoch 176/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6373 - mse: 0.6780 - val_loss: 1.8647 - val_mse: 0.9057\n",
      "Epoch 176: early stopping\n",
      "Restoring model weights from the end of the best epoch: 171.\n",
      "\u001b[1m2836/2836\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "best valid RMSE : 0.9515813302618304,\t\t\t params = {'k': 15, 'lambda_': 0.0002}\n",
      "-------------------------\n",
      "New best RMSE 0.9515813302618304 < inf,\n",
      "params = {'k': 15, 'lambda_': 0.0002}\n",
      "Epoch 1/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 13.0220 - mse: 13.0182 - val_loss: 12.0685 - val_mse: 12.0648\n",
      "Epoch 2/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.5597 - mse: 11.5523 - val_loss: 9.3248 - val_mse: 9.2919\n",
      "Epoch 3/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.1893 - mse: 8.1391 - val_loss: 5.3488 - val_mse: 5.2380\n",
      "Epoch 4/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.4632 - mse: 4.3305 - val_loss: 3.2644 - val_mse: 3.0704\n",
      "Epoch 5/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.7478 - mse: 2.5373 - val_loss: 2.5367 - val_mse: 2.2825\n",
      "Epoch 6/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.1379 - mse: 1.8721 - val_loss: 2.1944 - val_mse: 1.8971\n",
      "Epoch 7/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.8167 - mse: 1.5104 - val_loss: 1.9938 - val_mse: 1.6625\n",
      "Epoch 8/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6385 - mse: 1.2998 - val_loss: 1.8677 - val_mse: 1.5088\n",
      "Epoch 9/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.5146 - mse: 1.1496 - val_loss: 1.7841 - val_mse: 1.4022\n",
      "Epoch 10/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.4295 - mse: 1.0428 - val_loss: 1.7261 - val_mse: 1.3252\n",
      "Epoch 11/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.3669 - mse: 0.9617 - val_loss: 1.6841 - val_mse: 1.2675\n",
      "Epoch 12/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.3283 - mse: 0.9083 - val_loss: 1.6542 - val_mse: 1.2245\n",
      "Epoch 13/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.2901 - mse: 0.8575 - val_loss: 1.6312 - val_mse: 1.1908\n",
      "Epoch 14/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2551 - mse: 0.8123 - val_loss: 1.6144 - val_mse: 1.1653\n",
      "Epoch 15/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.2388 - mse: 0.7877 - val_loss: 1.6013 - val_mse: 1.1451\n",
      "Epoch 16/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.2215 - mse: 0.7638 - val_loss: 1.5899 - val_mse: 1.1278\n",
      "Epoch 17/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2102 - mse: 0.7469 - val_loss: 1.5803 - val_mse: 1.1136\n",
      "Epoch 18/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2043 - mse: 0.7365 - val_loss: 1.5731 - val_mse: 1.1027\n",
      "Epoch 19/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1919 - mse: 0.7208 - val_loss: 1.5673 - val_mse: 1.0941\n",
      "Epoch 20/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1814 - mse: 0.7078 - val_loss: 1.5604 - val_mse: 1.0853\n",
      "Epoch 21/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1723 - mse: 0.6967 - val_loss: 1.5545 - val_mse: 1.0778\n",
      "Epoch 22/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1606 - mse: 0.6837 - val_loss: 1.5504 - val_mse: 1.0730\n",
      "Epoch 23/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1527 - mse: 0.6750 - val_loss: 1.5446 - val_mse: 1.0667\n",
      "Epoch 24/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1513 - mse: 0.6733 - val_loss: 1.5387 - val_mse: 1.0606\n",
      "Epoch 25/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1438 - mse: 0.6657 - val_loss: 1.5335 - val_mse: 1.0559\n",
      "Epoch 26/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1332 - mse: 0.6558 - val_loss: 1.5285 - val_mse: 1.0512\n",
      "Epoch 27/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1223 - mse: 0.6453 - val_loss: 1.5240 - val_mse: 1.0475\n",
      "Epoch 28/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1180 - mse: 0.6417 - val_loss: 1.5176 - val_mse: 1.0420\n",
      "Epoch 29/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1099 - mse: 0.6346 - val_loss: 1.5148 - val_mse: 1.0403\n",
      "Epoch 30/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1020 - mse: 0.6279 - val_loss: 1.5099 - val_mse: 1.0366\n",
      "Epoch 31/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1041 - mse: 0.6314 - val_loss: 1.5043 - val_mse: 1.0323\n",
      "Epoch 32/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0913 - mse: 0.6197 - val_loss: 1.4994 - val_mse: 1.0288\n",
      "Epoch 33/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0778 - mse: 0.6075 - val_loss: 1.4944 - val_mse: 1.0252\n",
      "Epoch 34/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0759 - mse: 0.6072 - val_loss: 1.4899 - val_mse: 1.0224\n",
      "Epoch 35/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0731 - mse: 0.6057 - val_loss: 1.4852 - val_mse: 1.0187\n",
      "Epoch 36/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0593 - mse: 0.5932 - val_loss: 1.4803 - val_mse: 1.0156\n",
      "Epoch 37/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0676 - mse: 0.6033 - val_loss: 1.4752 - val_mse: 1.0117\n",
      "Epoch 38/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0493 - mse: 0.5863 - val_loss: 1.4720 - val_mse: 1.0101\n",
      "Epoch 39/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0460 - mse: 0.5844 - val_loss: 1.4664 - val_mse: 1.0056\n",
      "Epoch 40/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0396 - mse: 0.5793 - val_loss: 1.4621 - val_mse: 1.0030\n",
      "Epoch 41/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0279 - mse: 0.5690 - val_loss: 1.4581 - val_mse: 1.0002\n",
      "Epoch 42/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0301 - mse: 0.5726 - val_loss: 1.4533 - val_mse: 0.9965\n",
      "Epoch 43/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0255 - mse: 0.5691 - val_loss: 1.4492 - val_mse: 0.9933\n",
      "Epoch 44/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0160 - mse: 0.5606 - val_loss: 1.4454 - val_mse: 0.9909\n",
      "Epoch 45/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0072 - mse: 0.5532 - val_loss: 1.4418 - val_mse: 0.9882\n",
      "Epoch 46/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0039 - mse: 0.5508 - val_loss: 1.4376 - val_mse: 0.9852\n",
      "Epoch 47/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9981 - mse: 0.5461 - val_loss: 1.4342 - val_mse: 0.9826\n",
      "Epoch 48/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9877 - mse: 0.5365 - val_loss: 1.4308 - val_mse: 0.9803\n",
      "Epoch 49/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9782 - mse: 0.5281 - val_loss: 1.4275 - val_mse: 0.9777\n",
      "Epoch 50/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9708 - mse: 0.5213 - val_loss: 1.4231 - val_mse: 0.9740\n",
      "Epoch 51/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9655 - mse: 0.5168 - val_loss: 1.4195 - val_mse: 0.9711\n",
      "Epoch 52/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9661 - mse: 0.5181 - val_loss: 1.4165 - val_mse: 0.9686\n",
      "Epoch 53/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9584 - mse: 0.5109 - val_loss: 1.4132 - val_mse: 0.9661\n",
      "Epoch 54/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9543 - mse: 0.5075 - val_loss: 1.4102 - val_mse: 0.9634\n",
      "Epoch 55/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9486 - mse: 0.5021 - val_loss: 1.4066 - val_mse: 0.9603\n",
      "Epoch 56/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9399 - mse: 0.4939 - val_loss: 1.4035 - val_mse: 0.9577\n",
      "Epoch 57/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9332 - mse: 0.4875 - val_loss: 1.4015 - val_mse: 0.9559\n",
      "Epoch 58/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9289 - mse: 0.4835 - val_loss: 1.3978 - val_mse: 0.9526\n",
      "Epoch 59/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9204 - mse: 0.4754 - val_loss: 1.3969 - val_mse: 0.9518\n",
      "Epoch 60/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9199 - mse: 0.4751 - val_loss: 1.3940 - val_mse: 0.9492\n",
      "Epoch 61/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9118 - mse: 0.4672 - val_loss: 1.3917 - val_mse: 0.9469\n",
      "Epoch 62/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9061 - mse: 0.4615 - val_loss: 1.3900 - val_mse: 0.9453\n",
      "Epoch 63/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9037 - mse: 0.4592 - val_loss: 1.3879 - val_mse: 0.9434\n",
      "Epoch 64/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8937 - mse: 0.4492 - val_loss: 1.3855 - val_mse: 0.9410\n",
      "Epoch 65/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8891 - mse: 0.4447 - val_loss: 1.3834 - val_mse: 0.9387\n",
      "Epoch 66/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8878 - mse: 0.4431 - val_loss: 1.3817 - val_mse: 0.9372\n",
      "Epoch 67/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8770 - mse: 0.4325 - val_loss: 1.3810 - val_mse: 0.9362\n",
      "Epoch 68/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8708 - mse: 0.4260 - val_loss: 1.3781 - val_mse: 0.9333\n",
      "Epoch 69/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8673 - mse: 0.4224 - val_loss: 1.3782 - val_mse: 0.9335\n",
      "Epoch 70/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8649 - mse: 0.4201 - val_loss: 1.3763 - val_mse: 0.9314\n",
      "Epoch 71/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8561 - mse: 0.4113 - val_loss: 1.3750 - val_mse: 0.9299\n",
      "Epoch 72/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8559 - mse: 0.4109 - val_loss: 1.3736 - val_mse: 0.9282\n",
      "Epoch 73/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8478 - mse: 0.4024 - val_loss: 1.3726 - val_mse: 0.9271\n",
      "Epoch 74/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8488 - mse: 0.4034 - val_loss: 1.3714 - val_mse: 0.9256\n",
      "Epoch 75/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8420 - mse: 0.3963 - val_loss: 1.3705 - val_mse: 0.9245\n",
      "Epoch 76/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8343 - mse: 0.3885 - val_loss: 1.3698 - val_mse: 0.9236\n",
      "Epoch 77/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8342 - mse: 0.3882 - val_loss: 1.3695 - val_mse: 0.9231\n",
      "Epoch 78/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8339 - mse: 0.3876 - val_loss: 1.3680 - val_mse: 0.9217\n",
      "Epoch 79/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8254 - mse: 0.3791 - val_loss: 1.3667 - val_mse: 0.9202\n",
      "Epoch 80/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8249 - mse: 0.3783 - val_loss: 1.3663 - val_mse: 0.9196\n",
      "Epoch 81/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8162 - mse: 0.3697 - val_loss: 1.3658 - val_mse: 0.9189\n",
      "Epoch 82/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8157 - mse: 0.3690 - val_loss: 1.3650 - val_mse: 0.9178\n",
      "Epoch 83/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8141 - mse: 0.3671 - val_loss: 1.3645 - val_mse: 0.9172\n",
      "Epoch 84/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8068 - mse: 0.3596 - val_loss: 1.3634 - val_mse: 0.9158\n",
      "Epoch 85/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8123 - mse: 0.3649 - val_loss: 1.3638 - val_mse: 0.9161\n",
      "Epoch 86/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8057 - mse: 0.3581 - val_loss: 1.3627 - val_mse: 0.9150\n",
      "Epoch 87/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8003 - mse: 0.3526 - val_loss: 1.3627 - val_mse: 0.9149\n",
      "Epoch 88/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8005 - mse: 0.3528 - val_loss: 1.3622 - val_mse: 0.9144\n",
      "Epoch 89/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7949 - mse: 0.3471 - val_loss: 1.3620 - val_mse: 0.9140\n",
      "Epoch 90/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7954 - mse: 0.3475 - val_loss: 1.3607 - val_mse: 0.9125\n",
      "Epoch 91/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7895 - mse: 0.3415 - val_loss: 1.3611 - val_mse: 0.9129\n",
      "Epoch 92/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7834 - mse: 0.3353 - val_loss: 1.3604 - val_mse: 0.9122\n",
      "Epoch 93/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7843 - mse: 0.3362 - val_loss: 1.3594 - val_mse: 0.9110\n",
      "Epoch 94/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7853 - mse: 0.3369 - val_loss: 1.3596 - val_mse: 0.9112\n",
      "Epoch 95/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7814 - mse: 0.3331 - val_loss: 1.3589 - val_mse: 0.9106\n",
      "Epoch 96/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7779 - mse: 0.3297 - val_loss: 1.3584 - val_mse: 0.9100\n",
      "Epoch 97/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7757 - mse: 0.3275 - val_loss: 1.3577 - val_mse: 0.9092\n",
      "Epoch 98/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7752 - mse: 0.3268 - val_loss: 1.3578 - val_mse: 0.9093\n",
      "Epoch 99/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7687 - mse: 0.3204 - val_loss: 1.3568 - val_mse: 0.9082\n",
      "Epoch 100/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7709 - mse: 0.3224 - val_loss: 1.3570 - val_mse: 0.9085\n",
      "Epoch 101/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7686 - mse: 0.3202 - val_loss: 1.3569 - val_mse: 0.9085\n",
      "Epoch 102/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7679 - mse: 0.3195 - val_loss: 1.3570 - val_mse: 0.9085\n",
      "Epoch 103/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7620 - mse: 0.3136 - val_loss: 1.3553 - val_mse: 0.9069\n",
      "Epoch 104/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7640 - mse: 0.3158 - val_loss: 1.3566 - val_mse: 0.9082\n",
      "Epoch 105/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7612 - mse: 0.3129 - val_loss: 1.3550 - val_mse: 0.9066\n",
      "Epoch 106/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7578 - mse: 0.3097 - val_loss: 1.3550 - val_mse: 0.9065\n",
      "Epoch 107/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7602 - mse: 0.3118 - val_loss: 1.3547 - val_mse: 0.9064\n",
      "Epoch 108/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7554 - mse: 0.3073 - val_loss: 1.3551 - val_mse: 0.9067\n",
      "Epoch 109/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7540 - mse: 0.3057 - val_loss: 1.3539 - val_mse: 0.9057\n",
      "Epoch 110/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7540 - mse: 0.3059 - val_loss: 1.3540 - val_mse: 0.9059\n",
      "Epoch 111/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7529 - mse: 0.3050 - val_loss: 1.3532 - val_mse: 0.9050\n",
      "Epoch 112/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7486 - mse: 0.3006 - val_loss: 1.3535 - val_mse: 0.9056\n",
      "Epoch 113/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7488 - mse: 0.3011 - val_loss: 1.3522 - val_mse: 0.9044\n",
      "Epoch 114/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7469 - mse: 0.2993 - val_loss: 1.3529 - val_mse: 0.9051\n",
      "Epoch 115/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7483 - mse: 0.3005 - val_loss: 1.3519 - val_mse: 0.9042\n",
      "Epoch 116/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7452 - mse: 0.2976 - val_loss: 1.3514 - val_mse: 0.9038\n",
      "Epoch 117/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7437 - mse: 0.2961 - val_loss: 1.3516 - val_mse: 0.9040\n",
      "Epoch 118/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7448 - mse: 0.2974 - val_loss: 1.3514 - val_mse: 0.9038\n",
      "Epoch 119/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7413 - mse: 0.2939 - val_loss: 1.3507 - val_mse: 0.9034\n",
      "Epoch 120/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7413 - mse: 0.2942 - val_loss: 1.3505 - val_mse: 0.9031\n",
      "Epoch 121/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7391 - mse: 0.2920 - val_loss: 1.3503 - val_mse: 0.9031\n",
      "Epoch 122/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7377 - mse: 0.2907 - val_loss: 1.3497 - val_mse: 0.9025\n",
      "Epoch 123/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7352 - mse: 0.2883 - val_loss: 1.3489 - val_mse: 0.9018\n",
      "Epoch 124/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7373 - mse: 0.2903 - val_loss: 1.3483 - val_mse: 0.9015\n",
      "Epoch 125/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7363 - mse: 0.2897 - val_loss: 1.3482 - val_mse: 0.9013\n",
      "Epoch 126/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7361 - mse: 0.2894 - val_loss: 1.3479 - val_mse: 0.9011\n",
      "Epoch 127/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7343 - mse: 0.2876 - val_loss: 1.3476 - val_mse: 0.9010\n",
      "Epoch 128/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7332 - mse: 0.2868 - val_loss: 1.3475 - val_mse: 0.9011\n",
      "Epoch 129/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7308 - mse: 0.2846 - val_loss: 1.3466 - val_mse: 0.9003\n",
      "Epoch 130/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7321 - mse: 0.2859 - val_loss: 1.3467 - val_mse: 0.9004\n",
      "Epoch 131/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7332 - mse: 0.2872 - val_loss: 1.3460 - val_mse: 0.8998\n",
      "Epoch 132/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7269 - mse: 0.2809 - val_loss: 1.3458 - val_mse: 0.8997\n",
      "Epoch 133/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7300 - mse: 0.2841 - val_loss: 1.3454 - val_mse: 0.8994\n",
      "Epoch 134/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7330 - mse: 0.2872 - val_loss: 1.3448 - val_mse: 0.8990\n",
      "Epoch 135/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7295 - mse: 0.2838 - val_loss: 1.3445 - val_mse: 0.8988\n",
      "Epoch 136/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7257 - mse: 0.2800 - val_loss: 1.3444 - val_mse: 0.8990\n",
      "Epoch 137/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7267 - mse: 0.2813 - val_loss: 1.3437 - val_mse: 0.8983\n",
      "Epoch 138/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7209 - mse: 0.2757 - val_loss: 1.3434 - val_mse: 0.8981\n",
      "Epoch 139/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7243 - mse: 0.2791 - val_loss: 1.3428 - val_mse: 0.8975\n",
      "Epoch 140/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7232 - mse: 0.2782 - val_loss: 1.3424 - val_mse: 0.8973\n",
      "Epoch 141/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7242 - mse: 0.2794 - val_loss: 1.3417 - val_mse: 0.8965\n",
      "Epoch 142/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7189 - mse: 0.2739 - val_loss: 1.3419 - val_mse: 0.8969\n",
      "Epoch 143/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7167 - mse: 0.2720 - val_loss: 1.3414 - val_mse: 0.8965\n",
      "Epoch 144/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7210 - mse: 0.2763 - val_loss: 1.3408 - val_mse: 0.8961\n",
      "Epoch 145/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7189 - mse: 0.2744 - val_loss: 1.3402 - val_mse: 0.8956\n",
      "Epoch 146/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7184 - mse: 0.2739 - val_loss: 1.3403 - val_mse: 0.8958\n",
      "Epoch 147/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7178 - mse: 0.2733 - val_loss: 1.3396 - val_mse: 0.8953\n",
      "Epoch 148/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7210 - mse: 0.2768 - val_loss: 1.3390 - val_mse: 0.8947\n",
      "Epoch 149/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7193 - mse: 0.2750 - val_loss: 1.3393 - val_mse: 0.8951\n",
      "Epoch 150/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7163 - mse: 0.2723 - val_loss: 1.3385 - val_mse: 0.8944\n",
      "Epoch 151/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7179 - mse: 0.2740 - val_loss: 1.3381 - val_mse: 0.8941\n",
      "Epoch 152/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7105 - mse: 0.2668 - val_loss: 1.3381 - val_mse: 0.8943\n",
      "Epoch 153/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7161 - mse: 0.2722 - val_loss: 1.3373 - val_mse: 0.8934\n",
      "Epoch 154/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7151 - mse: 0.2714 - val_loss: 1.3374 - val_mse: 0.8937\n",
      "Epoch 155/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7146 - mse: 0.2710 - val_loss: 1.3364 - val_mse: 0.8928\n",
      "Epoch 156/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7147 - mse: 0.2713 - val_loss: 1.3364 - val_mse: 0.8928\n",
      "Epoch 157/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7130 - mse: 0.2696 - val_loss: 1.3362 - val_mse: 0.8928\n",
      "Epoch 158/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7127 - mse: 0.2694 - val_loss: 1.3360 - val_mse: 0.8927\n",
      "Epoch 159/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7131 - mse: 0.2700 - val_loss: 1.3351 - val_mse: 0.8918\n",
      "Epoch 160/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7120 - mse: 0.2689 - val_loss: 1.3348 - val_mse: 0.8917\n",
      "Epoch 161/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7121 - mse: 0.2691 - val_loss: 1.3345 - val_mse: 0.8914\n",
      "Epoch 162/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7112 - mse: 0.2683 - val_loss: 1.3344 - val_mse: 0.8914\n",
      "Epoch 163/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7102 - mse: 0.2673 - val_loss: 1.3339 - val_mse: 0.8910\n",
      "Epoch 164/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7078 - mse: 0.2650 - val_loss: 1.3337 - val_mse: 0.8910\n",
      "Epoch 165/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7095 - mse: 0.2669 - val_loss: 1.3332 - val_mse: 0.8905\n",
      "Epoch 166/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7086 - mse: 0.2661 - val_loss: 1.3330 - val_mse: 0.8904\n",
      "Epoch 167/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7090 - mse: 0.2665 - val_loss: 1.3329 - val_mse: 0.8904\n",
      "Epoch 168/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7049 - mse: 0.2626 - val_loss: 1.3322 - val_mse: 0.8897\n",
      "Epoch 169/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7069 - mse: 0.2644 - val_loss: 1.3321 - val_mse: 0.8897\n",
      "Epoch 170/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7094 - mse: 0.2671 - val_loss: 1.3317 - val_mse: 0.8893\n",
      "Epoch 171/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7094 - mse: 0.2671 - val_loss: 1.3319 - val_mse: 0.8897\n",
      "Epoch 172/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7076 - mse: 0.2655 - val_loss: 1.3312 - val_mse: 0.8891\n",
      "Epoch 173/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7078 - mse: 0.2658 - val_loss: 1.3308 - val_mse: 0.8887\n",
      "Epoch 174/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7052 - mse: 0.2631 - val_loss: 1.3302 - val_mse: 0.8882\n",
      "Epoch 175/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7066 - mse: 0.2646 - val_loss: 1.3303 - val_mse: 0.8884\n",
      "Epoch 176/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7062 - mse: 0.2644 - val_loss: 1.3298 - val_mse: 0.8878\n",
      "Epoch 177/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7063 - mse: 0.2645 - val_loss: 1.3298 - val_mse: 0.8879\n",
      "Epoch 178/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7032 - mse: 0.2615 - val_loss: 1.3294 - val_mse: 0.8876\n",
      "Epoch 179/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7013 - mse: 0.2598 - val_loss: 1.3291 - val_mse: 0.8874\n",
      "Epoch 180/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7043 - mse: 0.2626 - val_loss: 1.3288 - val_mse: 0.8872\n",
      "Epoch 181/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7036 - mse: 0.2620 - val_loss: 1.3289 - val_mse: 0.8873\n",
      "Epoch 182/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7000 - mse: 0.2586 - val_loss: 1.3283 - val_mse: 0.8868\n",
      "Epoch 183/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7018 - mse: 0.2604 - val_loss: 1.3280 - val_mse: 0.8864\n",
      "Epoch 184/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7041 - mse: 0.2626 - val_loss: 1.3278 - val_mse: 0.8864\n",
      "Epoch 185/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7012 - mse: 0.2599 - val_loss: 1.3280 - val_mse: 0.8865\n",
      "Epoch 186/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6990 - mse: 0.2577 - val_loss: 1.3276 - val_mse: 0.8863\n",
      "Epoch 187/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7030 - mse: 0.2619 - val_loss: 1.3275 - val_mse: 0.8860\n",
      "Epoch 188/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7015 - mse: 0.2601 - val_loss: 1.3271 - val_mse: 0.8858\n",
      "Epoch 189/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7021 - mse: 0.2610 - val_loss: 1.3270 - val_mse: 0.8857\n",
      "Epoch 190/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7000 - mse: 0.2590 - val_loss: 1.3265 - val_mse: 0.8853\n",
      "Epoch 191/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6993 - mse: 0.2582 - val_loss: 1.3265 - val_mse: 0.8854\n",
      "Epoch 192/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6958 - mse: 0.2548 - val_loss: 1.3259 - val_mse: 0.8849\n",
      "Epoch 193/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6980 - mse: 0.2572 - val_loss: 1.3261 - val_mse: 0.8851\n",
      "Epoch 194/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6979 - mse: 0.2569 - val_loss: 1.3256 - val_mse: 0.8847\n",
      "Epoch 195/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7008 - mse: 0.2600 - val_loss: 1.3252 - val_mse: 0.8842\n",
      "Epoch 196/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6985 - mse: 0.2576 - val_loss: 1.3251 - val_mse: 0.8842\n",
      "Epoch 197/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6965 - mse: 0.2557 - val_loss: 1.3249 - val_mse: 0.8842\n",
      "Epoch 198/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6980 - mse: 0.2574 - val_loss: 1.3244 - val_mse: 0.8836\n",
      "Epoch 199/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6962 - mse: 0.2556 - val_loss: 1.3242 - val_mse: 0.8835\n",
      "Epoch 200/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6963 - mse: 0.2556 - val_loss: 1.3242 - val_mse: 0.8835\n",
      "Epoch 201/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6995 - mse: 0.2588 - val_loss: 1.3246 - val_mse: 0.8838\n",
      "Epoch 202/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6967 - mse: 0.2562 - val_loss: 1.3240 - val_mse: 0.8833\n",
      "Epoch 203/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6969 - mse: 0.2564 - val_loss: 1.3241 - val_mse: 0.8835\n",
      "Epoch 204/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6971 - mse: 0.2566 - val_loss: 1.3237 - val_mse: 0.8831\n",
      "Epoch 205/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6951 - mse: 0.2546 - val_loss: 1.3236 - val_mse: 0.8830\n",
      "Epoch 206/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6964 - mse: 0.2559 - val_loss: 1.3235 - val_mse: 0.8829\n",
      "Epoch 207/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6966 - mse: 0.2561 - val_loss: 1.3234 - val_mse: 0.8830\n",
      "Epoch 208/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6955 - mse: 0.2552 - val_loss: 1.3233 - val_mse: 0.8828\n",
      "Epoch 209/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6948 - mse: 0.2544 - val_loss: 1.3229 - val_mse: 0.8825\n",
      "Epoch 210/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6948 - mse: 0.2545 - val_loss: 1.3231 - val_mse: 0.8828\n",
      "Epoch 211/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6950 - mse: 0.2547 - val_loss: 1.3228 - val_mse: 0.8824\n",
      "Epoch 212/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6951 - mse: 0.2548 - val_loss: 1.3228 - val_mse: 0.8824\n",
      "Epoch 213/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6922 - mse: 0.2519 - val_loss: 1.3222 - val_mse: 0.8819\n",
      "Epoch 214/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6925 - mse: 0.2523 - val_loss: 1.3222 - val_mse: 0.8817\n",
      "Epoch 215/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6946 - mse: 0.2543 - val_loss: 1.3222 - val_mse: 0.8819\n",
      "Epoch 216/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6968 - mse: 0.2567 - val_loss: 1.3219 - val_mse: 0.8816\n",
      "Epoch 217/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6965 - mse: 0.2563 - val_loss: 1.3219 - val_mse: 0.8816\n",
      "Epoch 218/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6914 - mse: 0.2513 - val_loss: 1.3220 - val_mse: 0.8818\n",
      "Epoch 219/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6959 - mse: 0.2557 - val_loss: 1.3210 - val_mse: 0.8808\n",
      "Epoch 220/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6965 - mse: 0.2565 - val_loss: 1.3213 - val_mse: 0.8810\n",
      "Epoch 221/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6927 - mse: 0.2526 - val_loss: 1.3208 - val_mse: 0.8805\n",
      "Epoch 222/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6942 - mse: 0.2540 - val_loss: 1.3208 - val_mse: 0.8806\n",
      "Epoch 223/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6959 - mse: 0.2557 - val_loss: 1.3210 - val_mse: 0.8808\n",
      "Epoch 224/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6946 - mse: 0.2546 - val_loss: 1.3209 - val_mse: 0.8807\n",
      "Epoch 225/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6919 - mse: 0.2518 - val_loss: 1.3204 - val_mse: 0.8803\n",
      "Epoch 226/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6896 - mse: 0.2496 - val_loss: 1.3208 - val_mse: 0.8807\n",
      "Epoch 227/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6930 - mse: 0.2530 - val_loss: 1.3204 - val_mse: 0.8803\n",
      "Epoch 228/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6919 - mse: 0.2520 - val_loss: 1.3199 - val_mse: 0.8798\n",
      "Epoch 229/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6912 - mse: 0.2513 - val_loss: 1.3202 - val_mse: 0.8801\n",
      "Epoch 230/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6900 - mse: 0.2501 - val_loss: 1.3201 - val_mse: 0.8799\n",
      "Epoch 231/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6935 - mse: 0.2534 - val_loss: 1.3197 - val_mse: 0.8796\n",
      "Epoch 232/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6936 - mse: 0.2536 - val_loss: 1.3200 - val_mse: 0.8800\n",
      "Epoch 233/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6936 - mse: 0.2537 - val_loss: 1.3201 - val_mse: 0.8800\n",
      "Epoch 234/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6922 - mse: 0.2522 - val_loss: 1.3198 - val_mse: 0.8797\n",
      "Epoch 235/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6904 - mse: 0.2504 - val_loss: 1.3196 - val_mse: 0.8796\n",
      "Epoch 236/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6925 - mse: 0.2526 - val_loss: 1.3194 - val_mse: 0.8794\n",
      "Epoch 237/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6918 - mse: 0.2518 - val_loss: 1.3193 - val_mse: 0.8792\n",
      "Epoch 238/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6925 - mse: 0.2526 - val_loss: 1.3190 - val_mse: 0.8790\n",
      "Epoch 239/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6893 - mse: 0.2494 - val_loss: 1.3193 - val_mse: 0.8793\n",
      "Epoch 240/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6927 - mse: 0.2528 - val_loss: 1.3194 - val_mse: 0.8794\n",
      "Epoch 241/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6902 - mse: 0.2504 - val_loss: 1.3190 - val_mse: 0.8791\n",
      "Epoch 242/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6899 - mse: 0.2500 - val_loss: 1.3194 - val_mse: 0.8794\n",
      "Epoch 243/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6885 - mse: 0.2487 - val_loss: 1.3188 - val_mse: 0.8787\n",
      "Epoch 244/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6931 - mse: 0.2532 - val_loss: 1.3191 - val_mse: 0.8790\n",
      "Epoch 245/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6874 - mse: 0.2476 - val_loss: 1.3189 - val_mse: 0.8789\n",
      "Epoch 246/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6900 - mse: 0.2501 - val_loss: 1.3190 - val_mse: 0.8790\n",
      "Epoch 247/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6900 - mse: 0.2502 - val_loss: 1.3188 - val_mse: 0.8788\n",
      "Epoch 248/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6879 - mse: 0.2479 - val_loss: 1.3187 - val_mse: 0.8788\n",
      "Epoch 248: early stopping\n",
      "Restoring model weights from the end of the best epoch: 243.\n",
      "\u001b[1m2836/2836\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "best valid RMSE : 0.9374045005477063,\t\t\t params = {'k': 15, 'lambda_': 5e-05}\n",
      "-------------------------\n",
      "New best RMSE 0.9374045005477063 < 0.9515813302618304,\n",
      "params = {'k': 15, 'lambda_': 5e-05}\n",
      "Epoch 1/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 13.0565 - mse: 13.0548 - val_loss: 12.0857 - val_mse: 12.0840\n",
      "Epoch 2/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 11.6282 - mse: 11.6250 - val_loss: 9.4201 - val_mse: 9.4069\n",
      "Epoch 3/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.2576 - mse: 8.2376 - val_loss: 5.3099 - val_mse: 5.2647\n",
      "Epoch 4/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.3894 - mse: 4.3350 - val_loss: 3.0887 - val_mse: 3.0081\n",
      "Epoch 5/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.5882 - mse: 2.5005 - val_loss: 2.3184 - val_mse: 2.2120\n",
      "Epoch 6/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.8968 - mse: 1.7854 - val_loss: 1.9526 - val_mse: 1.8273\n",
      "Epoch 7/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.5695 - mse: 1.4402 - val_loss: 1.7390 - val_mse: 1.5985\n",
      "Epoch 8/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.3758 - mse: 1.2321 - val_loss: 1.6001 - val_mse: 1.4470\n",
      "Epoch 9/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.2444 - mse: 1.0885 - val_loss: 1.5060 - val_mse: 1.3421\n",
      "Epoch 10/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1626 - mse: 0.9962 - val_loss: 1.4402 - val_mse: 1.2670\n",
      "Epoch 11/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0920 - mse: 0.9167 - val_loss: 1.3933 - val_mse: 1.2120\n",
      "Epoch 12/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0294 - mse: 0.8463 - val_loss: 1.3574 - val_mse: 1.1691\n",
      "Epoch 13/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0031 - mse: 0.8133 - val_loss: 1.3306 - val_mse: 1.1362\n",
      "Epoch 14/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9780 - mse: 0.7823 - val_loss: 1.3114 - val_mse: 1.1117\n",
      "Epoch 15/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9491 - mse: 0.7482 - val_loss: 1.2950 - val_mse: 1.0907\n",
      "Epoch 16/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9345 - mse: 0.7292 - val_loss: 1.2827 - val_mse: 1.0743\n",
      "Epoch 17/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9129 - mse: 0.7035 - val_loss: 1.2738 - val_mse: 1.0619\n",
      "Epoch 18/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9009 - mse: 0.6882 - val_loss: 1.2654 - val_mse: 1.0504\n",
      "Epoch 19/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8869 - mse: 0.6712 - val_loss: 1.2592 - val_mse: 1.0416\n",
      "Epoch 20/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8760 - mse: 0.6578 - val_loss: 1.2530 - val_mse: 1.0331\n",
      "Epoch 21/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8664 - mse: 0.6460 - val_loss: 1.2495 - val_mse: 1.0276\n",
      "Epoch 22/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8654 - mse: 0.6432 - val_loss: 1.2457 - val_mse: 1.0221\n",
      "Epoch 23/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8564 - mse: 0.6324 - val_loss: 1.2421 - val_mse: 1.0172\n",
      "Epoch 24/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8441 - mse: 0.6190 - val_loss: 1.2395 - val_mse: 1.0134\n",
      "Epoch 25/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8442 - mse: 0.6177 - val_loss: 1.2366 - val_mse: 1.0094\n",
      "Epoch 26/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8322 - mse: 0.6048 - val_loss: 1.2334 - val_mse: 1.0054\n",
      "Epoch 27/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8289 - mse: 0.6008 - val_loss: 1.2312 - val_mse: 1.0025\n",
      "Epoch 28/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8215 - mse: 0.5926 - val_loss: 1.2290 - val_mse: 0.9999\n",
      "Epoch 29/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8190 - mse: 0.5898 - val_loss: 1.2266 - val_mse: 0.9970\n",
      "Epoch 30/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8125 - mse: 0.5829 - val_loss: 1.2244 - val_mse: 0.9944\n",
      "Epoch 31/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8028 - mse: 0.5727 - val_loss: 1.2228 - val_mse: 0.9926\n",
      "Epoch 32/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7927 - mse: 0.5624 - val_loss: 1.2212 - val_mse: 0.9909\n",
      "Epoch 33/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7908 - mse: 0.5604 - val_loss: 1.2198 - val_mse: 0.9893\n",
      "Epoch 34/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7835 - mse: 0.5529 - val_loss: 1.2181 - val_mse: 0.9874\n",
      "Epoch 35/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7790 - mse: 0.5484 - val_loss: 1.2158 - val_mse: 0.9852\n",
      "Epoch 36/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7711 - mse: 0.5405 - val_loss: 1.2148 - val_mse: 0.9841\n",
      "Epoch 37/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7692 - mse: 0.5387 - val_loss: 1.2141 - val_mse: 0.9833\n",
      "Epoch 38/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7553 - mse: 0.5247 - val_loss: 1.2124 - val_mse: 0.9818\n",
      "Epoch 39/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7457 - mse: 0.5151 - val_loss: 1.2125 - val_mse: 0.9820\n",
      "Epoch 40/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7455 - mse: 0.5151 - val_loss: 1.2111 - val_mse: 0.9806\n",
      "Epoch 41/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7396 - mse: 0.5092 - val_loss: 1.2104 - val_mse: 0.9799\n",
      "Epoch 42/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7307 - mse: 0.5003 - val_loss: 1.2093 - val_mse: 0.9789\n",
      "Epoch 43/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7283 - mse: 0.4980 - val_loss: 1.2079 - val_mse: 0.9775\n",
      "Epoch 44/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7154 - mse: 0.4851 - val_loss: 1.2080 - val_mse: 0.9777\n",
      "Epoch 45/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7125 - mse: 0.4824 - val_loss: 1.2078 - val_mse: 0.9777\n",
      "Epoch 46/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6996 - mse: 0.4695 - val_loss: 1.2073 - val_mse: 0.9772\n",
      "Epoch 47/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6967 - mse: 0.4667 - val_loss: 1.2073 - val_mse: 0.9773\n",
      "Epoch 48/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6914 - mse: 0.4615 - val_loss: 1.2080 - val_mse: 0.9779\n",
      "Epoch 49/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6865 - mse: 0.4565 - val_loss: 1.2075 - val_mse: 0.9775\n",
      "Epoch 50/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6776 - mse: 0.4477 - val_loss: 1.2071 - val_mse: 0.9770\n",
      "Epoch 51/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6654 - mse: 0.4355 - val_loss: 1.2081 - val_mse: 0.9781\n",
      "Epoch 52/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6668 - mse: 0.4369 - val_loss: 1.2082 - val_mse: 0.9781\n",
      "Epoch 53/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6549 - mse: 0.4248 - val_loss: 1.2089 - val_mse: 0.9788\n",
      "Epoch 54/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6528 - mse: 0.4227 - val_loss: 1.2088 - val_mse: 0.9785\n",
      "Epoch 55/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6412 - mse: 0.4109 - val_loss: 1.2095 - val_mse: 0.9791\n",
      "Epoch 55: early stopping\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "\u001b[1m2836/2836\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "best valid RMSE : 0.9884312029676271,\t\t\t params = {'k': 15, 'lambda_': 2e-05}\n",
      "Epoch 1/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 13.0554 - mse: 13.0317 - val_loss: 12.0644 - val_mse: 12.0487\n",
      "Epoch 2/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 11.3749 - mse: 11.3350 - val_loss: 8.1773 - val_mse: 7.9578\n",
      "Epoch 3/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 6.7718 - mse: 6.4393 - val_loss: 4.3414 - val_mse: 3.6728\n",
      "Epoch 4/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.7893 - mse: 3.0377 - val_loss: 3.4820 - val_mse: 2.5360\n",
      "Epoch 5/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.0929 - mse: 2.1029 - val_loss: 3.1962 - val_mse: 2.0961\n",
      "Epoch 6/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.8380 - mse: 1.7111 - val_loss: 3.0615 - val_mse: 1.8646\n",
      "Epoch 7/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.7112 - mse: 1.4972 - val_loss: 2.9833 - val_mse: 1.7241\n",
      "Epoch 8/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.6411 - mse: 1.3709 - val_loss: 2.9301 - val_mse: 1.6314\n",
      "Epoch 9/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.5939 - mse: 1.2877 - val_loss: 2.8884 - val_mse: 1.5651\n",
      "Epoch 10/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.5539 - mse: 1.2265 - val_loss: 2.8545 - val_mse: 1.5167\n",
      "Epoch 11/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.5223 - mse: 1.1817 - val_loss: 2.8234 - val_mse: 1.4797\n",
      "Epoch 12/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.4888 - mse: 1.1440 - val_loss: 2.7911 - val_mse: 1.4469\n",
      "Epoch 13/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.4593 - mse: 1.1154 - val_loss: 2.7642 - val_mse: 1.4230\n",
      "Epoch 14/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.4385 - mse: 1.0985 - val_loss: 2.7357 - val_mse: 1.4007\n",
      "Epoch 15/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.4153 - mse: 1.0818 - val_loss: 2.7063 - val_mse: 1.3804\n",
      "Epoch 16/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.3953 - mse: 1.0718 - val_loss: 2.6794 - val_mse: 1.3640\n",
      "Epoch 17/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.3642 - mse: 1.0510 - val_loss: 2.6496 - val_mse: 1.3456\n",
      "Epoch 18/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.3336 - mse: 1.0329 - val_loss: 2.6218 - val_mse: 1.3298\n",
      "Epoch 19/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.3136 - mse: 1.0247 - val_loss: 2.5933 - val_mse: 1.3154\n",
      "Epoch 20/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.2918 - mse: 1.0171 - val_loss: 2.5657 - val_mse: 1.3008\n",
      "Epoch 21/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.2634 - mse: 1.0016 - val_loss: 2.5373 - val_mse: 1.2865\n",
      "Epoch 22/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.2446 - mse: 0.9977 - val_loss: 2.5072 - val_mse: 1.2699\n",
      "Epoch 23/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.2213 - mse: 0.9874 - val_loss: 2.4821 - val_mse: 1.2591\n",
      "Epoch 24/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.1986 - mse: 0.9785 - val_loss: 2.4540 - val_mse: 1.2459\n",
      "Epoch 25/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.1669 - mse: 0.9622 - val_loss: 2.4283 - val_mse: 1.2331\n",
      "Epoch 26/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.1378 - mse: 0.9464 - val_loss: 2.4025 - val_mse: 1.2208\n",
      "Epoch 27/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.1169 - mse: 0.9383 - val_loss: 2.3766 - val_mse: 1.2080\n",
      "Epoch 28/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.1069 - mse: 0.9418 - val_loss: 2.3530 - val_mse: 1.1977\n",
      "Epoch 29/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.0798 - mse: 0.9278 - val_loss: 2.3298 - val_mse: 1.1871\n",
      "Epoch 30/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.0593 - mse: 0.9193 - val_loss: 2.3061 - val_mse: 1.1747\n",
      "Epoch 31/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.0399 - mse: 0.9111 - val_loss: 2.2854 - val_mse: 1.1654\n",
      "Epoch 32/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.0180 - mse: 0.9015 - val_loss: 2.2651 - val_mse: 1.1557\n",
      "Epoch 33/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.0010 - mse: 0.8946 - val_loss: 2.2447 - val_mse: 1.1457\n",
      "Epoch 34/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.9806 - mse: 0.8851 - val_loss: 2.2254 - val_mse: 1.1363\n",
      "Epoch 35/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.9666 - mse: 0.8800 - val_loss: 2.2071 - val_mse: 1.1282\n",
      "Epoch 36/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.9521 - mse: 0.8755 - val_loss: 2.1885 - val_mse: 1.1179\n",
      "Epoch 37/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9334 - mse: 0.8646 - val_loss: 2.1723 - val_mse: 1.1107\n",
      "Epoch 38/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.9177 - mse: 0.8583 - val_loss: 2.1550 - val_mse: 1.1012\n",
      "Epoch 39/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.9104 - mse: 0.8586 - val_loss: 2.1390 - val_mse: 1.0939\n",
      "Epoch 40/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9005 - mse: 0.8562 - val_loss: 2.1254 - val_mse: 1.0874\n",
      "Epoch 41/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8830 - mse: 0.8463 - val_loss: 2.1112 - val_mse: 1.0805\n",
      "Epoch 42/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8755 - mse: 0.8459 - val_loss: 2.0970 - val_mse: 1.0721\n",
      "Epoch 43/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8569 - mse: 0.8334 - val_loss: 2.0834 - val_mse: 1.0648\n",
      "Epoch 44/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8421 - mse: 0.8245 - val_loss: 2.0717 - val_mse: 1.0590\n",
      "Epoch 45/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8324 - mse: 0.8207 - val_loss: 2.0601 - val_mse: 1.0536\n",
      "Epoch 46/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8232 - mse: 0.8171 - val_loss: 2.0495 - val_mse: 1.0475\n",
      "Epoch 47/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8170 - mse: 0.8154 - val_loss: 2.0386 - val_mse: 1.0421\n",
      "Epoch 48/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8169 - mse: 0.8209 - val_loss: 2.0282 - val_mse: 1.0351\n",
      "Epoch 49/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8054 - mse: 0.8133 - val_loss: 2.0176 - val_mse: 1.0291\n",
      "Epoch 50/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7852 - mse: 0.7971 - val_loss: 2.0091 - val_mse: 1.0247\n",
      "Epoch 51/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7822 - mse: 0.7986 - val_loss: 2.0002 - val_mse: 1.0188\n",
      "Epoch 52/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7760 - mse: 0.7955 - val_loss: 1.9922 - val_mse: 1.0146\n",
      "Epoch 53/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7679 - mse: 0.7903 - val_loss: 1.9847 - val_mse: 1.0104\n",
      "Epoch 54/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7583 - mse: 0.7841 - val_loss: 1.9777 - val_mse: 1.0065\n",
      "Epoch 55/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7465 - mse: 0.7761 - val_loss: 1.9705 - val_mse: 1.0008\n",
      "Epoch 56/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7511 - mse: 0.7819 - val_loss: 1.9642 - val_mse: 0.9974\n",
      "Epoch 57/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7447 - mse: 0.7779 - val_loss: 1.9579 - val_mse: 0.9932\n",
      "Epoch 58/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7363 - mse: 0.7723 - val_loss: 1.9528 - val_mse: 0.9902\n",
      "Epoch 59/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7291 - mse: 0.7664 - val_loss: 1.9472 - val_mse: 0.9858\n",
      "Epoch 60/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7261 - mse: 0.7652 - val_loss: 1.9423 - val_mse: 0.9835\n",
      "Epoch 61/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7192 - mse: 0.7594 - val_loss: 1.9376 - val_mse: 0.9803\n",
      "Epoch 62/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7109 - mse: 0.7534 - val_loss: 1.9336 - val_mse: 0.9767\n",
      "Epoch 63/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7064 - mse: 0.7494 - val_loss: 1.9289 - val_mse: 0.9738\n",
      "Epoch 64/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7075 - mse: 0.7519 - val_loss: 1.9259 - val_mse: 0.9711\n",
      "Epoch 65/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7064 - mse: 0.7518 - val_loss: 1.9220 - val_mse: 0.9681\n",
      "Epoch 66/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7003 - mse: 0.7462 - val_loss: 1.9184 - val_mse: 0.9649\n",
      "Epoch 67/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7001 - mse: 0.7463 - val_loss: 1.9152 - val_mse: 0.9627\n",
      "Epoch 68/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6951 - mse: 0.7420 - val_loss: 1.9125 - val_mse: 0.9601\n",
      "Epoch 69/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6993 - mse: 0.7465 - val_loss: 1.9103 - val_mse: 0.9582\n",
      "Epoch 70/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6870 - mse: 0.7347 - val_loss: 1.9073 - val_mse: 0.9556\n",
      "Epoch 71/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6881 - mse: 0.7358 - val_loss: 1.9048 - val_mse: 0.9534\n",
      "Epoch 72/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6798 - mse: 0.7283 - val_loss: 1.9020 - val_mse: 0.9507\n",
      "Epoch 73/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6765 - mse: 0.7244 - val_loss: 1.9003 - val_mse: 0.9496\n",
      "Epoch 74/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6790 - mse: 0.7276 - val_loss: 1.8994 - val_mse: 0.9481\n",
      "Epoch 75/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6722 - mse: 0.7207 - val_loss: 1.8970 - val_mse: 0.9455\n",
      "Epoch 76/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6629 - mse: 0.7113 - val_loss: 1.8954 - val_mse: 0.9441\n",
      "Epoch 77/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6716 - mse: 0.7200 - val_loss: 1.8940 - val_mse: 0.9425\n",
      "Epoch 78/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6681 - mse: 0.7161 - val_loss: 1.8929 - val_mse: 0.9414\n",
      "Epoch 79/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6686 - mse: 0.7165 - val_loss: 1.8911 - val_mse: 0.9389\n",
      "Epoch 80/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6605 - mse: 0.7086 - val_loss: 1.8905 - val_mse: 0.9381\n",
      "Epoch 81/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6624 - mse: 0.7096 - val_loss: 1.8890 - val_mse: 0.9360\n",
      "Epoch 82/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6614 - mse: 0.7085 - val_loss: 1.8882 - val_mse: 0.9348\n",
      "Epoch 83/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6618 - mse: 0.7080 - val_loss: 1.8873 - val_mse: 0.9340\n",
      "Epoch 84/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6563 - mse: 0.7030 - val_loss: 1.8863 - val_mse: 0.9325\n",
      "Epoch 85/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6643 - mse: 0.7099 - val_loss: 1.8849 - val_mse: 0.9309\n",
      "Epoch 86/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6658 - mse: 0.7124 - val_loss: 1.8846 - val_mse: 0.9297\n",
      "Epoch 87/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6623 - mse: 0.7072 - val_loss: 1.8849 - val_mse: 0.9297\n",
      "Epoch 88/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6463 - mse: 0.6912 - val_loss: 1.8836 - val_mse: 0.9283\n",
      "Epoch 89/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6510 - mse: 0.6953 - val_loss: 1.8835 - val_mse: 0.9273\n",
      "Epoch 90/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6554 - mse: 0.6992 - val_loss: 1.8830 - val_mse: 0.9272\n",
      "Epoch 91/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6520 - mse: 0.6960 - val_loss: 1.8825 - val_mse: 0.9260\n",
      "Epoch 92/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6483 - mse: 0.6914 - val_loss: 1.8813 - val_mse: 0.9246\n",
      "Epoch 93/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6502 - mse: 0.6931 - val_loss: 1.8815 - val_mse: 0.9244\n",
      "Epoch 94/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6482 - mse: 0.6906 - val_loss: 1.8802 - val_mse: 0.9223\n",
      "Epoch 95/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6490 - mse: 0.6911 - val_loss: 1.8807 - val_mse: 0.9224\n",
      "Epoch 96/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6563 - mse: 0.6978 - val_loss: 1.8803 - val_mse: 0.9219\n",
      "Epoch 97/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6489 - mse: 0.6897 - val_loss: 1.8798 - val_mse: 0.9213\n",
      "Epoch 98/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6509 - mse: 0.6921 - val_loss: 1.8796 - val_mse: 0.9204\n",
      "Epoch 99/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6494 - mse: 0.6895 - val_loss: 1.8793 - val_mse: 0.9194\n",
      "Epoch 100/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6519 - mse: 0.6918 - val_loss: 1.8791 - val_mse: 0.9190\n",
      "Epoch 101/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6586 - mse: 0.6978 - val_loss: 1.8789 - val_mse: 0.9185\n",
      "Epoch 102/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6487 - mse: 0.6877 - val_loss: 1.8788 - val_mse: 0.9184\n",
      "Epoch 103/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6415 - mse: 0.6804 - val_loss: 1.8790 - val_mse: 0.9178\n",
      "Epoch 104/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6465 - mse: 0.6848 - val_loss: 1.8785 - val_mse: 0.9170\n",
      "Epoch 105/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6511 - mse: 0.6891 - val_loss: 1.8784 - val_mse: 0.9161\n",
      "Epoch 106/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6456 - mse: 0.6829 - val_loss: 1.8784 - val_mse: 0.9157\n",
      "Epoch 107/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6422 - mse: 0.6790 - val_loss: 1.8778 - val_mse: 0.9153\n",
      "Epoch 108/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6537 - mse: 0.6904 - val_loss: 1.8784 - val_mse: 0.9151\n",
      "Epoch 109/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6460 - mse: 0.6824 - val_loss: 1.8781 - val_mse: 0.9141\n",
      "Epoch 110/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6436 - mse: 0.6790 - val_loss: 1.8780 - val_mse: 0.9137\n",
      "Epoch 111/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6480 - mse: 0.6833 - val_loss: 1.8781 - val_mse: 0.9135\n",
      "Epoch 112/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6462 - mse: 0.6808 - val_loss: 1.8780 - val_mse: 0.9131\n",
      "Epoch 113/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6411 - mse: 0.6759 - val_loss: 1.8784 - val_mse: 0.9131\n",
      "Epoch 114/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6398 - mse: 0.6740 - val_loss: 1.8782 - val_mse: 0.9124\n",
      "Epoch 115/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6376 - mse: 0.6711 - val_loss: 1.8780 - val_mse: 0.9120\n",
      "Epoch 116/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6445 - mse: 0.6780 - val_loss: 1.8781 - val_mse: 0.9120\n",
      "Epoch 117/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6430 - mse: 0.6765 - val_loss: 1.8782 - val_mse: 0.9109\n",
      "Epoch 118/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6400 - mse: 0.6721 - val_loss: 1.8775 - val_mse: 0.9108\n",
      "Epoch 119/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6395 - mse: 0.6725 - val_loss: 1.8780 - val_mse: 0.9104\n",
      "Epoch 120/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6396 - mse: 0.6714 - val_loss: 1.8783 - val_mse: 0.9106\n",
      "Epoch 121/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6416 - mse: 0.6736 - val_loss: 1.8782 - val_mse: 0.9101\n",
      "Epoch 122/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6378 - mse: 0.6694 - val_loss: 1.8783 - val_mse: 0.9098\n",
      "Epoch 123/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6447 - mse: 0.6761 - val_loss: 1.8786 - val_mse: 0.9101\n",
      "Epoch 124/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6360 - mse: 0.6668 - val_loss: 1.8784 - val_mse: 0.9091\n",
      "Epoch 125/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6413 - mse: 0.6716 - val_loss: 1.8784 - val_mse: 0.9088\n",
      "Epoch 126/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6405 - mse: 0.6710 - val_loss: 1.8784 - val_mse: 0.9084\n",
      "Epoch 127/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6413 - mse: 0.6712 - val_loss: 1.8785 - val_mse: 0.9083\n",
      "Epoch 128/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6476 - mse: 0.6771 - val_loss: 1.8787 - val_mse: 0.9085\n",
      "Epoch 129/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6431 - mse: 0.6725 - val_loss: 1.8789 - val_mse: 0.9088\n",
      "Epoch 130/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6323 - mse: 0.6618 - val_loss: 1.8786 - val_mse: 0.9081\n",
      "Epoch 131/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6411 - mse: 0.6698 - val_loss: 1.8789 - val_mse: 0.9080\n",
      "Epoch 132/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6418 - mse: 0.6705 - val_loss: 1.8790 - val_mse: 0.9078\n",
      "Epoch 133/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6405 - mse: 0.6688 - val_loss: 1.8789 - val_mse: 0.9077\n",
      "Epoch 134/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6390 - mse: 0.6671 - val_loss: 1.8791 - val_mse: 0.9071\n",
      "Epoch 135/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6312 - mse: 0.6594 - val_loss: 1.8794 - val_mse: 0.9068\n",
      "Epoch 136/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6391 - mse: 0.6663 - val_loss: 1.8794 - val_mse: 0.9071\n",
      "Epoch 137/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6450 - mse: 0.6717 - val_loss: 1.8796 - val_mse: 0.9071\n",
      "Epoch 138/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6360 - mse: 0.6630 - val_loss: 1.8796 - val_mse: 0.9071\n",
      "Epoch 139/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6404 - mse: 0.6675 - val_loss: 1.8795 - val_mse: 0.9062\n",
      "Epoch 140/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6419 - mse: 0.6686 - val_loss: 1.8794 - val_mse: 0.9062\n",
      "Epoch 141/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6374 - mse: 0.6635 - val_loss: 1.8797 - val_mse: 0.9062\n",
      "Epoch 142/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6464 - mse: 0.6720 - val_loss: 1.8796 - val_mse: 0.9062\n",
      "Epoch 143/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6423 - mse: 0.6683 - val_loss: 1.8801 - val_mse: 0.9062\n",
      "Epoch 144/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6388 - mse: 0.6644 - val_loss: 1.8803 - val_mse: 0.9061\n",
      "Epoch 145/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6416 - mse: 0.6669 - val_loss: 1.8802 - val_mse: 0.9056\n",
      "Epoch 146/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6358 - mse: 0.6611 - val_loss: 1.8803 - val_mse: 0.9055\n",
      "Epoch 147/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6402 - mse: 0.6650 - val_loss: 1.8804 - val_mse: 0.9058\n",
      "Epoch 148/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6394 - mse: 0.6642 - val_loss: 1.8807 - val_mse: 0.9055\n",
      "Epoch 149/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6423 - mse: 0.6670 - val_loss: 1.8807 - val_mse: 0.9051\n",
      "Epoch 150/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6369 - mse: 0.6610 - val_loss: 1.8810 - val_mse: 0.9051\n",
      "Epoch 151/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6386 - mse: 0.6622 - val_loss: 1.8812 - val_mse: 0.9051\n",
      "Epoch 152/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6411 - mse: 0.6648 - val_loss: 1.8813 - val_mse: 0.9051\n",
      "Epoch 153/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6476 - mse: 0.6711 - val_loss: 1.8810 - val_mse: 0.9048\n",
      "Epoch 154/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6373 - mse: 0.6611 - val_loss: 1.8813 - val_mse: 0.9051\n",
      "Epoch 155/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6437 - mse: 0.6667 - val_loss: 1.8817 - val_mse: 0.9056\n",
      "Epoch 156/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6382 - mse: 0.6615 - val_loss: 1.8813 - val_mse: 0.9049\n",
      "Epoch 157/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6444 - mse: 0.6675 - val_loss: 1.8817 - val_mse: 0.9049\n",
      "Epoch 158/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6421 - mse: 0.6649 - val_loss: 1.8819 - val_mse: 0.9052\n",
      "Epoch 158: early stopping\n",
      "Restoring model weights from the end of the best epoch: 153.\n",
      "\u001b[1m2836/2836\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "best valid RMSE : 0.9512042092970489,\t\t\t params = {'k': 30, 'lambda_': 0.0002}\n",
      "Epoch 1/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 13.0506 - mse: 13.0434 - val_loss: 12.0350 - val_mse: 12.0282\n",
      "Epoch 2/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 11.2353 - mse: 11.2208 - val_loss: 7.7506 - val_mse: 7.6843\n",
      "Epoch 3/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 6.2507 - mse: 6.1528 - val_loss: 3.5464 - val_mse: 3.3525\n",
      "Epoch 4/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.9226 - mse: 2.7032 - val_loss: 2.5044 - val_mse: 2.2225\n",
      "Epoch 5/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.0752 - mse: 1.7779 - val_loss: 2.1260 - val_mse: 1.7877\n",
      "Epoch 6/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7442 - mse: 1.3948 - val_loss: 1.9350 - val_mse: 1.5551\n",
      "Epoch 7/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.5567 - mse: 1.1682 - val_loss: 1.8284 - val_mse: 1.4168\n",
      "Epoch 8/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.4583 - mse: 1.0401 - val_loss: 1.7626 - val_mse: 1.3262\n",
      "Epoch 9/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.4023 - mse: 0.9607 - val_loss: 1.7201 - val_mse: 1.2642\n",
      "Epoch 10/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3561 - mse: 0.8961 - val_loss: 1.6912 - val_mse: 1.2199\n",
      "Epoch 11/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.3137 - mse: 0.8393 - val_loss: 1.6690 - val_mse: 1.1858\n",
      "Epoch 12/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2959 - mse: 0.8102 - val_loss: 1.6530 - val_mse: 1.1606\n",
      "Epoch 13/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2764 - mse: 0.7821 - val_loss: 1.6406 - val_mse: 1.1412\n",
      "Epoch 14/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2581 - mse: 0.7574 - val_loss: 1.6296 - val_mse: 1.1247\n",
      "Epoch 15/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2446 - mse: 0.7386 - val_loss: 1.6218 - val_mse: 1.1132\n",
      "Epoch 16/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2294 - mse: 0.7201 - val_loss: 1.6147 - val_mse: 1.1032\n",
      "Epoch 17/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2212 - mse: 0.7094 - val_loss: 1.6071 - val_mse: 1.0936\n",
      "Epoch 18/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2089 - mse: 0.6953 - val_loss: 1.6004 - val_mse: 1.0859\n",
      "Epoch 19/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1973 - mse: 0.6829 - val_loss: 1.5947 - val_mse: 1.0798\n",
      "Epoch 20/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1864 - mse: 0.6718 - val_loss: 1.5882 - val_mse: 1.0731\n",
      "Epoch 21/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1692 - mse: 0.6546 - val_loss: 1.5818 - val_mse: 1.0675\n",
      "Epoch 22/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1619 - mse: 0.6476 - val_loss: 1.5772 - val_mse: 1.0636\n",
      "Epoch 23/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1505 - mse: 0.6372 - val_loss: 1.5710 - val_mse: 1.0585\n",
      "Epoch 24/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1381 - mse: 0.6260 - val_loss: 1.5641 - val_mse: 1.0524\n",
      "Epoch 25/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1323 - mse: 0.6211 - val_loss: 1.5582 - val_mse: 1.0477\n",
      "Epoch 26/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1236 - mse: 0.6134 - val_loss: 1.5543 - val_mse: 1.0452\n",
      "Epoch 27/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1060 - mse: 0.5973 - val_loss: 1.5485 - val_mse: 1.0406\n",
      "Epoch 28/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1029 - mse: 0.5955 - val_loss: 1.5430 - val_mse: 1.0360\n",
      "Epoch 29/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0850 - mse: 0.5787 - val_loss: 1.5389 - val_mse: 1.0330\n",
      "Epoch 30/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0761 - mse: 0.5705 - val_loss: 1.5350 - val_mse: 1.0302\n",
      "Epoch 31/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0578 - mse: 0.5533 - val_loss: 1.5302 - val_mse: 1.0260\n",
      "Epoch 32/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0513 - mse: 0.5474 - val_loss: 1.5270 - val_mse: 1.0239\n",
      "Epoch 33/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0372 - mse: 0.5344 - val_loss: 1.5226 - val_mse: 1.0202\n",
      "Epoch 34/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0188 - mse: 0.5167 - val_loss: 1.5191 - val_mse: 1.0169\n",
      "Epoch 35/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0173 - mse: 0.5157 - val_loss: 1.5154 - val_mse: 1.0138\n",
      "Epoch 36/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.9985 - mse: 0.4972 - val_loss: 1.5127 - val_mse: 1.0116\n",
      "Epoch 37/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.9857 - mse: 0.4849 - val_loss: 1.5102 - val_mse: 1.0093\n",
      "Epoch 38/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.9719 - mse: 0.4713 - val_loss: 1.5067 - val_mse: 1.0061\n",
      "Epoch 39/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.9648 - mse: 0.4645 - val_loss: 1.5058 - val_mse: 1.0054\n",
      "Epoch 40/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.9512 - mse: 0.4510 - val_loss: 1.5018 - val_mse: 1.0012\n",
      "Epoch 41/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.9485 - mse: 0.4483 - val_loss: 1.4990 - val_mse: 0.9987\n",
      "Epoch 42/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.9355 - mse: 0.4354 - val_loss: 1.4995 - val_mse: 0.9994\n",
      "Epoch 43/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.9271 - mse: 0.4271 - val_loss: 1.4978 - val_mse: 0.9974\n",
      "Epoch 44/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.9191 - mse: 0.4190 - val_loss: 1.4931 - val_mse: 0.9926\n",
      "Epoch 45/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.9063 - mse: 0.4060 - val_loss: 1.4929 - val_mse: 0.9922\n",
      "Epoch 46/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8975 - mse: 0.3970 - val_loss: 1.4918 - val_mse: 0.9912\n",
      "Epoch 47/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.8939 - mse: 0.3934 - val_loss: 1.4911 - val_mse: 0.9901\n",
      "Epoch 48/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.8800 - mse: 0.3794 - val_loss: 1.4903 - val_mse: 0.9891\n",
      "Epoch 49/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.8696 - mse: 0.3686 - val_loss: 1.4881 - val_mse: 0.9866\n",
      "Epoch 50/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8653 - mse: 0.3639 - val_loss: 1.4870 - val_mse: 0.9852\n",
      "Epoch 51/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.8599 - mse: 0.3582 - val_loss: 1.4848 - val_mse: 0.9826\n",
      "Epoch 52/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.8514 - mse: 0.3497 - val_loss: 1.4839 - val_mse: 0.9815\n",
      "Epoch 53/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.8430 - mse: 0.3410 - val_loss: 1.4825 - val_mse: 0.9796\n",
      "Epoch 54/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8376 - mse: 0.3351 - val_loss: 1.4819 - val_mse: 0.9792\n",
      "Epoch 55/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.8262 - mse: 0.3234 - val_loss: 1.4792 - val_mse: 0.9760\n",
      "Epoch 56/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8256 - mse: 0.3227 - val_loss: 1.4804 - val_mse: 0.9769\n",
      "Epoch 57/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8200 - mse: 0.3168 - val_loss: 1.4785 - val_mse: 0.9752\n",
      "Epoch 58/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.8135 - mse: 0.3103 - val_loss: 1.4773 - val_mse: 0.9736\n",
      "Epoch 59/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8093 - mse: 0.3059 - val_loss: 1.4774 - val_mse: 0.9734\n",
      "Epoch 60/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7968 - mse: 0.2929 - val_loss: 1.4762 - val_mse: 0.9721\n",
      "Epoch 61/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7935 - mse: 0.2897 - val_loss: 1.4749 - val_mse: 0.9705\n",
      "Epoch 62/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7901 - mse: 0.2862 - val_loss: 1.4729 - val_mse: 0.9681\n",
      "Epoch 63/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7839 - mse: 0.2793 - val_loss: 1.4733 - val_mse: 0.9686\n",
      "Epoch 64/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7818 - mse: 0.2775 - val_loss: 1.4724 - val_mse: 0.9675\n",
      "Epoch 65/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7736 - mse: 0.2688 - val_loss: 1.4708 - val_mse: 0.9658\n",
      "Epoch 66/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7706 - mse: 0.2661 - val_loss: 1.4698 - val_mse: 0.9646\n",
      "Epoch 67/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7659 - mse: 0.2610 - val_loss: 1.4681 - val_mse: 0.9630\n",
      "Epoch 68/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7616 - mse: 0.2566 - val_loss: 1.4668 - val_mse: 0.9617\n",
      "Epoch 69/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7573 - mse: 0.2524 - val_loss: 1.4672 - val_mse: 0.9620\n",
      "Epoch 70/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7529 - mse: 0.2481 - val_loss: 1.4659 - val_mse: 0.9605\n",
      "Epoch 71/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7490 - mse: 0.2439 - val_loss: 1.4654 - val_mse: 0.9602\n",
      "Epoch 72/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7431 - mse: 0.2382 - val_loss: 1.4647 - val_mse: 0.9595\n",
      "Epoch 73/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7420 - mse: 0.2373 - val_loss: 1.4637 - val_mse: 0.9583\n",
      "Epoch 74/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7391 - mse: 0.2341 - val_loss: 1.4625 - val_mse: 0.9574\n",
      "Epoch 75/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7375 - mse: 0.2328 - val_loss: 1.4611 - val_mse: 0.9557\n",
      "Epoch 76/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7302 - mse: 0.2254 - val_loss: 1.4610 - val_mse: 0.9560\n",
      "Epoch 77/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7272 - mse: 0.2225 - val_loss: 1.4607 - val_mse: 0.9560\n",
      "Epoch 78/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7263 - mse: 0.2219 - val_loss: 1.4603 - val_mse: 0.9558\n",
      "Epoch 79/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7226 - mse: 0.2184 - val_loss: 1.4579 - val_mse: 0.9535\n",
      "Epoch 80/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7208 - mse: 0.2167 - val_loss: 1.4577 - val_mse: 0.9535\n",
      "Epoch 81/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7186 - mse: 0.2148 - val_loss: 1.4569 - val_mse: 0.9527\n",
      "Epoch 82/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7156 - mse: 0.2118 - val_loss: 1.4566 - val_mse: 0.9528\n",
      "Epoch 83/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7145 - mse: 0.2112 - val_loss: 1.4546 - val_mse: 0.9508\n",
      "Epoch 84/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7123 - mse: 0.2089 - val_loss: 1.4538 - val_mse: 0.9503\n",
      "Epoch 85/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7070 - mse: 0.2038 - val_loss: 1.4524 - val_mse: 0.9492\n",
      "Epoch 86/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7064 - mse: 0.2036 - val_loss: 1.4516 - val_mse: 0.9486\n",
      "Epoch 87/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7033 - mse: 0.2006 - val_loss: 1.4502 - val_mse: 0.9472\n",
      "Epoch 88/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7019 - mse: 0.1995 - val_loss: 1.4508 - val_mse: 0.9484\n",
      "Epoch 89/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7024 - mse: 0.2002 - val_loss: 1.4492 - val_mse: 0.9471\n",
      "Epoch 90/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6955 - mse: 0.1939 - val_loss: 1.4485 - val_mse: 0.9467\n",
      "Epoch 91/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6943 - mse: 0.1929 - val_loss: 1.4480 - val_mse: 0.9465\n",
      "Epoch 92/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.6923 - mse: 0.1913 - val_loss: 1.4467 - val_mse: 0.9454\n",
      "Epoch 93/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.6917 - mse: 0.1908 - val_loss: 1.4459 - val_mse: 0.9448\n",
      "Epoch 94/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.6890 - mse: 0.1886 - val_loss: 1.4438 - val_mse: 0.9431\n",
      "Epoch 95/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.6895 - mse: 0.1891 - val_loss: 1.4444 - val_mse: 0.9439\n",
      "Epoch 96/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6869 - mse: 0.1868 - val_loss: 1.4442 - val_mse: 0.9443\n",
      "Epoch 97/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6836 - mse: 0.1840 - val_loss: 1.4426 - val_mse: 0.9428\n",
      "Epoch 98/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6838 - mse: 0.1845 - val_loss: 1.4411 - val_mse: 0.9416\n",
      "Epoch 99/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6820 - mse: 0.1831 - val_loss: 1.4403 - val_mse: 0.9413\n",
      "Epoch 100/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.6802 - mse: 0.1816 - val_loss: 1.4391 - val_mse: 0.9403\n",
      "Epoch 101/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6778 - mse: 0.1795 - val_loss: 1.4385 - val_mse: 0.9403\n",
      "Epoch 102/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6772 - mse: 0.1792 - val_loss: 1.4381 - val_mse: 0.9400\n",
      "Epoch 103/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6735 - mse: 0.1761 - val_loss: 1.4368 - val_mse: 0.9391\n",
      "Epoch 104/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.6733 - mse: 0.1760 - val_loss: 1.4353 - val_mse: 0.9379\n",
      "Epoch 105/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6718 - mse: 0.1748 - val_loss: 1.4347 - val_mse: 0.9377\n",
      "Epoch 106/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6723 - mse: 0.1755 - val_loss: 1.4333 - val_mse: 0.9366\n",
      "Epoch 107/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6708 - mse: 0.1745 - val_loss: 1.4332 - val_mse: 0.9368\n",
      "Epoch 108/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6701 - mse: 0.1741 - val_loss: 1.4318 - val_mse: 0.9356\n",
      "Epoch 109/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6677 - mse: 0.1719 - val_loss: 1.4306 - val_mse: 0.9347\n",
      "Epoch 110/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6678 - mse: 0.1724 - val_loss: 1.4306 - val_mse: 0.9350\n",
      "Epoch 111/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6681 - mse: 0.1730 - val_loss: 1.4293 - val_mse: 0.9341\n",
      "Epoch 112/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6660 - mse: 0.1713 - val_loss: 1.4285 - val_mse: 0.9335\n",
      "Epoch 113/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6655 - mse: 0.1709 - val_loss: 1.4281 - val_mse: 0.9334\n",
      "Epoch 114/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6606 - mse: 0.1665 - val_loss: 1.4273 - val_mse: 0.9330\n",
      "Epoch 115/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6631 - mse: 0.1693 - val_loss: 1.4266 - val_mse: 0.9325\n",
      "Epoch 116/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6599 - mse: 0.1664 - val_loss: 1.4254 - val_mse: 0.9316\n",
      "Epoch 117/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6616 - mse: 0.1682 - val_loss: 1.4250 - val_mse: 0.9315\n",
      "Epoch 118/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6580 - mse: 0.1649 - val_loss: 1.4247 - val_mse: 0.9316\n",
      "Epoch 119/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6584 - mse: 0.1657 - val_loss: 1.4238 - val_mse: 0.9310\n",
      "Epoch 120/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6575 - mse: 0.1651 - val_loss: 1.4228 - val_mse: 0.9301\n",
      "Epoch 121/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6569 - mse: 0.1647 - val_loss: 1.4219 - val_mse: 0.9296\n",
      "Epoch 122/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6556 - mse: 0.1637 - val_loss: 1.4209 - val_mse: 0.9290\n",
      "Epoch 123/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6549 - mse: 0.1635 - val_loss: 1.4205 - val_mse: 0.9286\n",
      "Epoch 124/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6530 - mse: 0.1615 - val_loss: 1.4200 - val_mse: 0.9285\n",
      "Epoch 125/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6539 - mse: 0.1628 - val_loss: 1.4193 - val_mse: 0.9280\n",
      "Epoch 126/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6528 - mse: 0.1620 - val_loss: 1.4186 - val_mse: 0.9276\n",
      "Epoch 127/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6523 - mse: 0.1616 - val_loss: 1.4184 - val_mse: 0.9276\n",
      "Epoch 128/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6509 - mse: 0.1604 - val_loss: 1.4175 - val_mse: 0.9269\n",
      "Epoch 129/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6506 - mse: 0.1604 - val_loss: 1.4173 - val_mse: 0.9271\n",
      "Epoch 130/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6503 - mse: 0.1605 - val_loss: 1.4170 - val_mse: 0.9269\n",
      "Epoch 131/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6489 - mse: 0.1592 - val_loss: 1.4157 - val_mse: 0.9259\n",
      "Epoch 132/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6474 - mse: 0.1579 - val_loss: 1.4149 - val_mse: 0.9252\n",
      "Epoch 133/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6475 - mse: 0.1582 - val_loss: 1.4149 - val_mse: 0.9254\n",
      "Epoch 134/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6463 - mse: 0.1571 - val_loss: 1.4150 - val_mse: 0.9257\n",
      "Epoch 135/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6490 - mse: 0.1601 - val_loss: 1.4137 - val_mse: 0.9244\n",
      "Epoch 136/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6448 - mse: 0.1561 - val_loss: 1.4138 - val_mse: 0.9249\n",
      "Epoch 137/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6457 - mse: 0.1571 - val_loss: 1.4128 - val_mse: 0.9240\n",
      "Epoch 138/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6450 - mse: 0.1566 - val_loss: 1.4125 - val_mse: 0.9239\n",
      "Epoch 139/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6439 - mse: 0.1558 - val_loss: 1.4127 - val_mse: 0.9245\n",
      "Epoch 140/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6444 - mse: 0.1564 - val_loss: 1.4116 - val_mse: 0.9234\n",
      "Epoch 141/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6429 - mse: 0.1552 - val_loss: 1.4118 - val_mse: 0.9237\n",
      "Epoch 142/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6438 - mse: 0.1561 - val_loss: 1.4115 - val_mse: 0.9235\n",
      "Epoch 143/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6431 - mse: 0.1555 - val_loss: 1.4110 - val_mse: 0.9233\n",
      "Epoch 144/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6400 - mse: 0.1528 - val_loss: 1.4103 - val_mse: 0.9227\n",
      "Epoch 145/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6399 - mse: 0.1529 - val_loss: 1.4099 - val_mse: 0.9225\n",
      "Epoch 146/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6393 - mse: 0.1522 - val_loss: 1.4095 - val_mse: 0.9224\n",
      "Epoch 147/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6404 - mse: 0.1534 - val_loss: 1.4098 - val_mse: 0.9227\n",
      "Epoch 148/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6414 - mse: 0.1546 - val_loss: 1.4092 - val_mse: 0.9221\n",
      "Epoch 149/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6385 - mse: 0.1519 - val_loss: 1.4091 - val_mse: 0.9224\n",
      "Epoch 150/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6373 - mse: 0.1510 - val_loss: 1.4083 - val_mse: 0.9216\n",
      "Epoch 151/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6377 - mse: 0.1513 - val_loss: 1.4087 - val_mse: 0.9221\n",
      "Epoch 152/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6370 - mse: 0.1507 - val_loss: 1.4084 - val_mse: 0.9221\n",
      "Epoch 153/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6370 - mse: 0.1511 - val_loss: 1.4079 - val_mse: 0.9217\n",
      "Epoch 154/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6361 - mse: 0.1502 - val_loss: 1.4083 - val_mse: 0.9222\n",
      "Epoch 155/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6378 - mse: 0.1520 - val_loss: 1.4080 - val_mse: 0.9219\n",
      "Epoch 155: early stopping\n",
      "Restoring model weights from the end of the best epoch: 150.\n",
      "\u001b[1m2836/2836\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "best valid RMSE : 0.9599903990841631,\t\t\t params = {'k': 30, 'lambda_': 5e-05}\n",
      "Epoch 1/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 13.0470 - mse: 13.0436 - val_loss: 12.0453 - val_mse: 12.0421\n",
      "Epoch 2/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 11.3030 - mse: 11.2967 - val_loss: 7.8269 - val_mse: 7.8003\n",
      "Epoch 3/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6.2778 - mse: 6.2383 - val_loss: 3.4241 - val_mse: 3.3449\n",
      "Epoch 4/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.7652 - mse: 2.6752 - val_loss: 2.2982 - val_mse: 2.1813\n",
      "Epoch 5/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8564 - mse: 1.7326 - val_loss: 1.8803 - val_mse: 1.7382\n",
      "Epoch 6/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.4810 - mse: 1.3338 - val_loss: 1.6674 - val_mse: 1.5060\n",
      "Epoch 7/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2829 - mse: 1.1174 - val_loss: 1.5432 - val_mse: 1.3663\n",
      "Epoch 8/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1584 - mse: 0.9782 - val_loss: 1.4663 - val_mse: 1.2768\n",
      "Epoch 9/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0794 - mse: 0.8872 - val_loss: 1.4167 - val_mse: 1.2168\n",
      "Epoch 10/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0354 - mse: 0.8332 - val_loss: 1.3849 - val_mse: 1.1761\n",
      "Epoch 11/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.9782 - mse: 0.7675 - val_loss: 1.3599 - val_mse: 1.1437\n",
      "Epoch 12/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.9584 - mse: 0.7406 - val_loss: 1.3437 - val_mse: 1.1213\n",
      "Epoch 13/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.9309 - mse: 0.7071 - val_loss: 1.3332 - val_mse: 1.1055\n",
      "Epoch 14/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.9128 - mse: 0.6839 - val_loss: 1.3254 - val_mse: 1.0934\n",
      "Epoch 15/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.9014 - mse: 0.6684 - val_loss: 1.3185 - val_mse: 1.0828\n",
      "Epoch 16/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.8851 - mse: 0.6485 - val_loss: 1.3157 - val_mse: 1.0770\n",
      "Epoch 17/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.8661 - mse: 0.6267 - val_loss: 1.3112 - val_mse: 1.0700\n",
      "Epoch 18/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.8599 - mse: 0.6182 - val_loss: 1.3096 - val_mse: 1.0663\n",
      "Epoch 19/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8481 - mse: 0.6044 - val_loss: 1.3085 - val_mse: 1.0635\n",
      "Epoch 20/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.8372 - mse: 0.5919 - val_loss: 1.3074 - val_mse: 1.0610\n",
      "Epoch 21/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8334 - mse: 0.5867 - val_loss: 1.3073 - val_mse: 1.0600\n",
      "Epoch 22/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8120 - mse: 0.5645 - val_loss: 1.3048 - val_mse: 1.0566\n",
      "Epoch 23/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8072 - mse: 0.5589 - val_loss: 1.3041 - val_mse: 1.0552\n",
      "Epoch 24/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7979 - mse: 0.5488 - val_loss: 1.3053 - val_mse: 1.0559\n",
      "Epoch 25/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7926 - mse: 0.5431 - val_loss: 1.3059 - val_mse: 1.0560\n",
      "Epoch 26/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.7729 - mse: 0.5229 - val_loss: 1.3050 - val_mse: 1.0547\n",
      "Epoch 27/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7646 - mse: 0.5144 - val_loss: 1.3050 - val_mse: 1.0544\n",
      "Epoch 28/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7569 - mse: 0.5063 - val_loss: 1.3045 - val_mse: 1.0535\n",
      "Epoch 29/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7427 - mse: 0.4918 - val_loss: 1.3063 - val_mse: 1.0551\n",
      "Epoch 30/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7328 - mse: 0.4816 - val_loss: 1.3034 - val_mse: 1.0520\n",
      "Epoch 31/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7145 - mse: 0.4631 - val_loss: 1.3062 - val_mse: 1.0545\n",
      "Epoch 32/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7030 - mse: 0.4514 - val_loss: 1.3048 - val_mse: 1.0529\n",
      "Epoch 33/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6925 - mse: 0.4406 - val_loss: 1.3084 - val_mse: 1.0563\n",
      "Epoch 34/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.6789 - mse: 0.4268 - val_loss: 1.3068 - val_mse: 1.0544\n",
      "Epoch 35/500\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6737 - mse: 0.4214 - val_loss: 1.3091 - val_mse: 1.0564\n",
      "Epoch 35: early stopping\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "\u001b[1m2836/2836\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "best valid RMSE : 1.0256556577127833,\t\t\t params = {'k': 30, 'lambda_': 2e-05}\n"
     ]
    }
   ],
   "source": [
    "lambdas_ = [0.0002, 0.00005, 0.00002]\n",
    "ks = [15,30]\n",
    "\n",
    "param_grid = { 'k' : ks, 'lambda_' : lambdas_ }\n",
    "\n",
    "best_params, best_score, best_model = grid_search(train, param_grid, get_mf_bias_l2_reg_model,\n",
    "                                      nb_users, nb_movies, validation_size = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters : {'k': 15, 'lambda_': 5e-05}\n",
      "Best validation RMSE : 0.9374045005477063\n"
     ]
    }
   ],
   "source": [
    "print('Best hyper-parameters : ' + str(best_params))\n",
    "print('Best validation RMSE : ' + str(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m316/316\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977us/step\n",
      "Best model test RMSE : 0.9475657853049863 \n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"Best model test RMSE : %s \" % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain on all the dataset with the best hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually other hyper-parameters such as the ones of SGD should also be grid-searched, like the number of epochs or the batch size. But that would be a bit long for this course. \n",
    "\n",
    "Now we want to do the best prediction possible, so retrain below your model on the whole dataset, including the test set, with the best values obtained from your grid search to make new predictions with our optimal parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/ml4/lib/python3.12/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['u__user_id', 'i__movie_id']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 12.9618 - mse: 12.9548 - val_loss: 11.9221 - val_mse: 11.9150\n",
      "Epoch 2/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 10.9072 - mse: 10.8900 - val_loss: 6.8405 - val_mse: 6.7568\n",
      "Epoch 3/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5.3078 - mse: 5.1880 - val_loss: 3.1237 - val_mse: 2.9029\n",
      "Epoch 4/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.6130 - mse: 2.3684 - val_loss: 2.3258 - val_mse: 2.0218\n",
      "Epoch 5/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.9224 - mse: 1.6037 - val_loss: 2.0222 - val_mse: 1.6635\n",
      "Epoch 6/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6541 - mse: 1.2847 - val_loss: 1.8719 - val_mse: 1.4729\n",
      "Epoch 7/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.5004 - mse: 1.0934 - val_loss: 1.7870 - val_mse: 1.3574\n",
      "Epoch 8/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.4282 - mse: 0.9926 - val_loss: 1.7352 - val_mse: 1.2823\n",
      "Epoch 9/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3735 - mse: 0.9156 - val_loss: 1.7029 - val_mse: 1.2317\n",
      "Epoch 10/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.3270 - mse: 0.8519 - val_loss: 1.6796 - val_mse: 1.1939\n",
      "Epoch 11/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2983 - mse: 0.8097 - val_loss: 1.6632 - val_mse: 1.1668\n",
      "Epoch 12/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2676 - mse: 0.7689 - val_loss: 1.6502 - val_mse: 1.1449\n",
      "Epoch 13/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2435 - mse: 0.7367 - val_loss: 1.6396 - val_mse: 1.1282\n",
      "Epoch 14/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2354 - mse: 0.7227 - val_loss: 1.6322 - val_mse: 1.1159\n",
      "Epoch 15/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2118 - mse: 0.6947 - val_loss: 1.6243 - val_mse: 1.1050\n",
      "Epoch 16/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2036 - mse: 0.6837 - val_loss: 1.6166 - val_mse: 1.0946\n",
      "Epoch 17/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1899 - mse: 0.6676 - val_loss: 1.6096 - val_mse: 1.0865\n",
      "Epoch 18/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1853 - mse: 0.6617 - val_loss: 1.6045 - val_mse: 1.0810\n",
      "Epoch 19/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1676 - mse: 0.6440 - val_loss: 1.5982 - val_mse: 1.0745\n",
      "Epoch 20/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1568 - mse: 0.6330 - val_loss: 1.5934 - val_mse: 1.0702\n",
      "Epoch 21/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1438 - mse: 0.6209 - val_loss: 1.5868 - val_mse: 1.0643\n",
      "Epoch 22/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1410 - mse: 0.6190 - val_loss: 1.5820 - val_mse: 1.0604\n",
      "Epoch 23/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1339 - mse: 0.6130 - val_loss: 1.5752 - val_mse: 1.0547\n",
      "Epoch 24/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1170 - mse: 0.5971 - val_loss: 1.5708 - val_mse: 1.0522\n",
      "Epoch 25/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1053 - mse: 0.5871 - val_loss: 1.5632 - val_mse: 1.0462\n",
      "Epoch 26/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0925 - mse: 0.5758 - val_loss: 1.5570 - val_mse: 1.0415\n",
      "Epoch 27/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0800 - mse: 0.5652 - val_loss: 1.5529 - val_mse: 1.0390\n",
      "Epoch 28/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0734 - mse: 0.5603 - val_loss: 1.5492 - val_mse: 1.0367\n",
      "Epoch 29/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0654 - mse: 0.5535 - val_loss: 1.5430 - val_mse: 1.0320\n",
      "Epoch 30/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0531 - mse: 0.5427 - val_loss: 1.5364 - val_mse: 1.0269\n",
      "Epoch 31/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0371 - mse: 0.5284 - val_loss: 1.5326 - val_mse: 1.0249\n",
      "Epoch 32/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0262 - mse: 0.5189 - val_loss: 1.5270 - val_mse: 1.0203\n",
      "Epoch 33/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0156 - mse: 0.5094 - val_loss: 1.5215 - val_mse: 1.0161\n",
      "Epoch 34/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0058 - mse: 0.5010 - val_loss: 1.5158 - val_mse: 1.0115\n",
      "Epoch 35/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.9990 - mse: 0.4952 - val_loss: 1.5117 - val_mse: 1.0085\n",
      "Epoch 36/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9852 - mse: 0.4824 - val_loss: 1.5073 - val_mse: 1.0049\n",
      "Epoch 37/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9790 - mse: 0.4771 - val_loss: 1.5033 - val_mse: 1.0012\n",
      "Epoch 38/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9649 - mse: 0.4632 - val_loss: 1.4996 - val_mse: 0.9987\n",
      "Epoch 39/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9511 - mse: 0.4506 - val_loss: 1.4946 - val_mse: 0.9940\n",
      "Epoch 40/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9508 - mse: 0.4508 - val_loss: 1.4916 - val_mse: 0.9913\n",
      "Epoch 41/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9352 - mse: 0.4354 - val_loss: 1.4874 - val_mse: 0.9875\n",
      "Epoch 42/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9261 - mse: 0.4267 - val_loss: 1.4841 - val_mse: 0.9846\n",
      "Epoch 43/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9203 - mse: 0.4213 - val_loss: 1.4829 - val_mse: 0.9839\n",
      "Epoch 44/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9107 - mse: 0.4120 - val_loss: 1.4779 - val_mse: 0.9789\n",
      "Epoch 45/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8974 - mse: 0.3988 - val_loss: 1.4760 - val_mse: 0.9771\n",
      "Epoch 46/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8936 - mse: 0.3950 - val_loss: 1.4719 - val_mse: 0.9731\n",
      "Epoch 47/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8838 - mse: 0.3854 - val_loss: 1.4699 - val_mse: 0.9711\n",
      "Epoch 48/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8755 - mse: 0.3771 - val_loss: 1.4675 - val_mse: 0.9684\n",
      "Epoch 49/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8691 - mse: 0.3705 - val_loss: 1.4651 - val_mse: 0.9660\n",
      "Epoch 50/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8594 - mse: 0.3606 - val_loss: 1.4619 - val_mse: 0.9628\n",
      "Epoch 51/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8540 - mse: 0.3551 - val_loss: 1.4594 - val_mse: 0.9601\n",
      "Epoch 52/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8435 - mse: 0.3445 - val_loss: 1.4573 - val_mse: 0.9579\n",
      "Epoch 53/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8420 - mse: 0.3431 - val_loss: 1.4566 - val_mse: 0.9568\n",
      "Epoch 54/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8375 - mse: 0.3380 - val_loss: 1.4545 - val_mse: 0.9546\n",
      "Epoch 55/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8230 - mse: 0.3237 - val_loss: 1.4525 - val_mse: 0.9523\n",
      "Epoch 56/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8158 - mse: 0.3159 - val_loss: 1.4510 - val_mse: 0.9508\n",
      "Epoch 57/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8113 - mse: 0.3113 - val_loss: 1.4499 - val_mse: 0.9493\n",
      "Epoch 58/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8101 - mse: 0.3100 - val_loss: 1.4473 - val_mse: 0.9468\n",
      "Epoch 59/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8011 - mse: 0.3010 - val_loss: 1.4457 - val_mse: 0.9448\n",
      "Epoch 60/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7970 - mse: 0.2965 - val_loss: 1.4445 - val_mse: 0.9438\n",
      "Epoch 61/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7891 - mse: 0.2886 - val_loss: 1.4434 - val_mse: 0.9425\n",
      "Epoch 62/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7888 - mse: 0.2881 - val_loss: 1.4427 - val_mse: 0.9417\n",
      "Epoch 63/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7833 - mse: 0.2828 - val_loss: 1.4410 - val_mse: 0.9398\n",
      "Epoch 64/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7794 - mse: 0.2786 - val_loss: 1.4385 - val_mse: 0.9372\n",
      "Epoch 65/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7726 - mse: 0.2718 - val_loss: 1.4382 - val_mse: 0.9369\n",
      "Epoch 66/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7715 - mse: 0.2703 - val_loss: 1.4369 - val_mse: 0.9357\n",
      "Epoch 67/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7670 - mse: 0.2664 - val_loss: 1.4353 - val_mse: 0.9339\n",
      "Epoch 68/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7626 - mse: 0.2616 - val_loss: 1.4346 - val_mse: 0.9332\n",
      "Epoch 69/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7573 - mse: 0.2564 - val_loss: 1.4334 - val_mse: 0.9321\n",
      "Epoch 70/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7554 - mse: 0.2545 - val_loss: 1.4321 - val_mse: 0.9307\n",
      "Epoch 71/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7500 - mse: 0.2491 - val_loss: 1.4309 - val_mse: 0.9295\n",
      "Epoch 72/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7484 - mse: 0.2475 - val_loss: 1.4297 - val_mse: 0.9285\n",
      "Epoch 73/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7471 - mse: 0.2465 - val_loss: 1.4294 - val_mse: 0.9280\n",
      "Epoch 74/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7427 - mse: 0.2418 - val_loss: 1.4274 - val_mse: 0.9265\n",
      "Epoch 75/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7386 - mse: 0.2381 - val_loss: 1.4262 - val_mse: 0.9252\n",
      "Epoch 76/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7400 - mse: 0.2394 - val_loss: 1.4243 - val_mse: 0.9236\n",
      "Epoch 77/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7331 - mse: 0.2327 - val_loss: 1.4243 - val_mse: 0.9236\n",
      "Epoch 78/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7335 - mse: 0.2334 - val_loss: 1.4232 - val_mse: 0.9229\n",
      "Epoch 79/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7292 - mse: 0.2292 - val_loss: 1.4219 - val_mse: 0.9218\n",
      "Epoch 80/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7280 - mse: 0.2283 - val_loss: 1.4211 - val_mse: 0.9209\n",
      "Epoch 81/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7240 - mse: 0.2244 - val_loss: 1.4207 - val_mse: 0.9206\n",
      "Epoch 82/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7195 - mse: 0.2200 - val_loss: 1.4191 - val_mse: 0.9193\n",
      "Epoch 83/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7172 - mse: 0.2178 - val_loss: 1.4182 - val_mse: 0.9188\n",
      "Epoch 84/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7174 - mse: 0.2184 - val_loss: 1.4175 - val_mse: 0.9181\n",
      "Epoch 85/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7135 - mse: 0.2148 - val_loss: 1.4165 - val_mse: 0.9173\n",
      "Epoch 86/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7114 - mse: 0.2127 - val_loss: 1.4154 - val_mse: 0.9166\n",
      "Epoch 87/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7108 - mse: 0.2125 - val_loss: 1.4142 - val_mse: 0.9156\n",
      "Epoch 88/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7084 - mse: 0.2106 - val_loss: 1.4135 - val_mse: 0.9150\n",
      "Epoch 89/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7072 - mse: 0.2090 - val_loss: 1.4123 - val_mse: 0.9143\n",
      "Epoch 90/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7054 - mse: 0.2079 - val_loss: 1.4116 - val_mse: 0.9139\n",
      "Epoch 91/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7051 - mse: 0.2078 - val_loss: 1.4104 - val_mse: 0.9130\n",
      "Epoch 92/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7003 - mse: 0.2033 - val_loss: 1.4092 - val_mse: 0.9117\n",
      "Epoch 93/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6984 - mse: 0.2015 - val_loss: 1.4083 - val_mse: 0.9114\n",
      "Epoch 94/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6976 - mse: 0.2011 - val_loss: 1.4071 - val_mse: 0.9103\n",
      "Epoch 95/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6965 - mse: 0.2002 - val_loss: 1.4062 - val_mse: 0.9097\n",
      "Epoch 96/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6955 - mse: 0.1994 - val_loss: 1.4058 - val_mse: 0.9097\n",
      "Epoch 97/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6938 - mse: 0.1981 - val_loss: 1.4054 - val_mse: 0.9095\n",
      "Epoch 98/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6935 - mse: 0.1980 - val_loss: 1.4036 - val_mse: 0.9080\n",
      "Epoch 99/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6899 - mse: 0.1948 - val_loss: 1.4030 - val_mse: 0.9078\n",
      "Epoch 100/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6897 - mse: 0.1947 - val_loss: 1.4021 - val_mse: 0.9070\n",
      "Epoch 101/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6876 - mse: 0.1930 - val_loss: 1.4010 - val_mse: 0.9062\n",
      "Epoch 102/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6848 - mse: 0.1904 - val_loss: 1.4005 - val_mse: 0.9060\n",
      "Epoch 103/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6852 - mse: 0.1912 - val_loss: 1.3999 - val_mse: 0.9056\n",
      "Epoch 104/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6853 - mse: 0.1916 - val_loss: 1.3977 - val_mse: 0.9036\n",
      "Epoch 105/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6836 - mse: 0.1900 - val_loss: 1.3975 - val_mse: 0.9038\n",
      "Epoch 106/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6824 - mse: 0.1891 - val_loss: 1.3971 - val_mse: 0.9036\n",
      "Epoch 107/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6829 - mse: 0.1898 - val_loss: 1.3970 - val_mse: 0.9037\n",
      "Epoch 108/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6809 - mse: 0.1880 - val_loss: 1.3966 - val_mse: 0.9037\n",
      "Epoch 109/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6806 - mse: 0.1879 - val_loss: 1.3950 - val_mse: 0.9023\n",
      "Epoch 110/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6769 - mse: 0.1845 - val_loss: 1.3948 - val_mse: 0.9023\n",
      "Epoch 111/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6762 - mse: 0.1841 - val_loss: 1.3936 - val_mse: 0.9012\n",
      "Epoch 112/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6762 - mse: 0.1842 - val_loss: 1.3926 - val_mse: 0.9006\n",
      "Epoch 113/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6760 - mse: 0.1845 - val_loss: 1.3924 - val_mse: 0.9006\n",
      "Epoch 114/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6750 - mse: 0.1837 - val_loss: 1.3912 - val_mse: 0.8996\n",
      "Epoch 115/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6732 - mse: 0.1821 - val_loss: 1.3907 - val_mse: 0.8994\n",
      "Epoch 116/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6726 - mse: 0.1816 - val_loss: 1.3904 - val_mse: 0.8993\n",
      "Epoch 117/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6723 - mse: 0.1817 - val_loss: 1.3899 - val_mse: 0.8989\n",
      "Epoch 118/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6727 - mse: 0.1821 - val_loss: 1.3889 - val_mse: 0.8981\n",
      "Epoch 119/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6727 - mse: 0.1824 - val_loss: 1.3888 - val_mse: 0.8982\n",
      "Epoch 120/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6707 - mse: 0.1806 - val_loss: 1.3879 - val_mse: 0.8976\n",
      "Epoch 121/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6685 - mse: 0.1786 - val_loss: 1.3878 - val_mse: 0.8977\n",
      "Epoch 122/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6701 - mse: 0.1805 - val_loss: 1.3871 - val_mse: 0.8972\n",
      "Epoch 123/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6693 - mse: 0.1798 - val_loss: 1.3865 - val_mse: 0.8969\n",
      "Epoch 124/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6686 - mse: 0.1795 - val_loss: 1.3858 - val_mse: 0.8964\n",
      "Epoch 125/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6660 - mse: 0.1767 - val_loss: 1.3857 - val_mse: 0.8965\n",
      "Epoch 126/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6667 - mse: 0.1780 - val_loss: 1.3845 - val_mse: 0.8954\n",
      "Epoch 127/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6655 - mse: 0.1768 - val_loss: 1.3846 - val_mse: 0.8958\n",
      "Epoch 128/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6648 - mse: 0.1763 - val_loss: 1.3837 - val_mse: 0.8951\n",
      "Epoch 129/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6632 - mse: 0.1750 - val_loss: 1.3836 - val_mse: 0.8951\n",
      "Epoch 130/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6631 - mse: 0.1750 - val_loss: 1.3827 - val_mse: 0.8942\n",
      "Epoch 131/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6630 - mse: 0.1748 - val_loss: 1.3823 - val_mse: 0.8943\n",
      "Epoch 132/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6627 - mse: 0.1749 - val_loss: 1.3822 - val_mse: 0.8942\n",
      "Epoch 133/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6615 - mse: 0.1739 - val_loss: 1.3816 - val_mse: 0.8938\n",
      "Epoch 134/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6631 - mse: 0.1756 - val_loss: 1.3818 - val_mse: 0.8940\n",
      "Epoch 135/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6601 - mse: 0.1728 - val_loss: 1.3811 - val_mse: 0.8936\n",
      "Epoch 136/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6605 - mse: 0.1734 - val_loss: 1.3811 - val_mse: 0.8936\n",
      "Epoch 137/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6609 - mse: 0.1738 - val_loss: 1.3804 - val_mse: 0.8930\n",
      "Epoch 138/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6600 - mse: 0.1731 - val_loss: 1.3797 - val_mse: 0.8924\n",
      "Epoch 139/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6596 - mse: 0.1728 - val_loss: 1.3796 - val_mse: 0.8927\n",
      "Epoch 140/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6594 - mse: 0.1727 - val_loss: 1.3796 - val_mse: 0.8927\n",
      "Epoch 141/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6574 - mse: 0.1708 - val_loss: 1.3789 - val_mse: 0.8920\n",
      "Epoch 142/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6564 - mse: 0.1700 - val_loss: 1.3786 - val_mse: 0.8921\n",
      "Epoch 143/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6581 - mse: 0.1718 - val_loss: 1.3783 - val_mse: 0.8917\n",
      "Epoch 144/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6557 - mse: 0.1695 - val_loss: 1.3780 - val_mse: 0.8916\n",
      "Epoch 145/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6570 - mse: 0.1708 - val_loss: 1.3776 - val_mse: 0.8913\n",
      "Epoch 146/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6583 - mse: 0.1723 - val_loss: 1.3778 - val_mse: 0.8916\n",
      "Epoch 147/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6559 - mse: 0.1700 - val_loss: 1.3769 - val_mse: 0.8909\n",
      "Epoch 148/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6548 - mse: 0.1692 - val_loss: 1.3774 - val_mse: 0.8913\n",
      "Epoch 149/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6542 - mse: 0.1686 - val_loss: 1.3769 - val_mse: 0.8910\n",
      "Epoch 150/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6564 - mse: 0.1708 - val_loss: 1.3770 - val_mse: 0.8912\n",
      "Epoch 151/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6556 - mse: 0.1703 - val_loss: 1.3762 - val_mse: 0.8904\n",
      "Epoch 152/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6550 - mse: 0.1694 - val_loss: 1.3760 - val_mse: 0.8904\n",
      "Epoch 153/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6539 - mse: 0.1688 - val_loss: 1.3763 - val_mse: 0.8907\n",
      "Epoch 154/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6532 - mse: 0.1680 - val_loss: 1.3755 - val_mse: 0.8900\n",
      "Epoch 155/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6526 - mse: 0.1674 - val_loss: 1.3760 - val_mse: 0.8905\n",
      "Epoch 156/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6547 - mse: 0.1695 - val_loss: 1.3757 - val_mse: 0.8903\n",
      "Epoch 157/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6540 - mse: 0.1689 - val_loss: 1.3755 - val_mse: 0.8901\n",
      "Epoch 158/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6527 - mse: 0.1679 - val_loss: 1.3757 - val_mse: 0.8904\n",
      "Epoch 159/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6525 - mse: 0.1676 - val_loss: 1.3749 - val_mse: 0.8896\n",
      "Epoch 160/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6522 - mse: 0.1674 - val_loss: 1.3746 - val_mse: 0.8895\n",
      "Epoch 161/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6536 - mse: 0.1687 - val_loss: 1.3750 - val_mse: 0.8899\n",
      "Epoch 162/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6520 - mse: 0.1673 - val_loss: 1.3746 - val_mse: 0.8898\n",
      "Epoch 163/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6540 - mse: 0.1693 - val_loss: 1.3748 - val_mse: 0.8900\n",
      "Epoch 164/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6524 - mse: 0.1681 - val_loss: 1.3750 - val_mse: 0.8902\n",
      "Epoch 165/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6503 - mse: 0.1659 - val_loss: 1.3750 - val_mse: 0.8903\n",
      "Epoch 166/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6518 - mse: 0.1674 - val_loss: 1.3748 - val_mse: 0.8901\n",
      "Epoch 167/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6497 - mse: 0.1652 - val_loss: 1.3752 - val_mse: 0.8906\n",
      "Epoch 168/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6486 - mse: 0.1643 - val_loss: 1.3746 - val_mse: 0.8900\n",
      "Epoch 169/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6509 - mse: 0.1665 - val_loss: 1.3746 - val_mse: 0.8900\n",
      "Epoch 170/500\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6512 - mse: 0.1670 - val_loss: 1.3741 - val_mse: 0.8896\n",
      "Epoch 170: early stopping\n",
      "Restoring model weights from the end of the best epoch: 160.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f5b46641fa0>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "#Best hyper-parameters : {'k': 30, 'lambda_': 5e-05}\n",
    "\n",
    "\n",
    "X = [dataset[\"userId\"].to_numpy(), dataset[\"movieId\"].to_numpy()]\n",
    "y = dataset[\"rating\"].to_numpy()\n",
    "\n",
    "#TOFILL\n",
    "best_model=get_mf_bias_l2_reg_model(nb_users, nb_movies, k =30, lambda_ =5e-05)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_mse', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "best_model.fit(X, y, epochs=500, batch_size=512, validation_split=0.1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend the top-5 movies for the 10 first users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your retrained best model with optimal hyper parameters, compute the predictions for all the ratings that are not in the `dataset` for the 10 first users (indexes from 0 to 9). That means all the movies $i$ that these users $u \\in 0,\\ldots,9$ haven't rated, thus all the $u,i$ combinations that are not in the `dataset` dataframe rows.\n",
    "\n",
    "Order these predicted ratings for these users by decreasing order, and print out the 5 first ones, i.e. the ones that have the highest predicted ratings. Use the *movies.csv* file to print the real titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top5_for_user(model, user_id, dataset):\n",
    "    \"\"\"\n",
    "    Returns a list of the 5 movies that have the highest ratings among the unrated movies\n",
    "    of user `user_id`, along with a list of their predicted ratings.\n",
    "    \n",
    "    Input :\n",
    "        model : keras.models.Model : A trained matrix factorization model\n",
    "        user_id : int : The user id to use\n",
    "        dataset : DataFrame : The whole dataset, useful to find the movies \n",
    "            the user `user_id` has already rated\n",
    "    \n",
    "    Output :\n",
    "        five_best_movie_ids : list : The five movie ids among unrated movies by user `user_id` \n",
    "            that have the highest predicted ratings, in order\n",
    "        five_best_ratings : list : The corresponding five ratings\n",
    "    \"\"\"\n",
    "    X = dataset\n",
    "    \n",
    "    #TOFILL\n",
    "    \n",
    "    \n",
    "    return five_best_movie_ids, five_best_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([413, 476, 138, ..., 356, 598, 138])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 86/316\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/ml4/lib/python3.12/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['u__user_id', 'i__movie_id']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m316/316\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Best model test RMSE : 0.9431332970838369 \n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"Best model test RMSE : %s \" % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now have a look at what is going on in the embedding space of the movies that we learnt. Our brain cannot picture anything beyond 3 dimensions, and we learnt high dimensional embeddings (k=15 or 30), so we are going to project the movies embeddings on a 2D plane, first with PCA, and then with another algorithm made for visualizing high dimensional spaces called t-sne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have already studied PCA, it is a useful technique for dimensionality reduction, but also simply for visualization. Don't forget to scale your embeddings first. To access the embeddings values of your keras model, have a look at the *get_weights()* function.\n",
    "\n",
    "Compute a PCA on all your movies embeddings, get the 2 first principal components, and do a scatter plot of all the movies on a 2D plane, where each movie is a point defined by the two values of the two first principal components of the PCA from its embedding. Add the titles of the movies to each point of the plot (use plotly to do so it will be clearer), and try to see if you can interpret the axes of the PCA through to different movie genres, like in Figure 3 from the article *Matrix Factorization Techniques for Recommender Systems*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-sne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same with t-sne, an algorithm specialized for visualizing high dimensional spaces, you can read more about it there : https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#TOFILL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-sne in general tends to preserve local similarities better than PCA. In any case, it's always interesting to try both for visualizing high dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, you can export your embedding and upload them on https://projector.tensorflow.org/ to visualize the embeddings in 3D. You can also use the movies genres from the *movies.csv* file to make one plot for each movie genre and try to see if some parts of the embedding space are representative of a movie genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOING FURTHER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend movies to yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that ask you to rate 20 movies, then add your own ratings to the dataset, retrain the model, and compute your own top-5 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def rate_my_movies(my_user_id, dataset, nb_movies, nb_to_rate, movie_ids_map):\n",
    "    \"\"\"\n",
    "    Returns a dataframe in the same format as the dataset dataframe, with\n",
    "    ratings entered by the user for `nb_to_rate` random movies\n",
    "    \n",
    "    Input :\n",
    "        my_user_id : int : The user_id of the new ratings\n",
    "        dataset : DataFrame : The whole dataset \n",
    "        nb_movies : int : Number of unique movie ids\n",
    "        nb_to_rate : int : Number of movies to rate\n",
    "        movie_ids_map : dict : The mapping of original file userId to a new index starting at 0.\n",
    "    \n",
    "    Output : \n",
    "        my_ratings : DataFrame : A dataframe with the same column as `dataset` containing\n",
    "            the new ratings entered by the user\n",
    "    \"\"\"\n",
    "    #TOFILL\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    return my_ratings\n",
    "\n",
    "\n",
    "my_user_id = len(user_ids_map)\n",
    "\n",
    "my_ratings = rate_my_movies(my_user_id, dataset, nb_movies, 20, movie_ids_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_me = pd.concat([dataset, my_ratings], axis = 0).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X_with_me = [dataset_with_me[\"userId\"].to_numpy(), dataset_with_me[\"movieId\"].to_numpy()]\n",
    "y_with_me = dataset_with_me[\"rating\"].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model=get_mf_bias_l2_reg_model(nb_users + 1, nb_movies, k = best_params['k'], lambda_ = best_params['lambda_'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_mse', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "best_model.fit(X_with_me, y_with_me, epochs=500, batch_size=512, validation_split=0.1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_best_movie_ids, five_best_ratings =  get_top5_for_user(best_model, my_user_id, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuse the movie embeddings to predict the movies genre with multi-label classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the goal of predicting missing rating, the matrix factorization techniques also produces vectorial representation of movies and users: their embeddings, what we just visualized for the movies. With a big enough dataset, these embeddings actually are good abstract representations of the movies and of the users, and can be reused as features for other tasks, such as classification.\n",
    "\n",
    "In the *movies.csv*, there is a column that gives the genres of each movie. Let's try to predict the genres of the movies from the embeddings we learnt. As you can see, each movie can have more than one genre, so in classification terms, more than one class. We can achieve that with *multilabel classification*. You can read more about it there: https://scikit-learn.org/stable/modules/multiclass.html\n",
    "\n",
    "Load the movies genre, encode them as binary classes and use the classes imported below to train a multilabel classifier that uses the movie embeddings as features, and the movie genres as classes. Use the *OneVsRestClassifier* with a simple *LinearSVC* without any hyper-parameter tuning. Finally print the test accuracy, F1, precision and recall for each class, as well as the number of time each class appears in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rare classes, you should get a very high accuracy, with a very low F1. Indeed these classes are really imbalanced : there are a few positives, hence the classifier is largely biased toward the negatives, and rarely predict a positive for these classes. This is why accuracy is generally a bad measure with imbalanced dataset : the high number of true negatives makes the accuracy number high, while our model is actually barely capable of predicting true positives.\n",
    "\n",
    "Let's compare our classifier performance with a *DummyClassifier*, the dummy classifier takes the ratio $r = \\frac{nb\\_positives}{nb\\_positives + nb\\_negatives}$ as the probability to predict a positive, and then do it randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "#TOFILL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, simply respecting the class balance, even at random, produces better F1 on most classes. One way to compensate for class imbalance is to tell the classifier to weight more the true samples at training time, accordingly with the ratio $r$ between true and false samples. With scikit-learn SVM implementation, you can use the argument *class_weight* for setting the weight of the positive and negative samples at training time. See : https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html\n",
    "\n",
    "But if you just want to set the class weights accordingly with the ratio between positives and negatives, you can just set *class_weight = balanced*. Test it with the LinearSVC classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TOFILL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 is now much better than with the dummy classifier, however is is still not very convincing. This is quite normal given the size of the dataset we are using, which is pretty small to get really meaningful embeddings. But with bigger datasets, reusing embeddings as features for auxiliary tasks such as classification is actually a very effective way of doing so when there is no other informations about the items we try to classify. Here the items are the movies, the dataset doesn't provide more information about them, but one could imagine fetching from internet textual descriptions of the movies and use them as features alongside the embeddings to improve the classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out the different SGD algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the notebook we used the 'adam' `optimizer` to train our model, which is a variation of SGD. Keras proposes different variations of SGD: https://keras.io/optimizers/ . This article gif images gives an intuitive view of their different behavior : https://medium.com/@ramrajchandradevan/the-evolution-of-gradient-descend-optimization-algorithm-4106a6702d39\n",
    "\n",
    "Try a few ones with our model and see how the training and testing loss evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the global bias $\\mu$  parameter to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we didn't added the global bias $\\mu$ to our model yet (Equations (4-5) from Koren's paper). Use your best google skills to find a way to add an embedding layer that does that.\n",
    "\n",
    "Hint : Use a constant `Input` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your own Stochastic Gradient Descent for Matrix Factorization with numpy instead of Keras (very optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know everything to implement your own matrix factorization SGD model, all with numpy arrays. Start without the biases again, and without mini-batches. The gradient update equations are described in page 4 of Koren's paper. Let's initialize your $p$ and $q$ embeddings with a gaussian sampling. Print the RMSE at the beginning of each epoch, and finally compute the RMSE of your model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import normal\n",
    "\n",
    "P = normal(size = (nb_users,k))\n",
    "Q = normal(size = (nb_movies,k))\n",
    "\n",
    "gamma = 0.1\n",
    "lambda_ = 0.00001\n",
    "epochs = 10\n",
    "\n",
    "for e in range(epochs):\n",
    "    for j in range(train.shape[0]):\n",
    "        u = train['userId'].iloc[j]\n",
    "        i = train['movieId'].iloc[j]\n",
    "        r_ui = train['rating'].iloc[j]\n",
    "        \n",
    "        #TOFILL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
