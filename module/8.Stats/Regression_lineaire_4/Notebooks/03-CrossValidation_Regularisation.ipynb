{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation, Model Selection & Regularisation\n",
    "\n",
    "Signification des Ã©moticones :\n",
    "- ðŸŒž : documentations importantes\n",
    "- ðŸ‘€ : documentations intÃ©ressantes Ã  connaÃ®tre\n",
    "- ðŸŒš : en complÃ©ment\n",
    "- (vide) : Ã  vous de voir\n",
    "\n",
    "In this notebook we will introduce the concepts below, and how they can be implemented in `scikit-learn`.\n",
    "* Cross validation - a method for estimating the test error rate when test data is not available\n",
    "* Model selection - how we use cross validation to select which model (from a selection) we should use for a particular data set\n",
    "* Regularisation - an adaptation of linear regression to make it more flexible\n",
    "\n",
    "[This video](https://www.youtube.com/watch?v=DQWI1kvmwRg) describes some of the ideas you will face in the coming notebook. The ideas we are covering here are described much more throughly in **ISLR** (see suggested sections in module overview).\n",
    "\n",
    "ðŸŒš https://trevorhastie.github.io/ISLR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "We have already seen the notion of splitting *test* and *training sets* in order to asses model performance. Now we will introduce the idea of *validation sets*.\n",
    "\n",
    "Once again **ISTL** (Section 5.1) provides a very clear overview of how these techniques work:\n",
    "\n",
    ">Resampling methods are an indispensable tool in modern statistics. They\n",
    "involve repeatedly drawing samples from a training set and refitting a model\n",
    "of interest on each sample in order to obtain additional information about\n",
    "the fitted model. For example, in order to estimate the variability of a linear\n",
    "regression fit, we can repeatedly draw different samples from the training\n",
    "data, fit a linear regression to each new sample, and then examine the\n",
    "extent to which the resulting fits differ. Such an approach may allow us to\n",
    "obtain information that would not be available from fitting the model only\n",
    "once using the original training sample.\n",
    "\n",
    ">Resampling approaches can be computationally expensive, because they\n",
    "involve fitting the same statistical method multiple times using different\n",
    "subsets of the training data. However, due to recent advances in computing\n",
    "power, the computational requirements of resampling methods generally\n",
    "are not prohibitive. [...] cross-validation can be used to estimate the test\n",
    "error associated with a given statistical learning method in order to evaluate\n",
    "its performance, or to select the appropriate level of flexibility. The process\n",
    "of evaluating a modelâ€™s performance is known as model assessment, whereas model\n",
    "the process of selecting the proper level of flexibility for a model is known as assessment\n",
    "model selection.\n",
    "\n",
    "The diagram below illustrates this process. Here the train and validation sets are used to do model assesment and model selection (NOTE: the test set is strictly forbidden from being used in any way during this process!). Once a model is selected the test set is used to do a final assesment of performance to see if the model selected will generalise as well as predicted.\n",
    "\n",
    "<img src=\"./images/testtrainvalid.png\" width=\"450px\">\n",
    "\n",
    "There is a simple overview these ideas ðŸ‘€[here](https://towardsdatascience.com/training-vs-testing-vs-validation-sets-a44bed52a0e1/), and a more thorough overview ðŸŒš[here](https://machinelearningmastery.com/difference-test-validation-datasets/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross validation\n",
    "\n",
    "A very common method called *k-fold* is often used, which actually splits the training set multiple times. This allows us to assess the accuracy of the model over $k$ validation splits of data. The ðŸŒš[image below](http://ethen8181.github.io/machine-learning/model_selection/model_selection.html) illustrates how this works for $k = 5$ splits.\n",
    "\n",
    "<img src=\"./images/kfold.png\" width=\"450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scikit-learn` documentation offers a simple ðŸŒž[example](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) of implementing k-fold on some dummy data. We will examine this below. NOTE: The `scikit-learn` documentation is **FANSTASTIC(!)** and contains working examples of every function, it should always be the first place you look when you wish to implement a new function.\n",
    "\n",
    "**Task 1:** \n",
    "\n",
    "* Have a look at the code below and check you understand what is going on. (Add some print statements in various places to help.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "X = np.array([['A', 'B'], ['C', 'D'], ['E', 'F'], ['G', 'H']])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "kf = KFold(n_splits=3) # here we choose the number of folds (or splits) we will make\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "for train_index, valid_index in kf.split(X): # kf.split(X) is an iterable which gives us the indices of the data in each fold\n",
    "    X_train, X_valid = X[train_index], X[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this to any data set to perform operations as it gives us the dataframe/numpy array indicies in each loop to select the appropriate data for each fold. For example we can apply this to the auto data set. This contains rows which correspond to individual cars and their attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0  18.0          8         307.0         130    3504          12.0    70   \n",
       "1  15.0          8         350.0         165    3693          11.5    70   \n",
       "2  18.0          8         318.0         150    3436          11.0    70   \n",
       "3  16.0          8         304.0         150    3433          12.0    70   \n",
       "4  17.0          8         302.0         140    3449          10.5    70   \n",
       "\n",
       "   origin                       name  \n",
       "0       1  chevrolet chevelle malibu  \n",
       "1       1          buick skylark 320  \n",
       "2       1         plymouth satellite  \n",
       "3       1              amc rebel sst  \n",
       "4       1                ford torino  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto = pd.read_csv('./data/Auto.csv')\n",
    "auto = auto[auto.horsepower != '?']\n",
    "auto['horsepower'] = auto.horsepower.astype(int)\n",
    "auto.reset_index(inplace=True, drop=True)\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg = auto.pop('mpg') # mpg will be our target and so we remove this into a seperate array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we are trying to predict 'mpg' from our other automobile data features. We can use KFold to iterate over the number of splits we choose.\n",
    "\n",
    "**Task 2:**\n",
    "* try adding print statements for the size of the dataframes in each split\n",
    "* try increasing the number of splits and re-run your code\n",
    "* use the code below to print a car name contained in the train and validation data set, for each split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=2, random_state=None, shuffle=False)\n",
      "------------------------------------------------------------\n",
      "This is split no: 1\n",
      "Train car name here\n",
      "Validation car name here\n",
      "------------------------------------------------------------\n",
      "This is split no: 2\n",
      "Train car name here\n",
      "Validation car name here\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=2) # here we choose the number of folds (or splits) we will make\n",
    "kf.get_n_splits(auto)\n",
    "\n",
    "print(kf)\n",
    "\n",
    "\n",
    "split_counter = 1\n",
    "for train_index, valid_index in kf.split(auto): # kf.split(X) is an iterable which gives us the indices of the data in each fold\n",
    "    print('-'*60)\n",
    "    print('This is split no: {}'.format(split_counter))\n",
    "    split_counter += 1 \n",
    "    X_train, X_valid = auto.iloc[train_index], auto.iloc[valid_index] # must use .iloc because its a dataframe this time\n",
    "    y_train, y_valid = mpg[train_index], mpg[valid_index]\n",
    "    \n",
    "    # your code here\n",
    "    print('Train car name here')\n",
    "    print('Validation car name here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your solution here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = auto.drop('name', axis=1, errors='ignore') # we do not need the car names so remove for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same loop to fit and evaluate our linear regression model on each train/validation split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "This is split no: 1\n",
      "Model results\n",
      "training MSE: 11.284070590566001\n",
      "validation MSE: 14.974307651304168\n",
      "------------------------------------------------------------\n",
      "This is split no: 2\n",
      "Model results\n",
      "training MSE: 11.155158050598772\n",
      "validation MSE: 10.905952427081171\n",
      "------------------------------------------------------------\n",
      "This is split no: 3\n",
      "Model results\n",
      "training MSE: 12.16010513687152\n",
      "validation MSE: 5.991708610108162\n",
      "------------------------------------------------------------\n",
      "This is split no: 4\n",
      "Model results\n",
      "training MSE: 9.921674145405685\n",
      "validation MSE: 15.587544657621601\n",
      "------------------------------------------------------------\n",
      "This is split no: 5\n",
      "Model results\n",
      "training MSE: 7.977511689294959\n",
      "validation MSE: 27.844743081984223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "split_counter = 1\n",
    "mse_scores = [] # create empty list to append mse scores for each split\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(auto)\n",
    "\n",
    "for train_index, valid_index in kf.split(auto): # kf.split(X) is an iterable which gives us the indices of the data in each fold\n",
    "    print('-'*60)\n",
    "    print('This is split no: {}'.format(split_counter))\n",
    "    print('Model results')\n",
    "    split_counter += 1 \n",
    "    X_train, X_valid = auto.iloc[train_index], auto.iloc[valid_index] # must use .iloc because its a dataframe this time\n",
    "    y_train, y_valid = mpg[train_index], mpg[valid_index]\n",
    "    \n",
    "    \n",
    "    #### fit polynomial to train data in this split\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "    \n",
    "    #### eval & print MSE training results in this split\n",
    "    mpg_train_pred = lin_reg.predict(X_train)\n",
    "    mse_train = mean_squared_error(y_train, mpg_train_pred)\n",
    "    print('training MSE: {0}'.format(mse_train))\n",
    "    \n",
    "    #### do the same for validation split\n",
    "    mpg_valid_pred = lin_reg.predict(X_valid)\n",
    "    mse_valid = mean_squared_error(y_valid, mpg_valid_pred)\n",
    "    print('validation MSE: {0}'.format(mse_valid))\n",
    "    \n",
    "    mse_scores.append(mse_valid) # assign validation MSE score to list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using k-fold cross validation we can analyse the validation MSE result for each split to assess the overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION SET MSE SCORES\n",
      "mean MSE: 15.060851285619865\n",
      "std MSE: 7.255691814890938\n"
     ]
    }
   ],
   "source": [
    "mse_scores  = np.array(mse_scores)\n",
    "print('VALIDATION SET MSE SCORES')\n",
    "print('mean MSE:', mse_scores.mean())\n",
    "print('std MSE:', mse_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation in practice in sklearn\n",
    "Most of the time we do not care about having access to each split. `Scikit-Learn` provide a much easier way to do all of this with the function `cross_val_score`. This allows us to do the same as above but in much less code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean MSE: 15.060851285619865\n",
      "std MSE: 7.255691814890938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "cv_scores = cross_val_score(lin_reg, auto, mpg, cv = 5, scoring='neg_mean_squared_error') \n",
    "\n",
    "print('mean MSE:',np.mean(-cv_scores))\n",
    "print('std MSE:',np.std(-cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-14.97430765, -10.90595243,  -5.99170861, -15.58754466,\n",
       "       -27.84474308])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:**\n",
    "\n",
    "* Make sure you understand the output of the cross_val_score above (i.e. What is cv_scores?)\n",
    "* Why is the scoring defined as negative MSE? Do some research\n",
    "* Investigate what the `cross_val_predict` function does.\n",
    "* Import and implement `cross_val_predict`on the same data as above.\n",
    "* What are the outputs of this function?\n",
    "* Can you use these to evaluate the results of your cross validation?\n",
    "* Do the cross-validation scores give you confidence this model is providing a useful prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1: wine cross-validation\n",
    "\n",
    "You must predict the alcohol content of various wines based on their other attributes.\n",
    "\n",
    "* Split the data into train and test data sets (Ensure you use the option: `random_state = 28`).\n",
    "* Perform linear regression using k-fold cross validation(ensure you use 5 folds). Return the cross validation MSE errors. Return the mean and standard deviations of these.\n",
    "* Evaluate the performance of the model on the test set.\n",
    "* Compare the cross-validation error and the test error (MSE). What do you find? \n",
    "* Try removing the random_state option. What happens to your results? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation\n",
    "\n",
    "An alternative to choosing models which contains smaller numbers of features is to use a method that *constrains* or *regularises* the coefficent estimates assigned to each feature, or that shrinks the coefficient towards zero. This technique is very similar to *least squares* which we have been using until now. Please refer to Section in 6.2 **ISTL** for a fuller explanation of this.\n",
    "\n",
    "When we move to use a regularised linear regression for prediction the additional term means that we now have a model parameter that requires setting or tuning. These terms are referred to as *hyperparameters* in machine learning. In practice this introduces another additional unknown parameter which we must choose somewhere in our modelling. It is common practice to run several models, each with different values of this hyperparameter, and then assess the error of each using cross validation for comparison.\n",
    "\n",
    "**Lasso vs Ridge** (just for information curiosity, no need to go in depth or spend to much time to understand the maths)\n",
    "\n",
    "For now we will focus on how to implement Lasso and Ridge regression in sklearn. These are both types of regularised linear regression.\n",
    "\n",
    "Lasso: objectif is to minimize\n",
    "$$ RSS + \\lambda \\sum |\\beta | $$\n",
    "* can force coefficients exactly to zero: behaves thus as variable selection\n",
    "\n",
    "\n",
    "Ridge: objectif is to minimize\n",
    "$$ RSS + \\lambda \\sum \\beta_i^2 $$\n",
    "* does not force coefficients exactly to zero\n",
    "* interestig when there are more predictors than observations\n",
    "\n",
    "see further reading ILS, search for Ridge and Lasso\n",
    "\n",
    "#### Lasso regression in sklearn\n",
    "\n",
    "In this example we aim to predict credit rating of individual customers. To train and predict using a Lasso regression we follow much the same procedure as we have seen before in `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional sources for further reading :\n",
    "\n",
    "ðŸ‘€ https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net\n",
    "\n",
    "ðŸŒš https://eric.univ-lyon2.fr/ricco/cours/slides/regularized_regression.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Balance</th>\n",
       "      <th>African American</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Caucasian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.891</td>\n",
       "      <td>3606</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106.025</td>\n",
       "      <td>6645</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>903</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104.593</td>\n",
       "      <td>7075</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>580</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148.924</td>\n",
       "      <td>9504</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>964</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.882</td>\n",
       "      <td>4897</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>331</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Income  Limit  Cards  Age  Education  Gender  Student  Married  Balance  \\\n",
       "0   14.891   3606      2   34         11       0        0        1      333   \n",
       "1  106.025   6645      3   82         15       1        1        1      903   \n",
       "2  104.593   7075      4   71         11       0        0        0      580   \n",
       "3  148.924   9504      3   36         11       1        0        0      964   \n",
       "4   55.882   4897      2   68         16       0        0        1      331   \n",
       "\n",
       "   African American  Asian  Caucasian  \n",
       "0                 0      0          1  \n",
       "1                 0      1          0  \n",
       "2                 0      1          0  \n",
       "3                 0      1          0  \n",
       "4                 0      0          1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit = pd.read_csv('./data/credit_modified.csv')\n",
    "rating = credit.pop('Rating')\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### splitting train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(credit, rating, random_state = 91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean MSE: 148.36580248151395\n",
      "std MSE: 22.577610846122532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha = 10)\n",
    "cv_scores = cross_val_score(lasso, X_train, y_train, cv = 5, scoring='neg_mean_squared_error')\n",
    "print('mean MSE:',np.mean(-cv_scores))\n",
    "print('std MSE:',np.std(-cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can alter the alpha parameter to change the amount of regularisation the model uses (try this yourself! - vary the value by at least factors of 10). With increases in regularisation we expect a reduction in the *variance* of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Lasso with grid search\n",
    "\n",
    "In practice we do not want to vary hyperparameters by hand to find which value is best (the model with minimum cross validation error). Of course `scikit-learn` has a function that automates this for you. Using `GridSearchCV` we pass a dictionary of parameter values we wish to investigate. The function will fit each model we have listed and calculate the cross validation error of each. It provides all the results through the object it returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002799</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'alpha': 0.001}</td>\n",
       "      <td>-134.693531</td>\n",
       "      <td>-105.857049</td>\n",
       "      <td>-101.376944</td>\n",
       "      <td>-87.569097</td>\n",
       "      <td>-78.913304</td>\n",
       "      <td>-76.137420</td>\n",
       "      <td>-109.786540</td>\n",
       "      <td>-103.885686</td>\n",
       "      <td>-106.104721</td>\n",
       "      <td>-113.455931</td>\n",
       "      <td>-101.778022</td>\n",
       "      <td>16.450021</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002287</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'alpha': 0.01}</td>\n",
       "      <td>-134.553201</td>\n",
       "      <td>-105.948212</td>\n",
       "      <td>-101.096381</td>\n",
       "      <td>-87.881139</td>\n",
       "      <td>-78.937345</td>\n",
       "      <td>-76.585584</td>\n",
       "      <td>-109.682491</td>\n",
       "      <td>-103.106018</td>\n",
       "      <td>-106.112572</td>\n",
       "      <td>-113.688788</td>\n",
       "      <td>-101.759173</td>\n",
       "      <td>16.329243</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>-133.132694</td>\n",
       "      <td>-106.809300</td>\n",
       "      <td>-98.722275</td>\n",
       "      <td>-91.394863</td>\n",
       "      <td>-79.351692</td>\n",
       "      <td>-78.012317</td>\n",
       "      <td>-108.680986</td>\n",
       "      <td>-96.604537</td>\n",
       "      <td>-106.093953</td>\n",
       "      <td>-115.062267</td>\n",
       "      <td>-101.386489</td>\n",
       "      <td>15.688554</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>1</td>\n",
       "      <td>{'alpha': 1}</td>\n",
       "      <td>-127.851853</td>\n",
       "      <td>-111.017555</td>\n",
       "      <td>-92.827631</td>\n",
       "      <td>-106.006184</td>\n",
       "      <td>-89.048154</td>\n",
       "      <td>-82.339570</td>\n",
       "      <td>-105.158164</td>\n",
       "      <td>-94.934777</td>\n",
       "      <td>-102.310102</td>\n",
       "      <td>-122.139519</td>\n",
       "      <td>-103.363351</td>\n",
       "      <td>13.605513</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>3</td>\n",
       "      <td>{'alpha': 3}</td>\n",
       "      <td>-132.499698</td>\n",
       "      <td>-117.173048</td>\n",
       "      <td>-106.238114</td>\n",
       "      <td>-112.301661</td>\n",
       "      <td>-94.847286</td>\n",
       "      <td>-76.537019</td>\n",
       "      <td>-102.509314</td>\n",
       "      <td>-98.851062</td>\n",
       "      <td>-105.902336</td>\n",
       "      <td>-131.338637</td>\n",
       "      <td>-107.819817</td>\n",
       "      <td>15.910751</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "0       0.002799      0.000410         0.001204        0.000389       0.001   \n",
       "1       0.002287      0.000457         0.001114        0.000311        0.01   \n",
       "2       0.002001      0.000016         0.000999        0.000023         0.1   \n",
       "3       0.001696      0.000448         0.000907        0.000303           1   \n",
       "4       0.001598      0.000490         0.000906        0.000302           3   \n",
       "\n",
       "             params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'alpha': 0.001}        -134.693531        -105.857049        -101.376944   \n",
       "1   {'alpha': 0.01}        -134.553201        -105.948212        -101.096381   \n",
       "2    {'alpha': 0.1}        -133.132694        -106.809300         -98.722275   \n",
       "3      {'alpha': 1}        -127.851853        -111.017555         -92.827631   \n",
       "4      {'alpha': 3}        -132.499698        -117.173048        -106.238114   \n",
       "\n",
       "   split3_test_score  split4_test_score  split5_test_score  split6_test_score  \\\n",
       "0         -87.569097         -78.913304         -76.137420        -109.786540   \n",
       "1         -87.881139         -78.937345         -76.585584        -109.682491   \n",
       "2         -91.394863         -79.351692         -78.012317        -108.680986   \n",
       "3        -106.006184         -89.048154         -82.339570        -105.158164   \n",
       "4        -112.301661         -94.847286         -76.537019        -102.509314   \n",
       "\n",
       "   split7_test_score  split8_test_score  split9_test_score  mean_test_score  \\\n",
       "0        -103.885686        -106.104721        -113.455931      -101.778022   \n",
       "1        -103.106018        -106.112572        -113.688788      -101.759173   \n",
       "2         -96.604537        -106.093953        -115.062267      -101.386489   \n",
       "3         -94.934777        -102.310102        -122.139519      -103.363351   \n",
       "4         -98.851062        -105.902336        -131.338637      -107.819817   \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "0       16.450021                3  \n",
       "1       16.329243                2  \n",
       "2       15.688554                1  \n",
       "3       13.605513                4  \n",
       "4       15.910751                5  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "lasso = Lasso(max_iter=10000)\n",
    "\n",
    "param_grid = [\n",
    " {'alpha': [0.001, 0.01, 0.1, 1, 3, 10, 100, 1000]}\n",
    " ]\n",
    "\n",
    "grid_search = GridSearchCV(lasso, param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "grid_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "grid_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridSearchCV` also returns a model (with the best hyperparmeter combination it found) which has been fitted one final time to all of the training data. Therefore it is ready to make predictions on the testing set. The model can be accessed like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.1, max_iter=10000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:**\n",
    "* How many times will the lasso model be fitted when the GridSearchCV function is called above?\n",
    "\n",
    "<details><summary>Hint</summary><br>\n",
    "Check what the `refit=True` parameter does in GridSearchCV\n",
    "</details>\n",
    "\n",
    "* Look through the columns of the `grid_results` dataframe. Try and understand what the table contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘€ https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "Congratulations! You have just done your first model hyperparameter tuning in `scikit-learn`! \n",
    "\n",
    "If we have a dataset for which we are interested in developing a predictive model. We do not know beforehand which model will perform best for this particular data or problem. Therefore, we fit and evaluate a number of different models to our data. The models could also be of varying type as well as flexibility (e.g. random forests, support vector machines, linear regression). We then need to decide which of our models we will choose to use in our final product.\n",
    "\n",
    "As **ISLR** states:\n",
    "> \"we can directly estimate the test error using the validation set and cross-validation methods\n",
    "discussed in Chapter 5. We can compute the validation set error or the\n",
    "cross-validation error for each model under consideration, and then select\n",
    "the model for which the resulting estimated test error is smallest.\"\n",
    "\n",
    "This works as a simple rule, which we will follow for the remainder of this notebook. However in practice the selection can sometimes be a bit more nuanced. Read more detail ðŸŒš[here](https://machinelearningmastery.com/a-gentle-introduction-to-model-selection-for-machine-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5: Ridge regression in sklearn\n",
    "\n",
    "Another type of regularised linear model is know as *Ridge regression*.\n",
    "\n",
    "* Repeat the model prediction process above on the credit data but use a ridge regression model.\n",
    "* Try replacing `GridSearchCV` with `RandomizedSearchCV`\n",
    "* How do these functions differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### change to randomised search CV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 2: Moneyball\n",
    "\n",
    "Moneyball, as well as being a fantastic story, is also a true story of statistical methods being applied in a real world context to make predictions for decision making. [The film Moneyball](https://www.youtube.com/watch?v=-4QPVo0UIzc) is well worth a watch if you have time. As well as in baseball most major competitive sports teams are now using data science to improve their performance, e.g. [football](https://www.isportconnect.com/the-rise-of-data-driven-football-how-ai-and-analytics-are-shaping-the-future-of-the-game/),...\n",
    "\n",
    "In this excercise you have been hired by Oakland Athletics general manager Billy Beane. Your first mission is to predict the salary each player will make based on other information that is available. This will allow Billy to understand what price he should pay for players in the next transfer season.\n",
    "\n",
    "You must:\n",
    "* Import and prepare the data\n",
    "* Create a train and test set\n",
    "* Implement a regularised model of your choice (Ridge or Lasso)\n",
    "* Choose optimal parameters for your regularised model\n",
    "* Estimate test-error using k-fold cross validation\n",
    "* Calculate the true test-error\n",
    "* Run a base line model to compare your model results. Base line model is the most simple approach based on strategy of choice (mean or other). It is then used as reference to conclude whether more complex models are better or not: see DummyRegressor in the sklearn library.\n",
    "\n",
    "HINT 1\n",
    "* Some values are missing. You can drop these rows.\n",
    "\n",
    "HINT 2\n",
    "* Some columns do not contain numerical values. You can drop these columns. In the Machine Learning model you'll learn more about labeling categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your solution here\n",
    "# import the hitters.csv dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train test split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear regression model build using CV to estimate what our model performance will be on data it hasnt seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### using k-fold cross val score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularised model - lasso build\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameter for model performance....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One important thing** to realise is that when we run our `GridSearchCV` for our lasso model we are changing the flexibility of our model and estimating our overall test error. In essence we are estimating this curve below, which we saw in the text book!\n",
    "\n",
    "<img src=\"./images/bias_var_tradeoff.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot our cv values for each of our model flexibilities (alpha values)...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cv values against alpha parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6:**\n",
    "* Ridge regression works best when the input variables are standardised. (See section 6.2 **ISLR** for more details.). Try standardising your data before running your model. Do you find different results?\n",
    "* does this model outperform a simple linear regression model?\n",
    "* Which variables are most important in the model?\n",
    "* Make a pairplot or scatter matrix: pd.plotting.scatter_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These and other things to consider in this problem...**\n",
    "\n",
    "* We must actually standardise our *X* variables by scaling each *X* by the standard deviation, if not Ridge will not penalise each variable in the same way. We will learn a robust an easy way to do this in the intro to ML module.\n",
    "* Once our variables are scaled we can use the $\\beta$ coefficients that our model finds to see which has a greater influence in our model. We can find as we see below.\n",
    "* The reason our model does not perform so well is that most of our *X* variables do not have a very clear linear relationship with *y* (see below)! Using non-linear models could be of use in this problem...again we will cover these soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "#standardize data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model and compare with results from non-standardized X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### solution plot scatter matrix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "328px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
